{"componentChunkName":"component---src-pages-search-jsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"nodes":[{"excerpt":"들어가며 Github Repository에 Issue Template과 PR Template을 적용하는 방법에 대해 작성한 글입니다. Issue Template Issue Template의 경우 Github Repository에 있는 Settings를 통해 쉽게 생성할 수 있습니다.  위 버튼을 누르면 아래와 같은 페이지가 뜹니다.  가운데 버튼을 눌러 …","fields":{"slug":"/apply-issue-and-pr-template/"},"frontmatter":{"date":"January 23, 2024","title":"[github] Github Repository에 Issue, PR Template 적용하기","tags":["git","github","Issue Template","PR Template"]},"rawMarkdownBody":"\n## 들어가며\n\nGithub Repository에 Issue Template과 PR Template을 적용하는 방법에 대해 작성한 글입니다.\n\n## Issue Template\n\nIssue Template의 경우 Github Repository에 있는 Settings를 통해 쉽게 생성할 수 있습니다.\n\n![Github Repository에 Settings 탭을 통해 Template 생성하기](set_up_templates.png)\n\n위 버튼을 누르면 아래와 같은 페이지가 뜹니다.\n\n![](create_issue_template_init_page.png)\n\n가운데 버튼을 눌러 Bug Report, Feature에 대한 Template을 생성할 수 있습니다.\n\n![Template 종류](select_box.png)\n\n이번 글에서는 제 취향을 담아 `Custom template`을 선택하여 Bug와 Feature 구분없이 작성할 수 있는 **공통 Template**을 작성하겠습니다.\n\n위 버튼을 누르면 아래처럼 새로운 Template이 생성됩니다.\n\n![초기 생성된 Issue Template](create_new_issue_template.png)\n\n`Preview and edit` 버튼을 눌러 상세 내용을 적습니다.<br>\n저는 아래와 같이 적었습니다.\n\n![](result_issue_template.png)\n\n> 만약 Issue를 생성할 때 자동으로 제목에 PREFIX를 추가하고 싶다면 `Issue default title`에 값을 기입하면 됩니다.\n> \n> Assignees도 Issue Template에 따라 자동으로 설정할 수 있습니다.<br>\n> Lables 또한 마찬가지입니다.\n\n입맛대로 작성이 완료됐다면 우측 상단에 `Propose changes` 버튼을 눌러 Commit 하면 아래와 같이 해당 브랜치에 `.github/ISSUE_TEMPLATE` 디렉터리가 생성되고 그 안에는 작성한 내용에 따라 생성된 md 파일이 존재합니다.\n\n![](result_of_creation.png)\n\n그리고 이제 Issue 탭에서 새로운 Issue를 생성할 때 만들어놓은 Template을 맘껏 활용하면 됩니다.\n\n> 추후 수정이 필요하다면 md 파일 자체를 수정하거나 Settings를 통해 접근해서 수정할 수 있습니다.\n\n## PR Template\n\nPR Template은 Issue Template처럼 Github Repository의 Settings 탭에서 생성할 수 있는 기능은 없습니다.\n\n따라서 직접 md 파일을 추가해야 합니다.\n\n아래 사진처럼 Issue Template을 만들어서 생성된 `.github` 디렉터리로 이동합니다.<br>\n그리고 우측 상단에 `Add file` - `Create new file` 버튼을 누릅니다.\n\n![](github_directory.png)\n\n그리고 아래와 같이 PR Template을 작성해줍니다.\n\n파일 제목은 `pull_request_template.md`로 해주세요.\n\n> 물론 Template 본문은 본인 취향에 맞게 작성해주세요. 🙏\n\n![](creation_pr_template.png)\n\n그리고 우측 상단에 `Commit changes...`를 눌러 작업 내용을 Commit 합니다.\n\n이제 PR을 생성할 때 자동으로 Template이 작성 되어있는 것을 확인할 수 있습니다."},{"excerpt":"들어가며 이 글은 그림으로 배우는 Http & Network Basic을 읽고 학습한 내용을 정리한 글입니다. 이번 글에서는 Http Request와 Http Response가 어떻게 동작하는지에 대해 작성하겠습니다. HTTP 메시지 HTTP 프로토콜은 여러 개의 행으로 이루어진 문자열입니다. 이를 HTTP 메시지 라고 합니다. HTTP 메시지는 정해진 …","fields":{"slug":"/03-http-detail/"},"frontmatter":{"date":"January 23, 2024","title":"3장. HTTP 정보는 HTTP 메시지에 있다","tags":["Network"]},"rawMarkdownBody":"\n## 들어가며 \n\n이 글은 [그림으로 배우는 Http & Network Basic](https://m.yes24.com/Goods/Detail/15894097)을 읽고 학습한 내용을 정리한 글입니다.\n\n이번 글에서는 Http Request와 Http Response가 어떻게 동작하는지에 대해 작성하겠습니다.\n\n## HTTP 메시지\n\nHTTP 프로토콜은 **여러 개의 행으로 이루어진 문자열**입니다.\n\n이를 *HTTP 메시지* 라고 합니다.\n\nHTTP 메시지는 정해진 구조가 있습니다.\n\n> 참고: [HTTP Request와 Response의 구조에 대해 정리한 글](https://kdkdhoho.github.io/02-http-protocol/)\n\n## HTTP 압축\n\nHTTP Request나 Response를 보낼 때 HTTP 메시지의 크기가 크다면 **압축**해서 보낼 수 있습니다.\n\n압축하는 방식은 *콘텐츠 코딩*, *청크 전송 코딩* 으로 나뉠 수 있습니다.\n\n### 콘텐츠 코딩\n\nHTTP 메시지 본문이나 전송되는 데이터를 압축하는 방식입니다.\n\n이미지나 영상같은 Multipart 파일이나 Json 형식의 String이 압축되는 것을 의미합니다.\n\n#### File 형식 압축 (Multipart 파일 압축)\n\n두 가지 방식이 있습니다.\n\n- 무손실 압축\n  - 데이터를 압축하는 과정에서 데이터의 손실이 발생하지 않습니다.\n  - 이미지에서는 `gif`, `png`의 파일 확장자를 가진 파일이 사용합니다.\n  - `zip`도 무손실 압축에 포함됩니다.\n\n- 손실 압축\n  - 사용자가 알아채기 힘든 정도로 데이터를 일부 손실하면서 압축하는 방식입니다.\n  - 웹에서 비디오 형식은 모두 손실 압축이며 이미지의 경우 `jpeg`가 적용됩니다.\n\n`webp`는 무손실 혹은 손실 모두 적용 가능합니다.\n\n#### 종단간 압축 (Json 형식의 String 압축)\n\n서버에서 Http Response를 압축하고 클라이언트가 수신한 Http Response를 압축 해제하는 방식을 _종단간 압축_ 방식이라고 합니다.\n\n흔히 사용되는 압축 알고리즘은 `gzip` 이 있습니다. 그리고 `br` 도 있습니다. \n\n브라우저는 브라우저가 지원하는 압축 알고리즘(gzip, br)을 `Accept-Encoding` 헤더에 정보를 담아서 보냅니다.<br>\n이를 활용해 서버는 적절한 압축 알고리즘 방식으로 Http Response를 압축하여 응답합니다.<br>\n이때, 선택한 알고리즘은 `Content-Encoding` 헤더를 통해 알려줍니다. 그리고 캐싱과 관련된 헤더인 `Vary` 헤더에 `Accept-Encoding` 값을 추가해 새로운 캐시 정책을 적용할 수 있도록 합니다. \n\n![종단간 압축](http_compress.png)\n\n스프링에서 `gzip` 방식 압축을 적용하려면 다음과 같이 작성하면 됩니다.\n\n```yaml\nserver:\n  compression:\n    enabled: true\n    min-response-size: ${압축 기준 크기} (default: 2048 (byte))\n    mime-types: ${압축을 지원하는 Content type} (default: text/html, text/xml, text/plain, text/css, text/javascript, application/javascript, application/json, application/xml)\n    \n```\n\n> 참고: [HTTP Response Compression - Spring Boot Reference](https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#howto.webserver.enable-response-compression)\n\n### 청크 전송 코딩\n\n데이터를 **여러 청크로 분할해 전송하는 방식**의 알고리즘입니다.\n\n이를 통해 전체 콘텐츠가 준비될 때까지 기다리지 않고 일부분을 먼저 송신 및 수신할 수 있습니다.<br>\n특히 큰 파일이나 동영상 스트리밍과 같은 상황에서 유용하게 적용됩니다.\n\n이를 위한 헤더로 클라이언트가 사용하는 `TE`, 서버가 사용하는 `Transfer-Encoding` 헤더가 있다.\n\n- TE\n  - 브라우저에서 수신할 수 있는 전송 인코딩 방식을 나타낸다\n  - 예) `TE: trailers, chunked` \n\n\n- Transfer-Encoding\n  - HTTP 메시지에 적용된 전송 인코딩 방식을 나타낸다\n  - 예) `Transfer-Encoding: chunked`\n\n> 참고: [Transfer-Encoding - MDN](https://developer.mozilla.org/ko/docs/Web/HTTP/Headers/Transfer-Encoding)\n\n## 여러 형식의 데이터를 보내는 멀티파트\n\nHTTP는 [MIME 프로토콜](https://developer.mozilla.org/ko/docs/Web/HTTP/Basics_of_HTTP/MIME_types)을 지원하기에 다양한 형식의 파일을 전송할 수 있습니다.\n\n### multipart/form-data\n\n흔히 브라우저에 파일을 업로드하고 이를 서버로 전송하는 경우가 있습니다.<br>\n이를 위한 html은 다음과 같을 수 있습니다.\n\n```html\n<form action=\"/\" method=\"post\" enctype=\"multipart/form-data\">\n  <input type=\"text\" name=\"description\" value=\"some text\" />\n  <input type=\"file\" name=\"myFile\" />\n  <button type=\"submit\">Submit</button>\n</form>\n```\n\n위 폼을 통해 서버가 수신하는 HTTP Request는 다음과 같습니다.\n\n```text\nPOST /foo HTTP/1.1\nContent-Length: 68137\nContent-Type: multipart/form-data; boundary=---------------------------974767299852498929531610575\nContent-Disposition: form-data; name=\"description\"\n---------------------------974767299852498929531610575\n\nsome text\n\n---------------------------974767299852498929531610575\nContent-Disposition: form-data; name=\"myFile\"; filename=\"foo.txt\"\nContent-Type: text/plain\n\n(content of the uploaded file foo.txt)\n\n---------------------------974767299852498929531610575--\n```\n\n`boundary` 속성으로 각각의 멀티파트 엔티티에 대해 구분합니다.\n\n그리고 마지막에는 boundary 뒤에 `--`를 붙여 마무리합니다.\n\n### multipart/byteranges\n\n이미지와 데이터를 특정 지점부터 원하는 만큼만 받을 수 있습니다.\n\n이를 _resume_ 기능이라고 하고, _Range Request_라고 부릅니다.\n\n```text\nGET /tip.jpg HTTP/1.1\nHost: www.usagidesing.jp\nRange: bytes=5001-10000\n```\n\n```text\nHTTP/1.1 206 Partial Content\nDate: Fri, 13 Jul 2012 04:39:17 GMT\nContent-Range: bytes 5001-10000/10000\nContent-Length: 5000\nContent-Type: image/jpeg\n```\n\n여기서 중요한 점은 Http Request에서 `Range` 헤더를 통해 원하는 만큼의 부분을 요청한다는 것과\n\nHttp Response에서 `Content-Range`를 통해 응답에 대한 정보를 담는다는 것입니다.<br>\n그리고 응답 코드로 `206 Partial Content`를 포함합니다.\n\n> 참고: [Content-Range - MDN](https://developer.mozilla.org/ko/docs/Web/HTTP/Headers/Content-Range)\n\n## 콘텐츠 협상 (Content Negotiation)\n\n동일한 URI에서 리소스의 서로 다른 버전을 제공하기 위해 사용되는 매커니즘입니다.\n\n브라우저가 사용자에게 가장 fit한 리소스를 제공하기 위해 Http Reqeust를 보낼 때 정보를 담고 서버는 그에 따라 다른 값을 응답할 수 있습니다.<br>\n예를 들어 문서의 언어, 이미지 포맷 혹은 콘텐츠 인코딩 방식 등에 적용될 수 있습니다.\n\n콘텐츠 협상에는 두 가지 방식이 있습니다.\n\n### 서버 주도적 협상\n\n협상의 주도권이 서버에 있습니다.\n\n브라우저는 협상에 필요한 정보들을 헤더에 담아 전송하면 서버는 이를 통해 가장 적합한 방식으로 리소스를 응답합니다.\n\n브라우저는 `Accept`, `Accept-Language`, `Accept-Charset`, `Accept-Encoding` 헤더를 통해 필요한 정보를 보냅니다.\n\n![서버 주도적 콘텐츠 협상](server-driven-negotiation.png)\n\n### 클라이언트 주도적 협상\n\n협상의 주도권이 클라이언트에게 있습니다.\n\n클라이언트가 서버에 요청을 보내면 서버는 응답 가능한 모든 버전의 리소스들에 대한 링크를 포함하여 응답합니다.<br>\n그러면 브라우저는 이를 통해 원하는 리소스에 해당하는 URL로 재요청을 보내는 방식입니다.\n\n![클라이언트 주도적 콘텐츠 협상](client-driven-nogotiation.png)\n\n### Reference\n> - [그림으로 배우는 Http & Network Basic](https://m.yes24.com/Goods/Detail/15894097)"},{"excerpt":"들어가며 이 글은 그림으로 배우는 Http & Network Basic을 읽고 학습한 내용을 정리한 글입니다. 이번 글에서는 HTTP 프로토콜에 대해 작성하겠습니다. HTTP 프로토콜 HyperText Transfer Protocol, HTTP 프로토콜은 HTML 문서와 같은 리소스를 가져올 수 있도록 하는 프로토콜입니다. 기본적으로 리소스를 요청하는 클…","fields":{"slug":"/02-http-protocol/"},"frontmatter":{"date":"January 17, 2024","title":"2장. 간단한 프로토콜 HTTP","tags":["Network"]},"rawMarkdownBody":"\n## 들어가며 \n\n이 글은 [그림으로 배우는 Http & Network Basic](https://m.yes24.com/Goods/Detail/15894097)을 읽고 학습한 내용을 정리한 글입니다.\n\n이번 글에서는 HTTP 프로토콜에 대해 작성하겠습니다.\n\n## HTTP 프로토콜\n\nHyperText Transfer Protocol, HTTP 프로토콜은 **HTML 문서와 같은 리소스를 가져올 수 있도록 하는 프로토콜**입니다.\n\n기본적으로 리소스를 요청하는 **클라이언트**와 리소스를 응답하는 **서버**가 존재합니다.\n\n클라이언트가 보내는 _Http Request_와 서버가 응답하는 _Http Response_ 를 통해 요청과 응답이 이루어집니다.\n\n이 Http Request와 Http Response는 _Http 메시지_ 라고 합니다.<br>\nHTTP 메시지는 ASCII로 인코딩된 동일한 구조의 텍스트입니다.\n\nHTTP 메시지를 정해진 구조 내에서 자유롭게 확장할 수도 있습니다.<br>\n(ex. 커스텀 헤더 추가)\n\n\n\n### Http Request의 구조\n\nHttp Request는 아래와 같은 구조로 되어있습니다.\n\n![Http Request 구조](frame_of_http_request.png)\n\n- Start Line\n  - Method: [HTTP 메서드](https://developer.mozilla.org/ko/docs/Web/HTTP/Methods)가 작성됩니다. (GET, POST, PUT, DELETE 등)\n  - Path: 리소스의 Path가 작성됩니다.\n  - Version: 사용한 Http Protocol의 버전이 작성됩니다.\n\n- [Headers](https://developer.mozilla.org/ko/docs/Web/HTTP/Headers)\n  - 부가적인 정보를 포함합니다.\n  - `:`를 구분자로 하여 헤더의 이름과 값이 작성됩니다.\n    - `:` 뒤에 공백은 무시됩니다.\n\n- Blank Line\n  - `Headers`와 `Body`를 구분짓기 위한 한 줄의 공백입니다.\n\n- Body\n  - 리소스를 등록(`POST`)하거나 업데이트(`PUT`, `PATCH`)할 때 필요한 정보를 담는 공간입니다. (반드시 Body에만 담아야하는 것은 아닙니다.)\n  - 경우에 따라 내용이 포함되지 않을 수 있습니다.\n\n### Http Response의 구조\n\nHttp Response의 구조는 다음과 같습니다.\n\n![Http Response 구조](frame_of_http_response.png)\n\n- Status Line\n  - Version: 사용한 Http Protocol의 버전이 작성됩니다.\n  - [Status Code](https://developer.mozilla.org/ko/docs/Web/HTTP/Status): 응답 결과를 숫자와 텍스트로 나타냅니다. \n\n- Headers\n  - Http Request의 Headers와 동일합니다.\n\n- Blank Line\n  - Http Request의 Blank Line과 동일합니다.\n\n- Body\n  - 리소스에 대한 정보를 Body에 담아 응답한다.\n\n## HTTP 프로토콜은 Stateless 하다\n\nHTTP 프로토콜은 기본적으로 Http Request와 Http Response를 저장하지 않습니다.<br>\n따라서 새로운 Http Reqeust가 보내질 때마다 이에 응하는 Http Response가 응답됩니다.<br>\n같은 요청을 수만 번 보내면 응답이 수만번 온다는 의미입니다!\n\n이러한 특징을 우리는 Stateless 하다고 부릅니다.\n\n하지만 유저의 로그인 상태를 유지해야하는 요구 사항이 생겼고, 이에 따라 *Cookie* 를 이용해 해결합니다.\n\n클라이언트가 로그인에 성공하면, 서버는 `Set-Cookie` 헤더에 로그인 상태를 유지할 수 있는 정보를 담아 응답합니다.<br>\n응답을 받은 클라이언트는 정보를 가지고 있다가 다시 서버로 요청을 보낼 때 `Cookie` 헤더에 정보를 담아 요청합니다.<br>\n요청을 받은 서버는 `Cookie` 헤더에 담긴 정보를 통해 어떤 사용자인지 확인하는 방식으로 진행됩니다.\n\n## 지속 연결, 파이프라인\n\n과거 HTTP/1.0 시절에는 요청과 응답 각각에 TCP 연결(3 way handshake)이 필요했습니다.<br>\n이는 요청의 수가 많아질수록 오버헤드가 많이 발생한다는 문제점이 존재합니다.\n\n이를 해결하기 위해 HTTP/1.1부터는 *지속 연결* 과 *파이프라인* 개념을 도입하여 더욱 효율적으로 통신을 할 수 있도록 합니다.\n\n[`Connection` 헤더](https://developer.mozilla.org/ko/docs/Web/HTTP/Headers/Connection)를 통해 위 기술을 적용합니다.\n\n### 지속 연결\n\n지속 연결은 **어느 한 쪽이 명시적으로 연결을 종료하지 않는 이상, TCP 연결을 계속 유지**하는 개념입니다.\n\n이를 통해 **한 번의 TCP Connection을 통해 많은 요청과 응답이 존재**할 수 있게 됩니다.\n\n### 파이프라인\n\n파이프라인은 Request를 보낸 후에 Response를 받기 전까지 기다린 뒤에야 다른 Request를 보내던 것을, Response를 기다리지 않고 바로 Request를 보낼 수 있도록 하는 개념입니다.  \n\n이는 하나의 html 페이지에 이미지 파일이 10개가 있다고 가정했을 때, 얼마나 효율적인 방식으로 통신하는지 이해할 수 있습니다.\n\n### Reference\n> - [그림으로 배우는 Http & Network Basic](https://m.yes24.com/Goods/Detail/15894097)\n> - [HTTP 개요 - MDN](https://developer.mozilla.org/ko/docs/Web/HTTP/Overview)"},{"excerpt":"들어가며 이번 글은 AWS EC2 인스턴스를 생성하고 수작업으로 Spring Boot를 실행시키는 방법을 정리한 글입니다. 개발 환경은 ,  입니다. 따라서 윈도우의 경우 인스턴스에 접속하는 방법이 약간 다릅니다.\n윈도우에서 접속하는 방법은 해당 글을 참고해주세요. AWS 계정 생성 우선 가장 먼저 AWS 계정을 생성해줍니다. 기존에 있다면 그대로 사용해…","fields":{"slug":"/deploy-spring-boot-in-aws-ec2-manually/"},"frontmatter":{"date":"January 16, 2024","title":"[Infra] AWS EC2에 Spring Boot 수작업으로 배포하기","tags":["Infra","EC2","Deploy"]},"rawMarkdownBody":"\n## 들어가며\n\n이번 글은 AWS EC2 인스턴스를 생성하고 수작업으로 Spring Boot를 실행시키는 방법을 정리한 글입니다.\n\n개발 환경은 `MacOS`, `Warp(Terminal)` 입니다.\n\n따라서 윈도우의 경우 인스턴스에 접속하는 방법이 약간 다릅니다.<br>\n윈도우에서 접속하는 방법은 [해당 글](https://wookim789.tistory.com/34)을 참고해주세요.\n\n## AWS 계정 생성\n\n우선 가장 먼저 AWS 계정을 생성해줍니다.\n\n기존에 있다면 그대로 사용해주시고 없다면 새로 회원가입 해주세요.\n\n이 부분은 따로 포함하지 않겠습니다.\n\n## EC2 인스턴스 생성\n\n이제 바로 EC2 인스턴스를 생성해봅시다.\n\n그 전에 **리전을 서울로** 설정합니다.\n\n![리전 선택](select_region.png)\n\n> 참고: [AWS - 리전 및 가용 영역](https://aws.amazon.com/ko/about-aws/global-infrastructure/regions_az/)\n\n그리고 좌측 상단 검색창에 `EC2` 를 검색해서 클릭하면 아래와 같이 EC2 대시보드 페이지가 나옵니다.\n\n![EC2 메인 페이지](dashboard_page_ec2.png)\n\n그리고 화면에 유일하게 주황색으로 강조되어 있는 *인스턴스 시작* 을 누르고 생성 페이지로 이동합니다.\n\n![EC2 생성 페이지](creation_page_ec2.png)\n\n이제 하나하나 설정해보겠습니다.\n\n---\n\n### 이름\n\n**EC2 인스턴스의 이름**입니다.\n\n저의 경우 `ec2-${project_name}` 같이 짓습니다.\n\n ![EC2 이름 짓기](ec2_name.png)\n\n### AMI 설정\n\n**인스턴스의 운영체제**입니다.\n\n아래와 같이 Linux 계열인 `Ubuntu`를 설정하고 아키텍처는 `64비트(Arm)`을 설정합니다.\n\n![AMI 설정](ec2_ami.png)\n\n> [CPU 아키텍처란?](https://kdkdhoho.github.io/what-is-ami-architecture-of-ec2)\n\n### 인스턴스 유형\n\n**EC2 인스턴스의 성능**입니다.<br>\n서버를 띄우는 목적과 비용에 따라 선택하면 됩니다.\n\n본문에서는 프리티어를 선택하도록 하겠습니다.\n\n![인스턴스 유형 선택](type_of_instance.png)\n\n> [프리티어?](https://aws.amazon.com/ko/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=*all&awsf.Free%20Tier%20Categories=*all)\n\n### 키 페어\n\n**로컬 PC에서 인스턴스로 원격 접속할 때 사용**되는 키 페어입니다.\n\n`새 키 페어 생성`을 눌러 다음과 같이 설정합니다.\n\n![새 키 페어 생성](create_key_pair.png)\n\n그리고 생성한 키 페어를 선택합니다.\n\n> ### ⚠️ 주의 ⚠️<br>\n> 키 페어는 한번 생성하면 재발급이 되지 않습니다.<br>\n> 따라서 개인 로컬 PC에 잘 저장해둬야 합니다.<br>\n> 그리고 해당 키로 언제든지 인스턴스로 접속 가능하기에 보안에 주의해야 합니다.\n\n### 네트워크 설정\n\n**인스턴스에 접속을 허용할 네트워크 범위**를 설정합니다.\n\n우선 우측 상단의 `편집`을 눌러줍니다.\n\n그리고 우선 다음과 같이 설정합니다.\n\n![1차 네트워크 설정](setting_network.png)\n\n그 다음 중요한 **인바운드 보안 그룹 규칙**에 대해 자세히 살펴보겠습니다.\n\n인바운드 보안 그룹 규칙에 따라 인스턴스가 허용할 트래픽의 범위를 지정합니다.<br>\n다시 말해, **인스턴스에 접근 가능한 네트워크 및 장소를 설정**하는 것입니다.\n\n우선 인스턴스에 ssh 연결은 필수이니 ssh 먼저 설정하겠습니다.<br>\n이때, 특정 장소의 네트워크를 통해서만 접근할 수 있도록 하려면 아래와 같이 소스 유형 - 원본에 해당 장소 네트워크의 IP를 입력해줍니다.\n\n![특정 네트워크 IP 설정](setting_specific_ip.png)\n\n만약 그렇지 않고 모든 장소에서 들어갈 수 있게 하려면 `위치 무관`으로 설정해주세요.\n\n그리고 `보안 그룹 규칙 추가`를 눌러 아래와 같이 새로운 허용 범위를 설정합니다.<br>\n추후 실행시킬 스프링에 접속할 수 있도록 미리 설정하는 것입니다.\n\n![8080 포트 허용](second_security_group_rule_setting.png)\n\n> 지금은 간단히 ssh와 TCP 연결을 모든 곳에 설정해두었습니다.<br>\n> 하지만 보안을 위해 특정 장소에서만 접속이 가능하게 하려면 인바운드 보안 그룹 규칙을 설정해서 접속 가능 범위를 좁혀야 합니다.\n\n### 스토리지 구성\n\n디스크의 크기를 설정합니다.\n\n이 부분은 기본 설정값 그대로 진행하겠습니다.\n\n그리고 `인스턴스 생성`을 누르면 끝입니다.\n\n---\n\n## 접속\n\n드디어 EC2 인스턴스가 생성됐습니다.\n\n이제 해당 인스턴스로 접속해서 스프링을 실행해보겠습니다.\n\n터미널을 띄우고, 아래 명령어를 입력합니다.\n\n```\n> chmod 400 ${key_path}\n> ssh -i ${key_path} ${instance_public_ip}\n```\n\n- `chmod 400 ${key_path}`: 키 페어 파일에 대한 운영체제 내의 접근 권한을 설정합니다.\n    > 참고\n    > - [SSH 접속 시 UNPROTECTED PRIVATE KEY FILE! 에러 해결](https://www.lesstif.com/lpt/ssh-unprotected-private-key-file-80249001.html)<br>\n    > - [리눅스 chmod 명령어 사용법 - 파일 권한 변경](https://recipes4dev.tistory.com/175)\n\n- `${key_path}`: 인스턴스를 생성할 때 만든 키 페어의 경로입니다.\n- `${instance_public_ip}`: 생성한 인스턴스의 퍼블릭 IPv4 주소입니다. 아래 그림을 통해 확인하면 됩니다. 참고로 인스턴스를 중지하고 다시 키면 IP 주소가 바뀝니다.\n![인스턴스의 퍼블릭 IP 주소](public_ip_address.png)\n\n위 명령어를 입력하고 아래와 같은 화면이 뜨면 성공입니다.\n\n![접속 성공](success_connect_to_instance.png)\n\n## 스프링 실행하기\n\n이제 스프링을 실행해보겠습니다.\n\n### JDK 설치\n\n그 전에 먼저 JDK를 설치합니다.\n\n저는 JDK 17을 설치하겠습니다.\n\n아래 명령어를 입력합니다.\n\n```text\n> wget -O - https://apt.corretto.aws/corretto.key | sudo gpg --dearmor -o /usr/share/keyrings/corretto-keyring.gpg && \\\necho \"deb [signed-by=/usr/share/keyrings/corretto-keyring.gpg] https://apt.corretto.aws stable main\" | sudo tee /etc/apt/sources.list.d/corretto.list\n\n> sudo apt-get update; sudo apt-get install -y java-17-amazon-corretto-jdk\n```\n\n대충 설치되는 화면이 완료되면 명령어 `java -version`을 통해 확인해봅니다.\n\n![JDK 설치 완료](installed_jdk.png)\n\n### 스프링 실행하기\n\n이제 진짜 스프링을 실행하겠습니다.\n\n우선 프로젝트 레포지토리를 EC2 내에 clone 합니다.<br>\nclone이 완료되면 `ls` 명령어를 통해 성공적으로 clone 됐는지 확인합니다.\n\n![레포지토리 clone 이후 확인](cloned-repository.png)\n\n그 다음 프로젝트 디렉터리로 들어가 명령어 `./gradlew bootJar`를 통해 Build 합니다.\n\n![스프링 부트 Build](build-spring-boot.png)\n\n> `gradle build` 명령어로도 프로젝트를 빌드할 수 있습니다. 하지만 테스트와 같은 작업이 포함됩니다.<br>\n> 따라서 단순히 프로젝트 build만 하고 싶으면 Spring Boot가 지원하는 명령어인 `gradlew bootJar`를 사용합니다.<br>\n> <br>\n> 참고<br>\n> [Stack Overflow - gradle build와 bootJar의 차이](https://stackoverflow.com/questions/64747475/difference-between-gradle-build-and-gradle-bootjar)<br>\n> [Spring Boot 공식문서](https://docs.spring.io/spring-boot/docs/current/gradle-plugin/reference/htmlsingle/)<br>\n> [Gradle 공식문서 - Build Lifecyle](https://docs.gradle.org/current/userguide/build_lifecycle.html)\n\n그럼 build의 결과물로 `build` 디렉터리가 생성됩니다.\n\n![빌드 성공](success_project_build.png)\n\n### jar 파일 실행하기\n\n이제 build 결과물인 jar 파일을 실행하면 끝입니다.\n\n`> cd build/libs` 명령어를 통해 해당 디렉터리로 들어가면 아래처럼 jar 파일이 생성되어 있습니다.\n\n![생성된 jar 파일](exists_jar_file.png)\n\n> 아마 jar 파일 이름이 `${project_name}-0.0.1-SNAPSHOT.jar` 으로 되어있을 수 있습니다.<br>\n> 이는, `build.gradle`에 작성된 `version`에 따라 이름이 POSTFIX로 붙어서 나오는 겁니다.<br>\n> 불편하다면 제거하고 다시 build하면 위처럼 깔끔하게 나옵니다.\n\n이제 `java -jar ${project_name}.jar` 명령어 통해 실행합니다. \n\n![스프링 애플리케이션 실행](run_spring.png)\n\n위와 같이 정상 작동되면 성공입니다!\n\n이제 브라우저를 통해 스프링으로 접속해보고 아래와 같은 화면이 뜨면 성공적으로 EC2에 스프링 애플리케이션을 배포한겁니다.\n\n![서버 접속 성공](success_connect_server.png)\n\n## 마치며\n\n이번에는 직접 인스턴스에 접속해서 프로젝트를 clone하고 build하고 jar 파일을 실행했습니다.\n\n하지만 배포를 할 때마다 이 작업을 매번 수행하는 것은 개발을 지속하기 힘들게 만들 것입니다.\n\n따라서 다음엔 배포를 조금 더 간편하게 하기 위해 셸 스크립트를 작성해서 배포해보겠습니다.\n\n감사합니다.\n\n### Reference\n> - [리눅스 chmod 명령어 사용법 - 파일 권한 변경](https://recipes4dev.tistory.com/175)\n> - [EC2에 JDK 설치](https://docs.aws.amazon.com/corretto/latest/corretto-17-ug/downloads-list.html)\n> - [SSH 접속 시 UNPROTECTED PRIVATE KEY FILE! 에러 해결](https://www.lesstif.com/lpt/ssh-unprotected-private-key-file-80249001.html)\n> - [EC2 인스턴스 유형](https://aws.amazon.com/ko/ec2/instance-types/)"},{"excerpt":"들어가며 EC2에 스프링 배포하기 글을 작성하다가 EC2 인스턴스를 생성할 때 AMI를 설정하게\n됩니다. 이때 선택할 수 있는 아키텍처는 아래와 같이 과  두 가지로 나뉩니다.  문득, 이 둘의 차이에 대해 궁금해져 학습하고 정리해봤습니다. Amazon Machine Image? 우선 Amazon Machine Image에 대해 알아보겠습니다. 공식 문서…","fields":{"slug":"/what-is-ami-architecture-of-ec2/"},"frontmatter":{"date":"January 16, 2024","title":"CPU 아키텍처란?","tags":["operation-system","CPU Architecture"]},"rawMarkdownBody":"\n## 들어가며\n\n[EC2에 스프링 배포하기](https://kdkdhoho.github.io/deploy-spring-boot-in-aws-ec2-manually/) 글을 작성하다가 EC2 인스턴스를 생성할 때 AMI를 설정하게\n됩니다.\n\n이때 선택할 수 있는 아키텍처는 아래와 같이 `x86`과 `Arm` 두 가지로 나뉩니다.\n\n![img.png](type_of_cpu_architecture.png)\n\n문득, 이 둘의 차이에 대해 궁금해져 학습하고 정리해봤습니다.\n\n## Amazon Machine Image?\n\n우선 Amazon Machine Image에 대해 알아보겠습니다.\n\n[공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/AMIs.html)에 따르면 다음과 같습니다.<br>\n> 인스턴스를 시작하는 데 필요한 정보를 제공하는 AWS에서 지원하는 이미지다.\n\n즉, 운영체제를 '도커의 이미지' 처럼 EC2 인스턴스에 설치하는 것으로 이해하면 되겠습니다.\n\n## 아키텍처?\n\n그럼 여기서의 아키텍처는 무엇을 의미하며 64비트(x86), 그리고 64비트(Arm)은 각각 무엇을 의미하는걸까요?\n\n이것들은 모두 _CPU 아키텍처_ 를 의미하는 것입니다.\n\nCPU 아키텍처란, 간단히 말해 CPU를 만드는 설계 방법입니다.\n\n이 CPU를 만드는 회사는 다양하고 각 회사마다 CPU를 만드는 방법이 조금씩 다른데요.\n\n가장 유명한 CPU 제조 회사인 [Intel](https://www.intel.com/content/www/us/en/homepage.html) 사가 만드는 CPU의 아키텍처는 `x86` 라고 부릅니다.<br>\n그리고 타사인 [ARM](https://www.ARM.com/) 사가 만드는 CPU의 아키텍처가 바로 `Arm` 입니다.\n\n이 두 CPU 아키텍처에는 큰 차이가 있습니다.\n\n## x86 vs Arm\n\n| 아키텍처 |     지원 OS     |      타겟      | 에너지 사용량 | 발열 | 비용 | 트랜지스터 수 |\n|:----:|:-------------:|:------------:|:-------:|:--:|:--:|:-------:|\n| x86  | 윈도우, 리눅스, 맥 등 |      PC      |   높음    | 높음 | 높음 |   많음    |\n| Arm  |   리눅스 계열 등    | 스마트폰, 저전력 PC |   낮음    | 낮음 | 낮음 |   적음    |    \n\nx86은 CPU 내에 상대적으로 더 많은 트랜지스터가 존재합니다.<br>\n따라서 **처리 성능은 뛰어납니다.** 하지만 그만큼 **에너지 사용량이 높고 발열량이 높습니다.**<br>\n그에 따라 **비용도 상대적으로 비쌉니다.**\n\nArm은 x86에 비해 **더 싸고 발열도 낮습니다.**<br>\n본래 Arm의 설계 목적은 스마트폰 같이 소형 전자 장치에 들어갈 CPU를 목적으로 설계되었습니다.<br>\n따라서 더 작고, 배터리 수명이 길고, **저비용**에 초점을 맞춰 설계합니다.\n\n## 그럼 어떤 아키텍처를 선택하면 좋을까?\n\n목적은 클라우딩 컴퓨터인 EC2에 들어갈 아키텍처를 선택하는 것입니다.\n\n클라우딩 컴퓨터는 \"저비용에 고효율\"을 가지는 대표적인 기술입니다.<br>\n또한 물리적 자원이 아니다보니 CPU 발열을 냉각시켜야 하는 제약이 존재하지 않습니다.\n\n추가로 아래 사진을 보시겠습니다.\n\n![x86 선택](x86.png)\n\n![Arm 선택](arm.png)\n\nx86을 선택하면 기본적으로 _t2.micro_ 타입이 설정됩니다.<br>\n반면, Arm을 선택하면 _t4g.micro_ 타입이 설정됩니다.\n\n이 두 인스턴스 타입의 차이에 대해 알아보겠습니다.\n\n![t2 유형](type_of_t2.png)\n\n![t4g 유형](type_of_t4g.png)\n\n두 타입 모두 프리티어이면서 동일한 AMI에서 설정되는 인스턴스 타입임에도 불구하고, CPU 아키텍처의 차이에 따라 성능과 비용의 차이가 있음을 확인할 수 있습니다.\n\n결론적으로, 저비용 고효율을 위해서라면 `Arm 아키텍처`를 선택하는 것이 바람직해보입니다.\n\n## 64비트는 무엇이지?\n\n그런데 64비트는 무엇일까요?\n\n여기서의 '비트'는 CPU가 가지는 *레지스터*의 크기를 의미합니다.\n\nCPU가 프로그램 명령어를 처리하기 위해 자체적으로 저장소를 가집니다.<br>\n이 저장소를 레지스터라고 합니다.<br>\n레지스터는 용량은 매우 작은 대신 속도는 매우 빠른 특징을 가집니다.\n\n따라서 저장소의 크기가 클수록 더 많은 데이터를 이용해 CPU가 명령어를 처리하는 능력이 향상됩니다.\n\n참고로 32비트도 존재합니다만 64비트와의 차이는 다음과 같습니다.<br>\n32비트는 2^32 = 4,294,967,296 bit ≒ **4GB**<br>\n64비트는 2^64 = 18,446,744,073,709,551,616 bit ≒ **16EB** (= 4GB x 4GB)\n\n현대의 많은 CPU는 대부분 64비트로 생산되고 있습니다.\n\n## 마치며\n\n지금까지 CPU 아키텍처를 선택하는 데에 있어 필요한 정보들을 알아보았습니다.\n\n처음 접한 내용이다보니 틀리거나 빠진 내용이 있을 수 있습니다.<br>\n만약 정보의 수정이 필요하다면 언제든 댓글 남겨주시길 바랍니다.\n\n감사합니다.\n\n### Reference\n\n> - [AMR vs x86 - Red Hat 공식문서](https://www.redhat.com/en/topics/linux/Arm-vs-x86)\n> - https://velog.io/@480/이제는-개발자도-CPU-아키텍처를-구분해야-합니다\n> - [CPU 32비트와 64비트 차이, 모르고 사용하면 낭비다 - 이포커스](https://www.e-focus.co.kr/news/articleView.html?idxno=2638858)\n> - https://dev.to/aws-builders/data-points-you-need-to-know-about-Arm-for-your-application-code-migration-5c0f\n> - https://youtu.be/ndXKHgFIIL4"},{"excerpt":"들어가며 이번 글에서는 브랜치 네이밍을 이용해 자동으로 커밋 타입과 이슈 번호를 추가하는 방법에 작성하겠습니다. 글의 대상은 다음과 같습니다. 브랜치 네이밍 컨벤션과 커밋 메시지 컨벤션을 정하려는 팀 커밋 메시지 컨벤션에 따라 매번 커밋 메시지를 작성하는 게 귀찮거나 자주 까먹는 사람 이 글을 읽고 나면 커밋 메시지를 라고만 했을 경우, 저장된 커밋 메시…","fields":{"slug":"/add-feat-and-issue-number-automatically/"},"frontmatter":{"date":"January 12, 2024","title":"[git&github] IntelliJ에서 커밋 메시지 작성 시, 자동으로 커밋 타입과 이슈 번호 추가하기","tags":["git","github"]},"rawMarkdownBody":"\n## 들어가며\n\n이번 글에서는 브랜치 네이밍을 이용해 자동으로 커밋 타입과 이슈 번호를 추가하는 방법에 작성하겠습니다.\n\n글의 대상은 다음과 같습니다.\n\n- 브랜치 네이밍 컨벤션과 커밋 메시지 컨벤션을 정하려는 팀\n- 커밋 메시지 컨벤션에 따라 매번 커밋 메시지를 작성하는 게 귀찮거나 자주 까먹는 사람\n\n### 이 글을 읽고 나면\n\n커밋 메시지를 `message`라고만 했을 경우, 저장된 커밋 메시지의 결과는 `feat: message (#1)`과 같이 작성될 것입니다.\n\n![커밋할 때](before.png)\n\n![커밋 이후 결과](after.png)\n\n### 참고\n\n시작하기 앞서 `커밋 타입`은 `feat`, `refactor`, `chore`와 같은 [Conventional Commits](https://www.conventionalcommits.org/en/v1.0.0/)의 `type`을 의미합니다.\n\n개발 환경은 다음과 같습니다.\n\n- MacOS 14.2.1(23C71)\n- IntelliJ 2023.2.5 (Ultimate Edition)\n\n## 컨벤션 정하기\n\n해당 작업을 수행하기 전 _브랜치 네이밍 컨벤션_ 과 _커밋 메시지 컨벤션_ 을 먼저 정해야 합니다.\n\n이번 글에선 브랜치 네이밍 컨벤션을 `type/issue_number` 와 같은 형식으로, 커밋 메시지 컨벤션을 `type: message (#1)`와 같이 설정하겠습니다.<br>\n> 예) 브랜치 네이밍 컨벤션 : `feat/1`<br>\n> 커밋 메시지 컨벤션 : `feat: 기능 구현 (#1)`\n\n## 스크립트 작성\n\n프로젝트의 루트 위치 아래에 `.githooks` 디렉터리를 만들고 그 안에 `commit-msg`를 만들어주세요.<br>\n`commit-msg`는 단순히 file 형태로 만들면 됩니다.\n\n![스크립트 경로](script_path.png)\n\n---\n\n그 다음 스크립트 코드를 작성하겠습니다.<br>\n코드에 대한 설명은 주석을 참고해주세요.\n\n```shell\n#!/bin/sh\n\n# 본 스크립트는 브랜치 명을 토대로 개발자가 작성한 커밋 메시지에 자동으로 PREFIX와 POSTFIX 문자열을 추가하는 스크립트입니다.\n# 브랜치 명은 \"feat/14\" 와 같이 \"{type}/{issue_number}\" 의 형식입니다.\n\nCOMMIT_MESSAGE_FILE_PATH=$1 # 작성한 커밋 메시지가 저장된 File의 경로 추출\nMESSAGE=$(cat \"$COMMIT_MESSAGE_FILE_PATH\") # 작성한 커밋 메시지 추출\n\n# 커밋 메시지가 존재하지 않은 경우, 스크립트 종료\nif [[ $(head -1 \"$COMMIT_MESSAGE_FILE_PATH\") == '' ]]; then\n  exit 0\nfi\n\n# 브랜치 이름에서 '/' 이전의 문자열만 남긴다. 만약 명시적으로 type을 작성한 경우 작성하지 않는다.\nif [[ $MESSAGE != *:* ]]; then\n  PREFIX=\"$(git branch | grep '\\*' | sed 's/\\* //' | sed 's/\\([^/]*\\).*/\\1/'):\"\nfi\n\n# 브랜치 이름에서 '/' 이후의 문자열만 남긴다. '/'가 없다면 브랜치 전체 이름이 POSTFIX가 된다.\n# POSTFIX의 첫 번째 '-' 앞 뒤의 문자열만 포함한다. '-'가 없다면 변경은 없다.\n# 명시적으로 이슈 번호를 작성한 경우 작성하지 않는다.\nif [[ $MESSAGE != *#* ]]; then\n  POSTFIX=\"(#$(git branch | grep '\\*' | sed 's/* //' | sed 's/^.*\\///' | sed 's/^\\([^-]*-[^-]*\\).*/\\1/'))\"\nfi\n\nprintf \"%s %s %s\" \"$PREFIX\" \"$MESSAGE\" \"$POSTFIX\" > \"$COMMIT_MESSAGE_FILE_PATH\"\n```\n\n## git hooks 설정\n\ngit hooks를 설정해야 작성한 `/.githooks/commit-msg` 스크립트를 적용할 수 있습니다.\n\n레포지토리에서 clone한 프로젝트를 로컬 환경에서 설정하기 위해\n\n`git config core.hooksPath .githooks` 명령어를 입력해야 합니다.\n\n이 외에도 권한 부여 명령어를 한꺼번에 편하게 작성하게 위해 Mac(or Linux) OS에 기본적으로 제공되는 [Makefile](https://www.gnu.org/software/make/)를 이용하겠습니다.\n\n---\n\n프로젝트 루트 경로에 `Makefile`을 만들어줍니다.\n\n![Makefile 위치](Makefile.png)\n\nMakefile에 아래의 코드를 작성합니다.\n\n```shell\ninit:\n\tgit config core.hooksPath .githooks\n\tchmod +x .githooks/commit-msg\n\tgit update-index --chmod=+x .githooks/commit-msg\n```\n\n> - Makefile 문법 상, 띄어쓰기가 아닌 Tab을 입력해야 합니다.<br>\n> <br>\n> - `git config core.hooksPath .githooks`: git hooks의 위치를 .githooks로 변경합니다.<br>\n> - `chmod +x .githooks/commit-msg`: ./githooks/commit-msg 파일을 실행할 권한을 부여합니다.<br>\n> - `git update-index --chmod=+x .githooks/commit-msg`: git이 .githooks/commit-msg 파일을 실행할 권한을 부여합니다.\n> \n\n그 다음 터미널에서 `make` 명령어를 입력해 위 파일을 실행시켜줍니다.\n\n![결과](result_after_make.png)\n\n## 확인하기\n\n이제 끝났습니다.\n\n직접 커밋을 남겨보고 그 결과를 확인해보면 Prefix와 Postfix가 추가된 상태로 커밋되는 것을 확인할 수 있습니다!\n\n## 주의사항 및 적용 해제하기\n\n### 주의 사항\n\n만약 프로젝트를 레포지토리에서 새로 clone 한 경우 터미널에 `make`를 입력해야 스크립트가 적용됩니다.\n\n### 적용 해제하기\n\n적용을 해제하려면 터미널에 `git config core.hooksPath .git/hooks`를 입력해서 원래 설정으로 돌려놓으면 됩니다.\n\n## 마치며\n\n해당 글에서는 적용 방법에 대한 내용만 작성했습니다.\n\n만약 각 과정의 이유에 대해 알아보고 싶은 분들은 [해당 포스팅](https://blog.deercorp.com/commit-convention/)을 참고해주세요.\n\n### Reference\n> - https://blog.deercorp.com/commit-convention/\n> - https://feb-dain.tistory.com/7"},{"excerpt":"@Controller와 @RestController의 역할과 차이점에 대해 알아보기 전에, 둘의 근본이 되는 Controller의 역할에 대해 이야기하겠습니다. (여기서 Controller는 Spring MVC에서의 Controller를 의미합니다.) Controller의 역할은? 제가 생각하는 Controller의 역할은 다음과 같습니다. 사용자의 요청…","fields":{"slug":"/difference-of-controller-and-restcontroller/"},"frontmatter":{"date":"January 12, 2024","title":"[Spring] @Controller와 @RestController 비교하기","tags":["Spring","Spring MVC"]},"rawMarkdownBody":"\n@Controller와 @RestController의 역할과 차이점에 대해 알아보기 전에, 둘의 근본이 되는 Controller의 역할에 대해 이야기하겠습니다. (여기서 Controller는 Spring MVC에서의 Controller를 의미합니다.)\n\n## Controller의 역할은?\n\n제가 생각하는 Controller의 역할은 다음과 같습니다.\n\n1. **사용자의 요청을 처리한다.**\n- 입력되는 데이터를 처리한다.\n- 사용자의 요청에 따라 수행할 비즈니스 로직을 결정한다.\n2. **요청의 결과를 반환한다.**\n\n이때, 사용자의 요청을 URI와 매핑하여 각기 다르게 처리합니다.\n이를 위해 @RequestMapping 을 이용하여 어떤 URI 요청을 처리할지 결정합니다.\n\n## @Controller, @RestController 살펴보기\n\n우선 [@Controller docs](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/stereotype/Controller.html)를 먼저 살펴보겠습니다.\n\n> Indicates that an annotated class is a \"Controller\" (e.g. a web controller).\nThis annotation serves as a specialization of @Component, allowing for implementation classes to be autodetected through classpath scanning. It is typically used in combination with annotated handler methods based on the RequestMapping annotation.\n>\n> 이 애노테이션이 클래스가 \"컨트롤러\" 임을 나타냅니다. (예: 웹 컨트롤러)\n> 이 애노테이션은 클래스 경로 탐색을 통해 자동으로 탐지되도록 허용함으로써  [@Component](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/stereotype/Component.html)의 특별성을 제공합니다.\n> 보통 [@RequestMapping](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/bind/annotation/RequestMapping.html) 이 달린 메서드와 함께 사용됩니다.\n\n또한 내부에 Optional Element로서 `String value` 만을 가집니다.\n이 value는 **logical component name**을 가리킨다고 합니다. 즉, Controller 클래스가 Bean으로 등록될 때의 **이름**을 변경하고 싶을 때 사용하는 요소입니다.\n\n---\n\n다음으로 [@RestController docs](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/bind/annotation/RestController.html)를 살펴보겠습니다.\n\n> A convenience annotation that is itself annotated with @Controller and @ResponseBody.\nTypes that carry this annotation are treated as controllers where @RequestMapping methods assume @ResponseBody semantics by default.\n>\n> @RestController는 그 자체로 @Controller와 @ResponseBody가 함께 있는 편리한 애노테이션입니다. 이 애노테이션이 포함된 타입은 @RequestMapping가 달린 메서드에 기본적으로 @ResponseBody도 함께 의미합니다.\n>\n> NOTE: @RestController is processed if an appropriate HandlerMapping-HandlerAdapter pair is configured such as the RequestMappingHandlerMapping-RequestMappingHandlerAdapter pair which are the default in the MVC Java config and the MVC namespace.\n>\n> 주의: @RestController는 `RequestMappingHandlerMapping-RequestMappingHanlderAdapter`와 같은 기본적인 MVC Java Config와 MVC namespace를 가진 `HandlerMapping-HandlerAdapter` 쌍이 설정되었을 때 처리됩니다.\n\n@Controller와 마찬가지로 내부에 `String value` Element 만을 가집니다. 의미하는 바도 같습니다.\n하지만 큰 차이가 하나 있습니다. 바로 `@ResponseBody`를 포함한다는 것인데요.\n그렇다면 @ResponseBody가 무엇인지 알아보기 위해 [공식문서](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/bind/annotation/ResponseBody.html)를 살펴보겠습니다.\n\n> Annotation that indicates a method return value should be bound to the web response body.\n>\n>As of version 4.0 this annotation can also be added on the type level in which case it is inherited and does not need to be added on the method level.\n>\n> 이 애노테이션이 가리키는 메서드는 웹 응답 body에 값을 반환하는 의무를 가집니다.\n>\n> 버전 4.0부터는 type(class, interface, or enum)에 추가할 수도 있습니다. 이 경우 상속이 되며, 메서드에 추가할 필요가 없습니다.\n\n핵심은 **이 애노테이션이 가리키는 메서드는 웹 응답시 body부에 값을 반환한다** 입니다.\n추가로 'class, interface, enum'에 작성할 수 있고, 이 경우 메서드에 따로 명시를 안해줘도 된다는 것입니다.\n\n## 결론\n\n그렇다면 이렇게 결론지을 수 있을 것 같습니다.\n\n@Controller는 기본적으로 웹 MVC에서 Controller의 역할을 한다.\n@RestController는 @Controller에 @ResponseBody의 의미를 함께 가진다. 따라서 해당 컨트롤러 내 모든 메서드들은 반환 시, 웹 응답 body부에 값을 넣어 반환한다.\n\n즉, 사용자의 요청에 따라 **데이터의 형식**을 다르게 반환하고 싶을 때, 이 둘을 구분해서 사용해야 합니다.\n\n## 코드로 비교하기\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>This is tmpPage.html</title>\n</head>\n<body>\nThis is tmpPage.html\n</body>\n</html>\n```\n\n```java\n@Controller\npublic class JustController {\n\n    @GetMapping(\"/Controller\")\n    public String justController() {\n        return \"tmpPage\";\n    }\n\n    @GetMapping(\"/RestController\")\n    @ResponseBody\n    public String restController() {\n        return \"tmpPage\";\n    }\n}\n\n```\n\n두 메서드는 URI를 각각 \"/Controller\", \"/RestController\"에 매핑하여 요청을 처리합니다.\n\n이때 \"localhost:8080/Controller\"에 Get 방식으로 요청했을 때 아래와 같은 결과가 나옵니다.\n\n![](https://velog.velcdn.com/images/donghokim1998/post/e7e0d11d-cc8f-4189-9a61-8b2221543629/image.png)\n\n다음은 \"/RestController\" 에 Get 방식으로 요청했을 때의 결과입니다.\n\n![](https://velog.velcdn.com/images/donghokim1998/post/27ab1cd6-6895-4364-ad84-62bb6d187fb0/image.png)\n\n코드 상으로는 같은 \"tmpPage\" 를 반환합니다.\n하지만 @ResponseBody 가 붙지 않은 메서드인, `justController()`의 경우 Spring 내의 [ViewResolver](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/servlet/ViewResolver.html)에 의해 view로 변환이 됩니다.\n\n반면 `restController()`의 경우 문자열 \"tmpPage\"이 그대로 Body에 반환되는 것을 확인할 수 있습니다."},{"excerpt":"들어가며 이 글은 그림으로 배우는 Http & Network Basic 책을 읽으며 학습한 내용을 정리한 글입니다. 이번 글에서는 웹, HTTP, 그리고 네트워크의 배경과 간단한 배경 지식에 대해 작성하겠습니다. 웹, 그리고 네트워크를 공부해야 하는 이유 책에는 없는 내용이지만 네트워크를 공부해야 하는 이유에 대해 생각해봤다. 웹, 그리고 네트워크는 통신…","fields":{"slug":"/01-comprehension-about-web-and-network-basic/"},"frontmatter":{"date":"January 12, 2024","title":"1장. 웹과 네트워크의 기본에 대해 알아보자","tags":["Network"]},"rawMarkdownBody":"\n## 들어가며\n\n이 글은 [그림으로 배우는 Http & Network Basic](https://m.yes24.com/Goods/Detail/15894097) 책을 읽으며 학습한 내용을 정리한 글입니다.\n\n이번 글에서는 웹, HTTP, 그리고 네트워크의 배경과 간단한 배경 지식에 대해 작성하겠습니다.\n\n## 웹, 그리고 네트워크를 공부해야 하는 이유\n\n> 책에는 없는 내용이지만 네트워크를 공부해야 하는 이유에 대해 생각해봤다.\n\n웹, 그리고 네트워크는 **통신하는 방법에 대한 개념이다.**<br>\n**통신은 게임, 쇼핑몰, IoT 분야 상관없이 IT 서비스에서는 필수다.**\n\n따라서 우리가 어디가서 개발한다고 하려면 적어도 네트워크가 어떻게 구성되어 있고, 어떻게 동작하는지 알아야 한다고 생각한다.\n\n## 클라이언트, 서버, 그리고 프로토콜\n\n기본적으로 **네트워크 상에서 클라이언트와 서버가 HTTP 프로토콜을 이용해 통신한다.**\n\n### 클라이언트\n\n통신을 요청하는 장치\n\n### 서버\n\n받은 요청에 따라, 정보나 서비스를 제공하는 장치\n\n### 프로토콜\n\n서로 다른 장치들끼리 통신하는 방식을 미리 정의해놓은 약속 또는 규칙\n\n## TCP/IP\n\nTCP/IP는, 전세계에서 가장 많이 사용되는 Internet Protocol Suit 중 하나다.\n> Internet Protocol Suit: 인터넷에서 컴퓨터 간 통신하는 데 쓰이는 프로토콜의 모음\n\n기본적으로 네트워크 상에서 일어나는 통신은 TCP/IP 프로토콜을 이용해 이루어진다고 보면 된다.<br>\n책의 주 내용인 HTTP 프로토콜도 TCP/IP 프로토콜에 기본적으로 포함된다.\n\nTCP/IP 프로토콜에서 가장 중요한 것은 **계층 구조**라는 것이다.\n\n> 워낙 방대한 내용이지만 해당 책에서는 간단한 소개 정도만 하고 있다.<br>\n> 보다 자세한 내용은 [테코톡 - 히히의 OSI 7 계층](https://youtu.be/1pfTxp25MA8?feature=shared)과 [테코톡 - 수리의 TCP/IP](https://www.youtube.com/watch?v=BEK354TRgZ8)를 참고하자.\n\n### TCP/IP 계층 구조\n\n통신 과정에 필요한 작업들을 모놀리식이 아닌, 4계층으로 나누어 각 계층 별로 담당하는 작업을 나눴다.<br>\n다시 말해 각 계층을 **모듈화**하였고, 자연스레 변경이 쉬워지며 다른 계층이 뭘 하는지 알 필요가 전혀 없어도 된다.\n\n4계층은 아래와 같이 나뉜다.\n\n1. Application Layer\n    - 애플리케이션에서 통신에 필요한 작업을 수행하는 계층이다.\n    - 프로세스와 운영체제 사이에 존재하는 소켓을 통해 데이터 송수신이 이루어진다.\n    - HTTP, FTP(File Transport Protocol), DNS(Domain Name System) 등이 계층에 포함된다.\n\n2. Transport Layer\n    - 통신의 신뢰성을 보장하는 작업을 처리한다.\n    - TCP와 UDP 두 가지 프로토콜이 포함된다.\n\n3. Internet Layer\n    - 네트워크 상에서 패킷의 이동을 처리한다.\n    - IP(Internet Protocol)이 포함된다.\n\n4. Link Layer (Data Link Layer, Network Interface Layer)\n    - 네트워크에 접속하는 하드웨어와 관련돤 계층이다.\n    - 랜 카드, 케이블 등이 해당 계층에 포함된다.\n\n### TCP/IP 통신의 흐름\n\n기본적으로 클라이언트와 서버 모두 **계층 순서대로 작업을 거쳐가며 통신한다.**\n\n![TCP/IP 통신의 흐름 (Network Layer는 Internet Layer로도 불린다.)](flow_of_tcp_ip.png)\n\n클라이언트의 Application Layer -> Transport Layer -> Internet Layer -> Link Layer -> 서버의 Link Layer -> Internet Layer ->\nTransport Layer -> Application Layer 순서대로 거치게 된다.\n\n### 캡슐화\n\n클라이언트의 Application Layer에서 만들어진 HTTP 데이터는 Transport Layer, Network Layer, Link Layer를 거치며 각 계층에서 만들어진 데이터가 추가된다.\n\n이렇게 만들어진 데이터가 서버의 Link Layer에 도착하고 HTTP 데이터가 서버의 Application Layer까지 도달하기 위해선, 마찬가지로 각 계층을 거치면서 각 계층이 필요한 데이터를 사용 및 삭제하고\n다음 계층으로 전달한다.\n\n이 과정을 _캡슐화_ 라고 한다.\n\n각 계층의 데이터를 추가할 때, **헤더**를 추가한다고 한다.<br>\n각 계층 별로 추가하는 헤더의 이름은 조금씩 다르다.\n\nTransport Layer가 씌우는 헤더는, **세그먼트**<br>\nInternet Layer가 씌우는 헤더는, **패킷**<br>\nLink Layer가 씌우는 헤더는, **프레임** 이라고 부른다.\n\n## HTTP와 관계가 깊은 프로토콜, IP / TCP / DNS\n\n### IP (Internet Protocol)\n\n인터넷 프로토콜(이하 IP)은 Internet Layer에 포함된다.\n\nIP의 역할은 패킷을 상대 컴퓨터에게 전달하는 것이다.\n\n패킷을 전달하기 위해서는 많은 정보가 필요하다.<br>\n그 중에서도 IP 주소와 MAC 주소가 중요하게 작용한다.\n\n- IP 주소\n  - 네트워크 통신에 있어 각각의 통신기기에 할당된 주소다. \n  - 인터넷 서비스 공급자(ISP)에 의해 할당받는다.\n  - 고유한 주소가 아니다. 경우에 따라 변경될 수 있다.\n\n- MAC 주소\n  - 네트워크에 존재하는 노드의 NIC와 같은 하드웨어에 고유적으로 부여된 주소다.\n  - 변하지 않는다.\n\nIP 주소와 MAC 주소를 이용해 도착지를 파악한다.<br>\n그런데 IP 주소는 가변성이 있기 때문에 도착지 정보를 IP 주소에서 MAC 주소로 변환하여 정확한 위치를 파악하는 것이 필요하다.\n\n이때 사용되는 프로토콜이 _ARP(Address Resolution Protocol) 프로토콜_ 이다.\n\nIP에서 또 중요한 것은 **라우팅**이다.<br>\n라우팅은 네트워크 상에서 패킷이 전달될 때 최적의 경로를 선택하는 과정이다.<br>\n\n### TCP (Transfer Control Protocol)\n\nTCP는 Transport Layer에 포함된다.\n\nTCP는 데이터 전달 보증, 패킷 전달 순서를 보장한다.\n\n_[3 way handshake](https://www.geeksforgeeks.org/tcp-3-way-handshake-process/)_ 를 통해 데이터 전달의 신뢰성을 보장한다.<br>\n여러 패킷들을 세그먼트로 나누고 각 세그먼트에 번호를 부여하는 방식으로 패킷의 전달 순서를 보장한다.\n\n### DNS (Domain Name Service)\n\nDNS는 Application Layer에 포함된다.\n\nDNS의 역할은 사람이 기억하기 어려운 IP 주소와 MAC 주소에 이름(도메인)을 붙여 사용하기 쉽도록 이름을 IP 주소로 변환하는 작업이다.\n\n### 전체 과정\n\nHTTP, DNS, TCP, IP가 동작하는 전체 과정의 그림을 통해 다시 이해해보자. \n\n![통신의 전체 과정](overview_connect.jpeg)\n\n## URI와 URL\n\nURI(Uniform Resource Identifier)은 네트워크 상에서 리소스를 식별할 수 있는 수단을 제공한다.\n\nURI에는 리소스의 위치를 가리키는 URL(Uniform Resources Location)과 리소스의 이름을 지정하는 URN(Uniform Resources Name)으로 나뉜다.<br>\nURN 만으로 실제 리소스를 식별하는 방법이 보편화되지 않고 URL 방식이 보편화되었다.<br>\n그래서 사실상 URI와 URL을 같은 의미로 사용한다.\n\n### URL 문법\n\n```text\nschema://[user:password@]server address[:port][/path][?query][#fragment]\n\nhttps://www.google.com:443/search?q=google&hl=ko\n```\n\n- schema (https)\n  - 사용할 프로토콜을 지정한다.\n\n- user:password\n  - 리소스를 얻기 위해 사용되는 인증 정보를 지정한다.\n  - 생략 가능하다.\n  \n- server address (www.google.com, localhost, celuveat.com)\n  - 통신을 요청할 서버의 주소를 지정한다.\n  - 도메인 이름이나 IP 주소를 입력한다.\n  \n- port (:80, :8080, :443, :22)\n  - 서버의 접속 대상이 되는 포트 번호를 지정한다.\n  - `:`로 시작한다.\n  - 생략 가능하다.\n    - 생략하면 schema에 따라 default port가 사용된다.\n  \n- path (/search)\n  - 리소스의 경로를 지정한다.\n  - `/`로 시작한다.\n  - 생략 가능하다.\n\n- query (?q=google&hl=ko)\n  - 지정된 리소스에 임의의 값을 전달하기 위해 지정한다.\n  - `?`로 시작하며 `key=value` 형태로 작성하고 각 값은 `&`를 통해 구분한다.\n  - 생략 가능하다.\n  - query parameter, query string라고도 불린다.\n\n- fragment\n  - 취득한 리소스에서 서브 리소스(특정 위치)를 가리키기 위한 값을 지정한다.\n    - html 내부 북마크 등에 사용한다.\n    - 예) https://docs.spring.io/spring-framework/reference/web/websocket.html#when-to-use-websockets\n  - 서버로 전송하는 정보가 아니다.\n\n### Reference\n\n> - [그림으로 배우는 Http & Network Basic](https://m.yes24.com/Goods/Detail/15894097)\n> - [모든 개발자를 위한 HTTP 웹 기본 지식](https://inf.run/8ZEU8)\n"},{"excerpt":"들어가며 MySQL에는 4가지의 격리 수준이 존재한다. 격리 수준에 따라 같은 상황, 그리고 같은 쿼리라 하더라도 결과가 매번 달라질 수 있다.\n이로 인해 정합성 문제가 발생할 수 있다. 우리가 실행하는 쿼리문이 적어도 격리 수준에 따라 어떤 결과를 가져올 지 정확하게 예측할 수 있어야 하기에, MySQL의 격리 수준에 대해 이해해보자. 트랜잭션 격리 수…","fields":{"slug":"/05-isolation-level-of-MySQL/"},"frontmatter":{"date":"January 11, 2024","title":"MySQL의 격리 수준","tags":["database","MySQL"]},"rawMarkdownBody":"\n## 들어가며\n\nMySQL에는 4가지의 격리 수준이 존재한다.\n\n격리 수준에 따라 같은 상황, 그리고 같은 쿼리라 하더라도 결과가 매번 달라질 수 있다.<br>\n이로 인해 정합성 문제가 발생할 수 있다.\n\n우리가 실행하는 쿼리문이 적어도 격리 수준에 따라 어떤 결과를 가져올 지 정확하게 예측할 수 있어야 하기에, MySQL의 격리 수준에 대해 이해해보자.\n\n## 트랜잭션 격리 수준\n\n트랜잭션 격리 수준이란, **여러 트랜잭션이 동시에 처리될 때 특정 트랜잭션이 변경하는 값을 다른 트랜잭션이 조회할 때 어떤 값을 반환할 지 결정하는 것**이다.\n\nMySQL의 트랜잭션 격리 수준은 총 4가지다.\n\n- Read Uncommitted\n- Read Committed\n- Repeatable Reade\n- Serializable\n\n각 격리 수준에 따라 동일한 상황에서 조회 결과가 달라진다.<br>\n그리고 트랜잭션을 동시에 처리할 수 있는 성능의 차이도 발생한다.\n\n일반적인 온라인 서비스 용도의 데이터베이스는 _Read Committed_와 _Repeatable Read_ 중 하나를 사용한다.<br>\nMySQL은 주로 _Repeatable Read_를 사용한다.\n\n|                  | Dirty Read | Non-Repeatable Read | Phantom Read  |\n|:----------------:|:----------:|:-------------------:|:-------------:|\n| Read Uncommitted |     O      |          O          |       O       |\n|  Read Committed  |     X      |          O          |       O       |\n|  RepeatableRead  |     X      |          X          | O (InnoDB는 X) |\n|   Serializable   |     X      |          X          |       X       |\n\n## Read Uncommitted\n\n먼저 그림으로 이해해보자.\n\n![](read_uncommitted_1.png)\n\n트랜잭션 1번이 특정 레코드에 UPDATE 작업을 수행한다.<br>\n동시에 언두 로그에는 변경되기 전의 값이 저장된다.\n\n트랜잭션 1번이 커밋이나 롤백하기 전, 트랜잭션 2번에 의해 `id=2`인 레코드를 조회하는 상황에 Read Uncommitted는 아래같이 수행된다.\n\n![](read_uncommitted_2.png)\n\n트랜잭션 2번은 버퍼 풀에 있는 값을 조회한다.\n\n만약 이후에 트랜잭션 1번이 롤백된다면, 트랜잭션 2번의 조회 결과는 데이터베이스에 존재하지 않은 결과가 된다.<br>\n즉, 정합성에 크리티컬한 문제가 발생한다.\n\n특정 트랜잭션에서 처리한 작업이 완료되지 않았는데도 다른 트랜잭션에서 볼 수 있는 현상을 *Dirty Read*라고 한다.<br>\n**_Read Uncommitted_ 에서는 이 _Dirty Read_ 문제가 발생한다.**\n\n## Read Committed\n\n이번에도 역시 그림으로 이해해보자.\n\n![](read_uncommitted_1.png)\n\n위와 동일한 상황이다.<br>\n마찬가지로 트랜잭션 1번이 Commit 되기 전, 트랜잭션 2번이 조회하는 상황에 Read Committed 격리 수준은 아래같이 수행된다.\n\n![](read_committed.png)\n\n_Read Uncommitted_ 와 달리 버퍼 풀이 아닌, **언두 로그에 있는 값을 조회한다.**<br>\n즉, **커밋이나 롤백되지 않은 변경은 조회하지 않게 된다.**\n\n따라서 Dirty Read 문제는 발생하지 않는다.<br>\n하지만 다른 문제가 존재한다.\n\n![](non_repeatable_read.png)\n\n트랜잭션 2번의 입장에서 **하나의 트랜잭션 안에서 동일한 쿼리의 결과가 달라진다.**<br>\n\n이러한 문제를 _Non-Repeatable Read_ 문제라고 하며, 분명하게 정합성에 오류가 존재하는 문제다.\n\n## Repeatable Read\n\n**MySQL에서 기본으로 사용되는 격리 수준이다.**\n\n_Read Committed_ 와 마찬가지로 언두 로그에 존재하는 값을 조회한다.<br>\n하지만 차이가 있다면, 언두 로그에 백업된 레코드의 여러 버전 중 몇 번째 이전 버전까지 조회할 것인가에 차이가 있다.\n\n더 자세히 알아보자.\n\n모든 InnoDB의 트랜잭션은 고유 번호를 가지며, 언두 로그의 모든 레코드에는 변경을 발생시킨 트랜잭션의 번호가 포함된다.<br>\n그리고 언두 로그의 데이터는 InnoDB가 불필요하다고 판단되는 시점에 주기적으로 삭제한다.\n\n_Repeatable Read_ 격리 수준일 때는 정합성을 보장하기 위해 **실행 중인 트랜잭션 중, 가장 오래된 트랜잭션 번호보다 더 앞선 언두 로그의 데이터는 삭제할 수 없다.**\n\n더 정확하게는 **특정 트랜잭션 번호의 구간 내에서 백업된 언두 데이터를 모두 보존한다.**\n\n그림을 통해 Repeatable Read 수준에서는 어떻게 동작하는지 이해해보자.\n\n![](repeatable_read_1.png)\n\n전제 조건은 다음과 같다.\n\n트랜잭션 1번이 2개의 레코드를 삽입한 상황이고, 트랜잭션 3번이 `id=2`인 레코드를 조회한다.\n\n그 다음 트랜잭션 2번이 `id=2`인 레코드에 UPDATE를 수행한다.\n\n![](repeatable_read_2.png)\n\n변경되는 레코드는 언두 로그에 백업된다.\n\n이후 트랜잭션 2번이 커밋 혹은 롤백되기 전에 트랜잭션 3번이 `id=2`를 조회하는 상황이다.\n\n![](repeatable_read_3.png)\n\n사실 Read Committed와 동일하다.\n\n하지만 내부적으로 트랜잭션 3번에서 실행되는 모든 SELECT 쿼리는, 해당 트랜잭션 번호인 3번보다 앞선 트랜잭션 번호를 가진, 언두 로그의 데이터들만 조회한다.\n\n언두 로그에는 하나의 레코드에 다양한 버전으로 백업된 레코드가 충분히 존재할 수 있다.<br>\n이러한 상황에서 Repeatable Read 격리 수준은 이 **트랜잭션 번호를 이용해서, 하나의 트랜잭션 내에서 발생하는 SELECT 쿼리에 대해 항상 동일한 값을 반환하도록 한다.**\n\nRepeatable Read 격리 수준에서도 발생하는 정합성 문제가 있다.<br>\n바로 _`Phantom Read`_ 문제다.\n\n![img.png](phantom_read.png)\n\n트랜잭션 3번이 2번의 `SELECT ... FOR UPDATE` 쿼리를 날리는 사이에, 트랜잭션 2번이 INSERT를 통해 값을 추가한다.<br>\n이때, 트랜잭션 3번이 날리는 2개의 `SELECT ... FOR UPDATE` 쿼리 결과는 달라진다.<br>\n이렇게 새로운 데이터가 생기거나 사라지는 현상을 _Phantom Read_ 라고 한다.\n\n전제 조건이 있긴 한데, 하나의 트랜잭션 안에서 `SELECT ... FOR UPDATE` 나 `SELECT ... FOR LOCK IN SHARE MODE` 로 조회하는 경우에 발생한다.\n\n위의 두 쿼리는 SELECT 하는 레코드에 **쓰기 잠금**을 걸어야 한다.<br>\n하지만 **언두 로그에는 잠금을 걸 수 없다.**<br>\n그래서 위 쿼리로 조회되는 레코드는 언두 영역의 레코드가 아닌, 버퍼 풀의 레코드이기 때문이다.\n\n**InnoDB 엔진에서는 **갭 락과 넥스트 키 락** 덕분에 해당 격리 수준에서도 Phantom Read 문제가 발생하지 않는다.**\n\n## Serializable\n\n가장 단순하며 엄격한 격리 수준이다.\n\n간단히 생각해서 뮤텍스처럼 동작하는 격리 수준이다.\n\nInnoDB 테이블에서 기본적으로 순수한 SELECT 작업은 다른 트랜잭션의 변경 작업 수행 여부와 상관없이 잠금을 대기하지 않고 바로 수행된다. (잠금 없는 일관된 읽기)\n\n하지만 격리 수준이 Serializable인 경우 순수 SELECT 작업도 공유 잠금(읽기 잠금)을 획득해야 한다.<br>\n즉, 특정 트랜잭션이 레코드를 조회하고 있을 때에도 다른 트랜잭션은 조회하지 못하고 잠금을 대기해야 한다.\n\n이러한 특징 때문에 정합성 관련 문제는 발생하지 않지만 동시성 처리 능력은 가장 떨어진다.\n\n### Reference\n\n> - https://product.kyobobook.co.kr/detail/S000001766482"},{"excerpt":"Lock에는 MySQL 엔진이 관리하는 Lock과 InnoDB 스토리지 엔진이 관리하는 락이 있다. 이번에는 InnoDB 스토리지 엔진이 관리하는 Lock에 대해 알아보자. 레코드 락 InnoDB 엔진이 관리하는 락 중에서 핵심이다. 다른 DBMS의 레코드 락과 동일하지만 한 가지 중요한 차이는 레코드 자체가 아니라 인덱스의 레코드를 잠근다는 것이다.\n따…","fields":{"slug":"/05-lock-of-innodb-storage-engine/"},"frontmatter":{"date":"January 11, 2024","title":"InnoDB 스토리지 엔진의 락","tags":["database","MySQL"]},"rawMarkdownBody":"\nLock에는 MySQL 엔진이 관리하는 Lock과 InnoDB 스토리지 엔진이 관리하는 락이 있다.\n\n이번에는 InnoDB 스토리지 엔진이 관리하는 Lock에 대해 알아보자.\n\n## 레코드 락\n\nInnoDB 엔진이 관리하는 락 중에서 핵심이다.\n\n다른 DBMS의 레코드 락과 동일하지만 한 가지 중요한 차이는 **레코드 자체가 아니라 인덱스의 레코드를 잠근다는 것이다.**<br>\n따로 생성한 세컨더리 인덱스 뿐만 아니라 PK 인덱스도 마찬가지다.<br>\n다만, PK 인덱스는 갭 락을 걸지 않고 레코드 락만 걸지만 세컨더리 인덱스에서는 넥스트 키 락 또는 갭 락을 이용한다.\n\n## 갭 락\n\n갭 락은 **레코드와 해당 레코드와 바로 인접한 레코드 사이 간격을 잠그는 것을 의미**한다.\n\n갭 락의 역할은 **레코드와 레코드 사이에 새로운 레코드가 INSERT 되는 것을 막기 위함**이다.\n\n갭 락 그 자체보다는 넥스트 키 락의 일부로 자주 사용된다.\n\n## 넥스트 키 락\n\n레코드 락과 갭 락을 합쳐놓은 형태의 잠금이다.\n\n갭 락이나 넥스트 키 락의 주목적은, 바이너리 로그에 기록되는 쿼리가 Slave 서버에서 실행될 때 Master 서버에서 만들어 낸 결과와 동일한 결과를 만들어내도록 보장하는 것이다.\n\n> [바이너리 로그?](https://www.linux.co.kr/bbs/board.php?bo_table=lecture&wr_id=5775)\n\n## Auto Increment 락\n\nMySQL에는 테이블에 INSERT 할 때, 자동 증가하는 숫자를 넣어주는 `AUTO_INCREMENT` 칼럼 속성이 존재한다.\n\n보통 ID 칼럼에 사용하곤 하는데, 테이블에 동시에 여러 건의 INSERT가 발생하는 경우 저장되는 각 레코드의 ID 값은 중복되지 않고 순서대로 증가하는 값이 자동으로 저장된다.\n\nInnoDB 스토리지 엔진에서는 이 작업을 위해 내부적으로 **AUTO_INCREMENT 락** 이라고 하는 테이블 수준의 잠금을 사용한다.\n\n어쩌면 당연하게 새로운 레코드를 저장하는 쿼리, INSERT, REPLACE 쿼리에서만 작동한다.\n\n해당 락은, **트랜잭션과 상관없이 INSERT 혹은 REPLACE 쿼리에서 AUTO_INCREMENT 값을 가져올 때만 락이 걸렸다가 즉시 해제한다.**\n\nAUTO_INCREMENT 락은 테이블 당 하나만 존재한다.<br>\n따라서 2개 이상의 INSERT 쿼리가 동시에 실행되는 경우, 하나의 쿼리가 해당 테이블에 AUTO_INCREMENT 락을 걸면 나머지 쿼리는 락이 해제될 때까지 기다려야 한다.\n\n> 참고로 AUTO_INCREMENT 칼럼에 값을 명시적으로 넣어준 쿼리가 실행되어도 AUTO_INCREMENT 락은 수행된다. 그 이유는, AUTO_INCREMENT 속성이 걸린 칼럼에 값이 명시적으로 담겨져\n> INSERT 되면, AUTO_INCREMENT 값을 재설정하는 작업이 수행되기 떄문이다.\n\nAUTO_INCREMENT 락은 명시적으로 획득하고 해제하는 방법은 없다.<br>\n그리고 아주 짧은 시간동안 존재하는 락이기 때문에 대부분 문제가 잘 되지 않는다.\n\n지금까지는 MySQL 5.0 이하 버전에서 사용되던 방식이다.<br>\nMySQL 5.1 이상부터는 `innodb_autoinc_lock_mode` 시스템 변수를 이용해 작동 방식을 변경할 수 있다.\n\n### innodb_autoinc_lock_mode = 0\n\n위에서 설명된 방식 그대로 사용한다.\n\n### innodb_autoinc_lock_mode = 1\n\nMySQL 서버가 INSERT 되는 레코드의 수를 정확히 예측할 수 있을 때에는 AUTO_INCREMENT 락을 걸지 않고, 훨씬 빠른 래치(뮤텍스)를 이용한다.\n\n락이 존재하는 타이밍이 존재하긴 하지만 AUTO_INCREMNET 락보다는 훨씬 빠르게 테이블에 락을 걸고 해제하게 된다.\n\n다만, _INSERT 되는 레코드의 수를 정확히 예측할 수 있어야 한다_는 전제 조건이 있기 때문에 이 조건을 만족하지 못하면 AUTO_INCREMENT를 사용한다.\n\n대량의 INSERT 쿼리가 처리될 때는, InnoDB 스토리지 엔진이 AUTO_INCREMENT 값을 한 번에 여러 값을 할당받아서 사용한다.<br>\n하지만 이때 할당받은 값을 모두 사용하지 못하게 되면 폐기하게 되어, 그 다음 INSERT 쿼리에서 AUTO_INCREMENT 값은 중간 값이 누락된 값이 저장될 수 있다.\n\n위 설정에서는 하나의 INSERT 쿼리로 저장되는 레코드는 연속된 자동 증가 값을 가진다는 특징을 가진다.<br>\n따라서 이 설정 모드를 _연속 모드_ 라고도 한다.\n\n### innodb_autoinc_lock_mode = 2\n\n해당 설정은 AUTO_INCREMENT 락을 걸지 않고 가벼운 래치(뮤텍스)를 사용한다.\n\n이 설정에서는 **대량 INSERT 문장이 실행되는 중에도 다른 커넥션에서 INSERT를 수행할 수 있다.**<br>\n결과적으로 동시 처리 성능이 높아진다.\n\n하지만 자동 증가 값이 연속되는 것은 보장할 수 없다. 단지 유니크한 값만 생성한다는 것만 보장한다.<br>\n하나의 INSERT 쿼리로 저장할 때에도 순서는 보장할 수 없다. 따라서 _인터리빙 모드_ 라고도 한다.\n\n위 설정에서 **STATEMENT 포맷의 바이너리 로그를 사용하는 Replication 에서는 Master와 Slave의 AUTO_INCREMENT 값이 달라질 수 있다.**<br>\nSTATEMENT 포맷의 바이너리 로그를 사용한다면 mode 값을 1로 설정하자.\n\n## 인덱스와 InnoDB의 잠금\n\n레코드 락에서 언급한대로 InnoDB는 **레코드 자체가 아니라 인덱스의 레코드를 잠근다.**\n\n이 말은 즉슨, **SELECT나 UPDATE, DELETE 처럼 변경해야 할 레코드를 찾기 위해 인덱스를 타고 검색하게 되는데, 이 과정에서 조건에 해당하는 레코드를 전부 잠근다.**\n\nMySQL의 InnoDB에서 인덱스 설계가 중요한 이유 또한 이것이다.\n\n### 레코드 수준의 잠금 확인 및 해제\n\nMySQL 8.0부터는 performance_schema의 data_locks와 data_lock_waits 테이블을 통해 확인할 수 있다.\n\n```sql\n# 현재 프로세스의 목록을 조회한다.\nmysql> SHOW PROCESSLIST;\n\n# performance_schema의 data_locks와 data_lock_waits 테이블을 통해 잠금 대기 순서를 조회한다.\nmysql> SELECT \n    r.trx_id waiting_trx_id, \n    r.trx_mysql_thread_id waiting_thread, \n    r.trx_query waiting_query, \n    b.trx_id blocking_trx_id,\n    b.trx_mysql_thread_id blocking_thread, \n    b.trx_query blocking_query \n    FROM performance_schema.data_lock_watis w\n    JOIN information_schema.innodb_trx b\n    ON b.trx_id = w.blocking_engine_transaction_id\n    JOIN infromation_schema.innodb_trx r\n    ON r.trx_id = w.requesting_engine_transaction_id;\n    \n# 각 스레드가 어떤 잠금을 가지는지 상세하게 조회한다.\nmysql> SELECT * FROM performance_schema.data_locks\\G\n```\n\n문제가 되는 잠금이 있다면 `mysql> KILL {thread_id}` 를 통해 스레드를 강제 종료하자.\n\n### Reference\n\n> - https://dev.mysql.com/doc/refman/8.0/en/example-auto-increment.html\n> - https://product.kyobobook.co.kr/detail/S000001766482"},{"excerpt":"들어가며 최근 개인 프로젝트를 시작하면서 ERD 설계를 진행했습니다. 이때, 작성 날짜나 시작 날짜 같은 날짜에 대한 값을 저장할 칼럼이 필요했고 해당 칼럼의 자료형을 결정하는 과정에서 DATETIME과 TIMESTAMP의 차이에 대해 궁금해졌습니다. 따라서 이 둘을 이해해보겠습니다. 기준은 MySQL 8.0 입니다. DATETIME과 TIMESTAMP의…","fields":{"slug":"/compare-of-datetime-and-timestamp/"},"frontmatter":{"date":"January 10, 2024","title":"[MySQL] DATETIME vs TIMESTAMP","tags":["database","MySQL","DATETIME","TIMESTAMP"]},"rawMarkdownBody":"\n## 들어가며\n\n최근 개인 프로젝트를 시작하면서 ERD 설계를 진행했습니다.\n\n이때, 작성 날짜나 시작 날짜 같은 **날짜**에 대한 값을 저장할 칼럼이 필요했고 해당 칼럼의 자료형을 결정하는 과정에서 **DATETIME과 TIMESTAMP의 차이에 대해 궁금해졌습니다.**\n\n따라서 이 둘을 이해해보겠습니다.\n\n> 기준은 MySQL 8.0 입니다.\n\n## DATETIME과 TIMESTAMP의 공통점\n\n-  두 타입 모두 **날짜와 시간을 표현합니다.**\n-  표현 형식은 `YYYY-MM-DD hh:mm:ss` 입니다.\n-  최대 6자리 정밀도의 소수점 초 부분이 포함될 수 있습니다.\n-  표현할 수 있는 범위의 차이 떄문에 약간 다른 점이 있습니다.\n  - DATETIME은 `1000-01-01 00:00:00.000000`부터 `9999-12-31 23:59:59.499999` 까지입니다.\n  - TIMESTAMP는 `1970-01-01 00:00:01.000000`부터 `2038-01-19 03:14:07.499999` 까지입니다.\n- 자동으로 초기화하거나 현재의 날짜 및 시간으로 업데이트하는 기능을 제공합니다.\n  - 예) `DEFAULT CURRENT_TIMESTAMP` or `ON UPDATE CURRENT_TIMESTAMP`\n\n## DATETIME과 TIMESTAMP의 차이점\n\n### 1. **지원하는 값의 범위에 차이가 있습니다.**\n\n- DATETIME은 `1000-01-01 00:00:00` 부터 `9999-12-31 24:59:59` 까지입니다.\n- TIMESTAMP는 `1970-01-01 00:00:01`UTC 부터 `2038-01-19 03:14:07`UTC 까지입니다.\n\n### 2. **Current Time Zone에 따라 UTC 값으로 변환해주는 작업에 차이가 있습니다.**\n\n- DATETIME은 작업을 지원하지 않습니다.\n- TIMESTAMP는 저장 혹은 검색 시에 변환해줍니다.\n    - 즉, TIMESTAMP는 Time Zone 변화에 유연합니다.\n    - 추가로 Session 별로 Time Zone을 설정할 수 있습니다.\n\n### 3. **값을 저장하는 공간의 크기에도 차이가 있습니다.**\n\n- DATETIME은 8 byte를 차지합니다.\n- TIMESTAMP는 4 byte를 차지합니다.\n\n## 결론\n\n날짜와 시간까지 함께 저장해야하는 칼럼의 자료형은, 가급적 **TIMESTAMP를 선택하는 것이 좋아보입니다.**\n\n저장 공간의 이점도 있을 뿐더러, 무엇보다 Time Zone에 따른 UTC 값으로 자동 변환해주는 작업의 유무에서 큰 차이가 있기 때문입니다.\n\n따라서 더 가볍고 유연한 테이블을 위해서는 DATETIME 보단 TIMESTAMP를 권장합니다.\n\n## 번외: DATE 타입\n\n- 시간없이 날짜에 대한 값만 지원합니다.\n- `YYYY-MM-DD` 형식으로 표현합니다.\n- 지원 범위는 `1000-01-01` 부터 `9999-12-31` 까지입니다.\n\n> ### Reference\n> - https://dev.mysql.com/doc/refman/8.0/en/datetime.html\n> - https://dev.mysql.com/doc/refman/8.0/en/timestamp-initialization.html"},{"excerpt":"MySQL 로그 파일 MySQL 서버에 문제가 생겼을 때, 로그 파일을 이용하면 문제의 원인을 쉽게 찾고 해결할 수 있다.\n따라서 가급적 아래에 나올 로그 파일들을 통해 문제를 파악하려는 태도를 가지는 것이 중요하다. 로그 파일의 종류는 아래와 같다. 에러 로그 서버가 시작되는 과정과 관련된 정보성 로그 및 에러 메시지 서버가 비정상적으로 종료될 때 기록…","fields":{"slug":"/04-mysql-log/"},"frontmatter":{"date":"January 03, 2024","title":"MySQL 로그 파일","tags":["database"]},"rawMarkdownBody":"\n## MySQL 로그 파일\n\nMySQL 서버에 문제가 생겼을 때, **로그 파일을 이용하면 문제의 원인을 쉽게 찾고 해결할 수 있다.**<br>\n따라서 가급적 아래에 나올 로그 파일들을 통해 문제를 파악하려는 태도를 가지는 것이 중요하다.\n\n로그 파일의 종류는 아래와 같다.\n\n- 에러 로그\n  - 서버가 시작되는 과정과 관련된 정보성 로그 및 에러 메시지\n  - 서버가 비정상적으로 종료될 때 기록되는 InnoDB의 트랜잭션 복구 메시지\n  - 쿼리 처리 도중 발생하는 문제에 대한 에러 메시지\n  - 비정상적으로 종료된 Connection에 대한 Log\n  - InnoDB의 모니터링 또는 상태 조회 명령의 결과 메시지\n  - MySQL의 종료 메시지\n- 제네럴 쿼리 로그\n- 슬로우 쿼리 로그\n\n## 1. 에러 로그 파일\n\n서버가 실행되는 도중 발생하는 에러가 출력되어 기록되는 로그 파일이다.\n\n`my.cnf`에서 `log_error` 파라미터에 정의된 경로로 설정된다.<br>\n별도로 지정하지 않았으면 `datadir` 파라미터에 설정된 디렉터리에 `.err` 라는 확장자가 붙은 파일로 생성된다.\n\n![초기 my.cnf 설정값](my-cnf.png)\n\n![datadir에 설정된 경로의 파일들](datadir.png)\n\n그런데 이 로그들이 어떻게 기록이 될까?\n\n![로그 파일이 저장되는 형태](query-files.png)\n\n위 사진처럼 설정한 위치에 `.log` 파일들이 기록된다.\n\n아래는 `error.log` 파일의 내용이다.\n\n![에러 로그 내용](error-log.png)\n\n> 에러 로그를 하나에 파일에 모두 기록하는 건 관리하거나 사용하는 측면에서 불편함이 예상되어, 일별로 저장하거나 mysql이 알아서 error 로그의 용량을 일정 수준을 유지해주는지에 대해 찾아봤지만 잘 보이지 않는다.\n> MySQL의 Error Log와 관련된 자세한 내용은 [공식 문서](https://dev.mysql.com/doc/refman/8.0/en/error-log-configuration.html)를 참고하자.\n\n아래 설명될 로그들은 에러 로그에서 주로 보게 될 로그들이다.\n\n### 1.1 MySQL이 시작하는 과정과 관련된 정보성 및 에러 로그\n\n설정 파일을 변경하거나 DB가 비정상적으로 종료된 이후 다시 시작하는 경우에, 반드시 MySQL 에러 로그 파일을 통해 변경이 제대로 적용됐는지 확인해야 한다.\n\n만약 정상적으로 가동됐다면 `mysqld:ready for connection` 메시지를 확인할 수 있다.\n\n### 1.2 서버가 비정상적으로 종료된 경우 나타나는 InnoDB의 트랜잭션 복구 메시지\n\nInnoDB의 경우, MySQL이 비정상적으로 종료되면 완료되지 못한 트랜잭션을 정리하고 디스크에 기록되지 못한 데이터가 있다면 기록하는 작업을 진행한다.\n\n이 과정에 대한 간단한 메시지가 출력되는데, 간혹 문제가 있어 복구되지 못할 때에는 에러 로그를 남기고 MySQL은 다시 종료된다.\n\n### 1.3 쿼리 처리 도중 발생하는 문제에 대한 로그\n\n쿼리 도중 발생하는 문제점은 사전 예방이 어려우며, 주기적으로 에러 로그 파일을 검토하는 과정에서 알게 된다.\n\n**쿼리의 실행 도중 발생한 에러나 복제에서 문제가 될 만한 쿼리에 대한 경고 메시지가 에러 로그에 기록된다.**\n\n### 1.4 비정상적으로 종료된 커넥션 메시지\n\n클라이언트 애플리케이션에서 정상적으로 접속 종료를 하지 못하고 커넥션이 종료된 경우의 내용이 기록된다.\n\n### 1.5 InnoDB의 모니터링 또는 상태 조회 명령어(`SHOW ENGINE INNODB STATUS`같은)의 결과 메시지\n\nInnoDB의 테이블 모니터링이나 락 모니터링, 또는 엔진 상태를 조회하는 명령은 상대적으로 큰 메시지를 로그 파일에 기록한다.\n\n엔진 모니터링을 활성화한 상태로 유지하는 경우 파일 시스템의 공간을 다 차지할 수도 있다.\n\n따라서 모니터링을 사용한 이후에는 다시 비활성화해서 에러 로그 파일이 커지지 않게 만들어야 한다.\n\n### 1.6 MySQL의 종료 메시지\n\n누군가가 MySQL 서버를 종료시킨 경우 `Received SHUTDOWN from user ...`의 메시지를 확인할 수 있다.\n\n그렇지 않고, 아무런 종료 관련 메시지가 없거나 스택 트레이스 같은 내용이 출력되는 경우에는 MySQL 서버가 세그멘테이션 폴트로 비정상 종료된 것으로 판단할 수 있다.\n\n> 세그멘테이션 폴트<br>\n> : 프로그램이 허용되지 않은 메모리 영역에 접근을 시도하거나, 허용되지 않은 방법으로 메모리 영역에 접근을 시도할 경우 발생한다.<br>\n\n## 2. 제네럴 쿼리 로그 파일(제네럴 로그 파일, General Log)\n\n- 실행 계획 분석을 하는 경우와 같이 사용되는 쿼리로 어떤 것들이 있는지 전체 목록을 뽑아서 검토하는 경우가 있다.\n  > 스프링 애플리케이션 기준, 테스트 코드가 있다면 JPA가 뽑아주는 로그로도 추출하는 방법도 있다. \n\n  이 경우, 제네럴 쿼리 로그를 활성화해서 쿼리를 로그 파일로 기록하게 한 다음 해당 파일을 검토하면 된다.\n\n- 시간 단위로 실행됐던 쿼리 내용이 모두 기록된다.\n\n- 슬로우 쿼리 로그와는 달리 MySQL이 쿼리 요청을 받으면 실행되기 전에 바로 기록한다.\n\n- `general_log_file` 이름의 파라미터에 설정돼 있다.<br>\n  별도로 설정되지 않은 경우에는, 아래와 같이 확인할 수 있다.\n  ![초기 general log 설정](init-general-config.png)<br>\n  기본적으로 general_log 설정은 꺼져있다.\n  ![general log를 킨 후](see-general-log.png)\n\n- 파일이 아닌 테이블로도 저장할 수 있다.<br>\n  - `log_output` 파라미터로 설정할 수 있는데, default 값은 `FILE`이고 테이블에 저장하려면 `TABLE`로 설정하면 된다. \n    ![log_output 설정 상태](log-output-config.png)<br>\n  - 해당 설정은 슬로우 쿼리 로그와 같이 적용된다.\n  - 옵션을 `TABLE`로 설정하더라도 mysql database의 *slow_log* 테이블과 *general_log* 테이블은 CSV 스토리지 엔진을 사용하기 때문에 결국 CSV 파일로 저장하는 것과 동일하게 작동한다.\n\n더 자세한 내용은 [MySQL 공식문서](https://dev.mysql.com/doc/refman/8.0/en/query-log.html)를 참조하자\n\n## 3. 슬로우 쿼리 로그\n\n`long_query_time` 시스템 변수에 설정한 시간 이상의 시간이 소요된 쿼리가 모두 기록된다.\n\nMySQL이 쿼리를 실행한 후, 실제 소요된 시간을 기준으로 기록하기 때문에 반드시 쿼리가 정상적으로 실행이 완료돼야 기록될 수 있다.\n\n아래 사진은 슬로우 쿼리 로그의 일부이다.\n\n여기서 중요하게 볼 것들을 설명해보겠다.\n\n![슬로우 쿼리 로그](slow-query-log.jpeg)\n\n- Time\n  - 쿼리가 종료된 시점\n  \n- Query_time\n  - 쿼리가 실행되는 데 걸린 전체 시간\n\n- Lock_time\n  - 두 가지 레벨(MySQL 엔진 레벨, 스토리지 엔진 레벨)의 Lock 중 MySQL 엔진 레벨에서 관장하는 테이블 Lock에 대한 대기 시간만 표현\n  - 값이 0이 아니라고 해서 무조건 Lock 대기가 있었다고 판단하기는 어렵다.<br>\n    실제 쿼리가 실행되는 데 필요한 잠금 체키와 같은 코드 실행 부분의 시간까지 모두 포함되기 때문이다.<br>\n    따라서 매우 작은 값이면 무시해도 된다.\n  - InnoDB 스토리지 엔진은 스토리지 엔진 레벨의 Lock을 지원한다.<br>\n    가끔 InnoDB 테이블에 대한 SELECT 쿼리의 경우에도 값이 상대적으로 큰 값이 발생할 수 있는데, 이는 InnoDB의 레코드 수준의 Lock이 아닌 MySQL 엔진 레벨에서 설장한 테이블 Lock 때문일 가능성이 높다. 따라서 InnoDB 테이블에만 접근하는 쿼리 문장의 슬로우 쿼리 로그에서는 Lock_time은 튜닝이나 쿼리 분석에 별로 도움이 되지 않는다.\n\n- Rows_examined\n  - 쿼리가 처리되기 위해 몇 건의 레코드에 접근했는지를 의미\n\n- Rows_sent\n  - 실제 몇 건의 처리 결과를 클라이언트에게 보냈는지를 의미\n  - 일반적으로 Rows_examined 값은 높지만 Rows_sent 값이 적다면, 해당 쿼리는 더 적은 레코드만 접근하도록 튜닝할 가치가 있다. "},{"excerpt":"트랜잭션 트랜잭션은 최소한의 논리적인 작업 단위이다. 하나의 작업 단위에 포함되는 모든 작업들이 100% 성공해야 트랜잭션이 commit 되며, 중간에 작업 하나라도 문제가 생겼을 땐 작업 전체를 무효화한다. (All or Nothing) 이를 통해 데이터의 정합성을 보장한다. ACID 모델 데이터베이스 설계 원칙으로 ACID 모델이 있다.  InnoDB…","fields":{"slug":"/05-transaction-and-lock/"},"frontmatter":{"date":"January 03, 2024","title":"MySQL에서의 트랜잭션과 Lock","tags":["database"]},"rawMarkdownBody":"\n## 트랜잭션\n\n트랜잭션은 **최소한의 논리적인 작업 단위이다.**\n\n하나의 작업 단위에 포함되는 모든 작업들이 100% 성공해야 트랜잭션이 commit 되며, 중간에 작업 하나라도 문제가 생겼을 땐 작업 전체를 무효화한다. (*All or Nothing*)\n\n이를 통해 데이터의 정합성을 보장한다.\n\n### ACID 모델\n\n데이터베이스 설계 원칙으로 ACID 모델이 있다.\n\n![](img.png)\n\nInnoDB의 트랜잭션은 이 ACID 모델을 준수한다.\n\n- **Atomic**: 원자성, 트랜잭션의 작업은 전부 적용되거나 전부 적용되지 않아야 한다.\n- **Consistency**: 일관성, 데이터베이스는 트랜잭션이 진행되거나 커밋되거나 롤백된 모든 경우에 항상 일관된 상태로 유지된다. \n- **Isolation**: 고립성, 트랜잭션끼리는 서로 간섭할 수 없다. 뒷장에 나올 고립 수준으로 고립성의 정도를 조절할 수 있다.\n- **Durability**: 지속성, 트랜잭션이 commit 된 후에는 반드시 디스크에 반영해야 한다.\n\n자세한 내용은 [MySQL 공식문서](https://dev.mysql.com/doc/refman/8.0/en/mysql-acid.html) 참고\n\n### 주의할 점\n\n트랜잭션 또한 DB Connection과 마찬가지로 **최소한의 범위로 적용하는 것이 좋다.**\n\n트랜잭션의 범위가 많다는 것은 처리해야 할 작업이 많다는 것이고 이는 Connection이 존재하는 시간이 길어진다는 의미이다.<br>\nDB Connection의 수는 한정적이므로 하나의 Connection이 존재하는 시간이 길어지면 그만큼 다른 요청을 처리하는 데에 장애가 생길 확률이 높아진다.\n\n그리고 트랜잭션에 **네트워크를 통해 원격 서버와 통신하는 등의 작업은 어떻게 해서든 트랜잭션 내에서 제거하는 것이 좋다.**<br>\n트랜잭션이 실행되는 동안 외부 서버와 통신할 수 없는 상태라면, 해당 트랜잭션은 영영 처리되지 못하게 되며 다른 트랜잭션에도 악영향을 끼칠 수 있다.\n\n## MySQL 엔진의 Lock\n\nLock은 동시성을 제어하기 위한 기능이다.\n\n여러 Connection이 동시에 동일한 자원에 대해 변경을 요청한 경우, 경쟁 조건이 발생하는 것을 방지하기 위한 역할을 한다.\n\n크게 MySQL 엔진 레벨과 스토리지 엔진 레벨의 Lock으로 나눌 수 있다.\n\nMySQL 엔진 레벨의 Lock은 모든 스토리지 엔진에 영향을 미치지만, 스토리지 엔진 레벨의 Lock은 스토리지 엔진 간 상호 영향을 미치지 않는다.\n\n### 글로벌 락\n\n- `FLUSH TABLES WITH READ LOCK;` 명령으로 글로벌 락을 획득할 수 있다.\n\n- MySQL에서 제공하는 Lock 가운데 범위가 가장 크다.\n\n- 하나의 Connection에서 글로벌 락을 획득하면 다른 Connection에서 SELECT를 제외한 대부분의 DDL이나 DML을 실행한 경우 글로벌 락이 해제될 때까지 대기한다.\n\n- 영향을 미치는 범위는 MySQL 서버 전체이며, 작업 대상 테이블이나 데이터베이스가 다르더라도 동일하게 영향을 미친다.\n\n### 테이블 락\n\n- 개별 테이블 단위에 설정하는 락이다.\n\n- 묵시적으로도 테이블 락을 획득할 수 있으나, 명시적으로는 `LOCK TABLES table_name [ READ | WRITE ];` 명렁으로 특정 테이블에 대한 락을 획득할 수 있다.<br>\n  이렇게 명시적으로 획득한 락은 `UNLOCK TABLES` 명령으로 락을 반환할 수 있다.<br>\n  사실 명시적인 잠금은 글로벌 락과 마찬가지로 특별한 상황이 아니라면 애플리케이션에서 거의 사용할 필요가 없다.\n\n- 묵시적 테이블 락은 MyISAM이나 MEMORY 테이블에 쓰기 작업의 쿼리를 실행하면 발생한다.<br>\n  쿼리가 실행되는 동안 자동으로 획득했다가 쿼리가 완료된 후 자동 해제되는 식으로 동작한다.\n  - InnoDB 테이블의 경우, 스토리지 엔진 차원에서 레코드 기반 Lock을 지원하므로 DML 쿼리에서는 묵시적 테이블 락이 적용되지는 않는다.<br>\n    DDL의 경우에만 묵시적 테이블 락을 획득한다.\n\n### 네임드 락\n\n- `GET_LOCK()` 함수를 이용해 **임의의 문자열에 대해 Lock을 설정할 수 있다.**\n\n- Lock의 대상이 데이터베이스 객체가 아니라는 특징이 있다.<br>\n  단순히 사용자가 지정한 문자열에 대해 획득하고 반납하는 Lock이다.\n\n- 자주 사용되지는 않는다.<br>\n  여러 클라이언트가 상호 동기화를 처리해야 할 때 네임드 락을 이용하면 쉽게 해결할 수 있다.<br>\n  또, 많은 레코드에 대해 복잡한 요건으로 레코드를 변경하는 트랜잭션에 유용할 수 있다.\n\n자세한 내용은 [MySQL 공식문서](https://dev.mysql.com/doc/refman/8.0/en/locking-functions.html) 참고\n\n> 한번 보면 좋을만한 글<br>\n> [우아한기술블로그 - MySQL의 네임드 락을 이용한 분산 락으로 여러 서버에 걸친 동시성 관리](https://techblog.woowahan.com/2631/)\n\n### 메타데이터 락\n\n- 데이터베이스 객체(테이블 or 뷰)의 이름이나 구조를 변경하는 경우에 획득하는 Lock이다.\n\n- 명시적으로 획득할 수는 없고, 테이블의 이름을 변경과 같은 경우에 자동으로 획득하는 Lock이다.\n  - 테이블을 변경하는 작업의 경우 원본 이름과 변경될 이름 두 개 모두 한꺼번에 Lock을 획득한다."},{"excerpt":"2024년을 시작하는 오늘, 지난 한 해를 되돌아보고 올 한 해를 어떻게 보낼 지 미리 생각해보려고 한다. 우아한테크코스 2월부터 11월까지 진행했던 우아한테크코스가 사실상 2023년에서 가장 핵심이며 유일한 이야깃거리다. 때문에 우테코에서 경험했던 것들에 대해 적어보고자 한다. 지원하기 전 개발을 너무나도 배우고 싶었던 마음이 컸던 시기에, 운 좋게도 …","fields":{"slug":"/2023-retrospection/"},"frontmatter":{"date":"January 01, 2024","title":"2023년 회고록","tags":["retrospection"]},"rawMarkdownBody":"\n2024년을 시작하는 오늘, 지난 한 해를 되돌아보고 올 한 해를 어떻게 보낼 지 미리 생각해보려고 한다.\n\n## 우아한테크코스\n\n2월부터 11월까지 진행했던 우아한테크코스가 사실상 2023년에서 가장 핵심이며 유일한 이야깃거리다.\n\n때문에 우테코에서 경험했던 것들에 대해 적어보고자 한다.\n\n### 지원하기 전\n\n개발을 너무나도 배우고 싶었던 마음이 컸던 시기에, 운 좋게도 현장 실습을 진행하며 동시에 프리코스를 진행했다.\n\n이 순간은 살면서 무언가에 몰입했던 첫 순간이었다. 프리코스를 진행하기에 하루는 너무 짧았고 얼른 내일이 왔으면 좋겠다는 마음이 든 것도 처음이었다.\n\n그 정도로 즐겁고 열심히 했던 순간이었다. 덕분에 프리코스 전과 비교해 정말 많은 것을 배웠기에 그것만으로도 감사한 마음으로 최종 합격을 크게 기대하지 않았다.\n\n하지만 더욱 감사하게도 최종 합격 메일을 받았고, 당시 눈물 날 정도로 기뻤다.\n\n### 진행하며\n\n참 많은 학습과 생각, 그리고 경험이 있었다.\n\n기본적으로 개발을 너무 배우고 싶었던 나에게 정말 과분한 시간이었다.<br>\n덕분에 짧은 시간 동안 폭발적인 성장을 했고 그 시간을 기반으로 **앞으로 꾸준하게 학습하고 나만의 속도대로 나아가는 방법과 힘을 얻었다.**\n\n그 과정에서 타인과 비교하는 행위가 어쩌면 백해무익하다는 것을 느꼈는데, 이 경험을 계기로 더 큰 성장을 했다.<br>\n당시에 했던 많은 생각들을 통해 나만의 속도와 방법을 파악하게 된 순간이었기 때문이다.\n\n그리고 더 큰 폭의 성장을 위해 경쟁이 아닌, 협력했을 때 더 많은 성장이 이뤄진다는 것을 몸소 경험했다.<br>\n자연스레 잡담과 기록, 회고의 중요성도 이해할 수 있었다.\n\n레벨 5 때는 취업 준비를 시작하며 나를 되돌아보고 어떤 개발자가 되기 위해 우테코를 지원했고 그토록 열심히 학습했는지에 대해 생각하는 계기가 됐다.\n\n**결과적으로 약 10개월 동안 열심히 살았던 시간 덕분에 인간으로서, 그리고 개발자로서 많은 성장을 이뤄냈던 것 같다.**\n\n### 아쉬웠던 점\n\n주변 사람들과 비교하며 나를 맞지도 않는 틀에 욱여넣으려고 했다.\n\n그리고 모르는 것이 많다는 핑계로 공유를 잘하지 못했다.<br>\n공유하고 싶은 내용이 있다면 공부해서 하면 되는 문제인데 당시에는 몰랐다.<br>\n주변 크루들이 아는 건 모두 따라서 알고 싶었기 때문이다.\n\n기록을 많이 하지 않았다.<br>\n때문에 취업 준비를 할 때 꽤나 고생했다.\n\n바쁘다는 핑계로 운동을 하지 않았다.<br>\n그 결과 근육은 다 빠져버렸고 운동에 흥미도 잃어버렸다.\n\n## 2024년\n\n누구나 늘 그렇듯, 새해 기념 목표를 설정해봤다.\n\n- [ ] 누구보다 나를 잘 이해하는 한 해 만들기\n- [ ] 몸과 마음을 다시 건강하게 만들기\n- [ ] 경제 공부하기\n- [ ] 속도보다 방향에 집중하기\n\n앞으로 얼마나 지속될 진 모르겠지만 포기만 하지말고 조금씩이라도 의식하며 실천해보려고 한다.\n\n캡틴 포비가 남겨주신 글을 끝으로 회고록을 끝내보자 \n\n![](comment-of-pobi.png)"},{"excerpt":"InnoDB 스토리지 엔진 MySQL에서 사용할 수 있는 스토리지 엔진 중 거의 유일하게 레코드 기반 Lock을 지원한다. 덕분에 높은 동시성 처리와 뛰어난 성능을 가진다. 아키텍처 InnoDB의 전반적인 아키텍처는 다음과 같다.  InnoDB의 주요 특징 프라이머리 키에 의한 클러스터링 모든 InnoDB 테이블은 PK에 의해 클러스터링 인덱스가 생성된다…","fields":{"slug":"/04-innodb-engine/"},"frontmatter":{"date":"December 17, 2023","title":"InnoDB 엔진 이해하기","tags":["database"]},"rawMarkdownBody":"\n## InnoDB 스토리지 엔진\n\nMySQL에서 사용할 수 있는 스토리지 엔진 중 거의 유일하게 **레코드 기반 Lock**을 지원한다.\n\n덕분에 **높은 동시성 처리**와 **뛰어난 성능**을 가진다.\n\n### 아키텍처\n\nInnoDB의 전반적인 아키텍처는 다음과 같다.\n\n![InnoDB 아키텍처](InnoDB-Architecture.png)\n\n## InnoDB의 주요 특징\n\n### 프라이머리 키에 의한 클러스터링\n\n모든 InnoDB 테이블은 PK에 의해 클러스터링 인덱스가 생성된다.\n\n따라서 PK를 이용한 조회가 성능에 큰 영향을 끼친다.\n\n### 외래 키 지원\n\nFK를 지원한다.\n\n다만, FK를 사용할 때는 부모 테이블과 자식 테이블에 데이터 존재 유무를 체크하는 작업이 필요하다.\n\n이때 Lock이 여러 테이블로 전파되고, 이로 인해 데드락이 발생할 가능성이 많으므로 사용에 주의해야 한다.\n\n`foregin_key_checks` 시스템 변수를 OFF로 설정하면 외래 키 관계에 대한 체크 작업을 일시적으로 멈출 수 있다.<br>\n데이터 추가 및 삭제 시에, 부가적인 작업이 발생하지 않게 된다.<br>\n다만, `foregin_key_checks`가 비활성화되면 `ON DELETE CASCADE` 나 `ON UPDATE CASCADE`도 무시하게 된다.\n\n> `foregin_key_checks` 시스템 변수는 적용 범위를 GLOBAL과 SESSION 모두 설정 가능하다.<br>\n> 따라서 본 작업을 수행할 때는 반드시 현재 작업을 수행하는 SESSION에서만 외래 키 체크 기능을 해제하는 것이 안전하다.<br>\n> 명령어는 다음과 같다.<br>\n> `mysql> SET SESSION foreign_key_checks=OFF;`<br>\n> 작업이 완료되면 반드시 현재 세션을 종료하기 전, 다시 활성화해야 한다.\n\n### MVCC(Multi Version Concurrency Control)\n\nMVCC의 가장 큰 목적은, **잠금을 사용하지 않는 일관된 읽기를 제공하는 데 있다.**\n\nInnoDB는 **언두 로그를 이용해 이 기능을 구현한다.**\n\n여기서 Multi Version이라 함은 하나의 레코드에 대해 여러 개의 버전이 동시에 관리된다는 의미다.\n\n예시를 통해 더 자세히 알아보자.\n\n```sql\nCREATE TABLE member (\n    id      BIGINT PRIMARY KEY AUTO_INCREMENT,\n    name    VARCHAR(20) NOTNULL,\n    area    VARCHAR(100) NOT NUL,\n    INDEX idx_area (area)\n);\n\nINSERT INTO member VALUES ('홍길동', '서울');\n```\n\n회원 정보를 저장하는 member 테이블을 생성하고 이름은 홍길동, 지역은 서울인 레코드를 추가한다.\n\n데이터베이스의 상태는 다음과 같이 바뀔 것이다.\n\n![](insert-state.png)\n\n이후, 지역을 서울에서 경기로 수정한다.\n\n```sql\nUPDATE member SET area='경기' where id = 1;\n```\n\nUPDATE 쿼리가 실행되면 InnoDB 버퍼 풀에는 커밋 여부와 상관없이 새로운 값인 경기로 수정된다.\n\n디스크에는 체크포인트나 Write 쓰레드에 의해 새로운 값으로 수정됐을 수도, 아닐 수도 있다.\n\n아직 커밋이나 롤백이 되지 않은 상태에서 다른 세션에 의해 해당 데이터의 조회가 발생하면 데이터베이스는 어떻게 작업을 수행할까?\n\n이때는 **MySQL 서버의 시스템 변수에 설정된 격리 수준에 따라 다르다.**\n\n격리 수준이 READ_UNCOMMITED인 경우에는, 버퍼 풀의 데이터를 조회한다.\n\n격리 수준이 READ_COMMITED 혹은 REPEATABLE_READ, SERIALIZABLE인 경우에는 언두 로그의 값을 조회한다.\n\n![](after-update.png)\n\n이러한 과정을 DBMS에서 MVCC라고 표현한다.\n\n만약 UPDATE 쿼리가 COMMIT 되면 지금의 상태를 영구적인 데이터로 만든다.\n\n반대로 **ROLLBACK 되면 언두 로그에 있는 데이터를 버퍼 풀로 다시 복구하고 언두 로그의 데이터를 삭제한다.**<br>\n\n언두 로그의 데이터는는, 해당 데이터를 필요로 하는 트랜잭션이 더는 없을 때 비로소 삭제된다.\n\n### 잠금 없는 일관된 읽기(Non-Locking Consistent Read)\n\n위의 MVCC 기술을 이용해 잠금을 걸지 않고 읽기 작업을 수행한다.\n\n격리 수준이 SERIALIZABLE이 아니며 INSERT와 연결되지 않은 순수한 SELECT 작업은, 다른 트랜잭션의 변경 작업과 관계없이 항상 Lock을 대기하지 않고 바로 실행된다.\n\n### 자동 데드락 감지\n\n내부적으로 잠금 대기 목록을 그래프(Wait-for List) 형태로 관리한다.\n\n데드락 감지 쓰레드가 주기적으로 잠금 대기 그래프를 검사해 데드락에 빠진 Tx들을 찾아 그 중 하나를 강제 종료한다.\n\n이때 기준은 언두 로그 레코드를 더 적게 가진 Tx를 강제 종료한다.\n\n> InnoDB는 스토리지 엔진이기에 MySQL 엔진에서 관리되는 테이블 잠금은 볼 수가 없어 데드락 감지가 불확실할 수 있다.<br>\n> 하지만 `innodb_table_locks` 시스템 변수를 활성화하면 InnoDB 스토리지 엔진의 레코드 잠금뿐만 아니라 테이블 레벨의 잠금까지 감지할 수 있게 된다.<br>\n> 따라서 특별한 이유가 있는 것이 아니라면 위 변수를 활성화하자.\n\n일반적인 서비스에서는 데드락 감지 스레드의 작업이 부담되지 않는다.<br>\n하지만 동시에 처리해야하는 스레드의 수가 매우 많아지거나 각 Tx이 가지는 잠금이 많아지면 데드락 감지 스레드의 작업이 느려진다.\n\n데드락 감지 스레드는 잠금 목록을 검사한다.<br>\n당시의 잠금 상태가 변경되지 않도록 잠금 목록이 저장된 리스트(잠금 테이블)에 새로운 잠금을 걸게 된다.<br>\n즉, 데드락 감지 스레드가 느려지면 서비스 쿼리에도 악영향이 미치게 된다.\n\n위 문제를 해결하기 위해 `innodb_deadlock_detect` 시스템 변수가 존재하고 해당 변수를 비활성화하여 데드락 감지 스레드를 사용하지 않을 수 있다.\n\n하지만 이렇게 되면 데드락이 발생할 수 있는 가능성이 존재하기 때문에 다른 대안이 필요하다.\n\n이럴 땐 `innodb_lock_wait_timeout` 시스템 변수를 활성화할 수 있다.\n\ndefault 값은 50초인데, 이보다 훨씬 낮은 시간으로 설정하여 데드락을 방지하자.\n\n### 자동화된 장애 복구\n\nMySQL 서버가 시작될 때 완료되지 못한 Tx이나 디스크에 일부만 기록된 데이터 페이지 등에 대한 일련의 복구 작업이 알아서 진행된다.\n\n만약 자동으로 복구될 수 없는 정도의 손상이라면 자동 복구를 멈추고 MySQL 서버를 종료시킨다.\n\n### InnoDB 버퍼 풀\n\n**InnoDB 스토리지 엔진에서 가장 핵심적인 부분이다.**\n\n**디스크의 데이터 파일이나 인덱스 정보를 메모리에 캐시해 두는 공간이다.**\n\n쓰기 작업을 지연시켜 일괄 작업으로 처리할 수 있도록 하는 버퍼의 역할도 수행한다.\n\n#### 버퍼 풀의 크기 설정\n\n버퍼 풀의 크기는 적절히 작은 값부터 시작해 조금씩 큰 값으로 설정하는 것이 최적의 방법이다.\n\n기존에 사용하는 MySQL 서버가 있다면 해당 서버의 메모리 설정을 기준으로 크기를 조정하자.<br>\n만약 처음으로 MySQL 서버를 준비한다고 했을 땐, 운영체제의 전체 메모리 공간이 8GB 미만이라면 50% 정도로 설정한다.<br>\n전체 메모리 공간이 8GB 이상이라면 50% 에서 시작해서 조금씩 올려가며 최적점을 찾는다.\n\n`innodb_buffer_pool_size` 시스템 변수로 크기를 설정할 수 있다.\n\n하지만 버퍼 풀 크기 변경은 크리티컬한 변경이므로 서버가 한가한 시간에 수행해야 한다.<br>\n그리고 용량을 더 크게 변경하는 건 시스템 영향도가 크진 않지만 줄이는 작업은 영향도가 매우 크므로 가능한 하지 않도록 하자.\n\n버퍼 풀은 내부적으로 128MB 청크 단위로 쪼개어 관리된다.<br>\n이는 버퍼 풀의 크기를 변경하는 단위로 사용된다.\n\n#### 버퍼 풀의 구조\n\n버퍼 풀은 3개의 자료 구조를 관리한다.\n\n- LRU(Least Recently Used) List\n  - 디스크로부터 한 번 읽어온 페이지를 최대한 오랫동안 보관하여 디스크 IO를 최소화하기 위해 사용\n- Flush List\n  - 디스크로 동기화되지 않은 데이터를 가진 데이터 페이지(더티 페이지)의 변경 시점 기준의 페이지 목록을 관리\n- Free List\n  - 실제 사용자 데이터로 채워지지 않은 비어 있는 페이지들의 목록\n  - 사용자의 쿼리가 새롭게 디스크의 데이터 페이지를 읽어와야 하는 경우 사용\n\n### Double Write Buffer\n\nInnoDB의 리두 로그는 공간을 효울적으로 사용하기 위해 페이지의 변경된 내용만 기록한다.<br>\n이로 인해 더티 페이지를 디스크 파일로 플러시할 때 일부만 기록되는 문제가 발생하면 그 페이지의 내용은 복구할 수 없을 수도 있다.<br>\n이렇게 페이지가 일부만 기록되는 현상을 Partial-page 또는 Torn-page 라고 하는데, 하드웨어의 오작동이나 시스템 비정상 종료 등올 발생할 수 있다.\n\n이 같은 문제를 막기 위해 사용하는 기법이 Double-Write 기법이다.\n\nInnoDB 버퍼 풀에 존재하는 더티 페이지를 한 번의 디스크 쓰기로 시스템 테이블스페이스의 DoubleWrite 버퍼에 기록한다.<br>\n그리고 각 더티 페이지를 데이터 파일에 하나씩 랜덤으로 쓰기를 실행한다.\n\n이렇게 시스템 테이블스페이스의 DoubleWrite 버퍼에 기록된 변경 내용은 디스크에 모두 정상적으로 기록되면 필요가 없어진다.<br>\n하지만 실제 데이터 파일의 쓰기가 중간에 실패할 때만 원래의 목적으로 사용된다.\n\nInnoDB 스토리지 엔진은 재시작될 때 항상 DoubleWrite 버퍼의 내용과 디스크 데이터 파일을 모두 비교해서 다른 내용을 담고 있는 페이지가 존재한다면, DoubleWrite 버퍼의 내용을 디스크 데이터 파일의 페이지로 복사한다.\n\n### 언두 로그\n\n트랜잭션과 격리 수준을 보장하기 위해 **변경되기 이전 버전의 데이터를 별도로 백업한다.**<br>\n이렇게 **백업된 데이터를 언두 로그라고 한다.**\n\n### 체인지 버퍼\n\n레코드가 INSERT 되거나 UPDATE 될 때는 데이터 파일 뿐만 아니라 해당 테이블에 포함된 인덱스를 업데이트하는 작업도 필요하다.<br>\n하지만 인덱스를 업데이트하는 작업은 디스크를 랜덤하게 읽는 작업이 포함된다.\n\n변경해야 할 인덱스 페이지가 버퍼 풀에 있으면 바로 업데이트를 수행하지만, 그렇지 않고 디스크로부터 읽어와서 업데이트해야 하는 상황이라면 임시 공간에 저장해두고 곧바로 사용자에게 결과를 반환하는 형태로 성능을 향상시킨다.<br>\n이때의 임시 공간이 체인지 버퍼다.\n\n### 리두 로그 및 로그 버퍼\n\nACID 중, 영속성과 밀접한 관련이 있다.<br>\n모종의 이유로 인해 MySQL 서버가 비정상적으로 종료됐을 때 데이터 파일에 기록되지 못한 데이터를 복구할 때 사용하는 것이 리두 로그다.\n\n대부분의 데이터베이스는 데이터 변경 내용을 로그로 먼저 기록한다.<br>\n이유는, 보통 쓰기보다 읽기의 성능을 고려한 자료 구조를 가지고 있기 때문에 데이터 파일 쓰기는 랜덤 디스크 액세스가 필요하다.<br>\n그래서 변경된 데이터를 데이터 파일에 기록하려면 상대적으로 큰 비용이 필요하다.\n\n이로 인한 **쓰기 성능 저하를 막기 위해 데이터베이스 서버는 쓰기 비용이 낮은 자료 구조인, 리두 로그를 가진다.**\n\n비정상 종료가 발생하면 리두 로그의 내용을 이용해 데이터 파일을 다시 서버가 종료되기 전으로 복구한다.\n\n### 어댑티브 해시 인덱스\n\nInnoDB 스토리지 엔진에서 **사용자가 자주 요청하는 데이터에 대해 자동으로 생성하는 인덱스이다.**\n\n어댑티브 해시 인덱스는 B-Tree 검색 시간을 줄여주기 위해 도입된 기능이다.\n\n자주 읽히는 데이터 페이지의 키 값을 이용해 해시 인덱스를 만들고, 필요할 때마다 어댑티브 해시 인덱스를 검색해서 해당 레코드가 저장된 데이터 페이지를 즉시 찾아갈 수 있다.\n\n해시 인덱스는 '인덱스 키 값'과 해당 인덱스 키 값이 저장된 '데이터 페이지 주소'의 쌍으로 관리된다.\n\n인덱스 키 값은, 'B-Tree 인덱스의 고유 번호'와 'B-Tree 인덱스의 실제 키 값'의 조합으로 생성된다.\n\n데이터 페이지 주소는, 실제 키 값이 저장된 데이터 페이지의 메모리 주소를 가진다. 이는 InnoDB 버퍼 풀에 로딩된 페이지의 주소를 의미한다. 따라서 어댑티브 해시 인덱스는 버퍼 풀에 올려진 데이터 페이지에 대해서만 관리되고, 버퍼 풀에 해당 데이터 페이지가 없어지면 어댑티브 해시 인덱스에서도 해당 데이터 페이지에 대한 정보가 사라진다."},{"excerpt":"MySQL 엔진 아키텍처  MySQL 서버는 크게 MySQL 엔진과 스토리지 엔진으로 나뉜다. MySQL 엔진 클라이언트와의 연결, 요청된 쿼리문을 파악하고 분석 및 최적화하는 작업을 수행한다. 그 후, 실제 동작은 스토리지 엔진에 요청한다. 이를 핸들러 요청이라고 하고, 여기에 사용되는 API를 핸들러 API라고 한다. 명령어로 핸들러 API를 확인할 …","fields":{"slug":"/04-mysql-engine-architecture/"},"frontmatter":{"date":"December 14, 2023","title":"MySQL 엔진 아키텍처","tags":["database"]},"rawMarkdownBody":"\n## MySQL 엔진 아키텍처\n\n![MySQL의 전체 구조](entire-structure-mysql.jpeg)\n\nMySQL 서버는 크게 **MySQL 엔진**과 **스토리지 엔진**으로 나뉜다.\n\n### MySQL 엔진\n\n클라이언트와의 연결, 요청된 쿼리문을 파악하고 분석 및 최적화하는 작업을 수행한다.\n\n그 후, 실제 동작은 *스토리지 엔진*에 요청한다.\n\n이를 *핸들러 요청*이라고 하고, 여기에 사용되는 API를 *핸들러 API*라고 한다.\n\n```sql\nmysql> `SHOW GLOBAL STATUS LIKE Handler%`;\n```\n\n명령어로 핸들러 API를 확인할 수 있다.\n\nMySQL 엔진은 아래와 같은 기능들로 이루어져 있다.\n\n- 커넥션 핸들러\n- SQL 인터페이스\n- SQL 파서\n- SQL 옵티마이저\n- 캐시 & 버퍼\n\n### 스토리지 엔진\n\n종류로는 MyISAM, InnoDB 스토리지 엔진이 있다.\n\n실제 데이터를 디스크에서 읽거나 쓰는 작업을 수행한다.\n\n## MySQL 스레딩 구조\n\nMySQL은 프로세스 기반이 아닌 쓰레드 기반으로 동작한다.\n\n```sql\nmysql> SELECT thread_id, name, type, processlist_user, processlist_host FROM performance_schema.threads ORDER BY type, thread_id;\n```\n\n위 명령어로 실행 중인 쓰레드를 확인할 수 있다.\n\n### Foreground Thread\n\n클라이언트의 요청을 처리하는 쓰레드이다. 때문에 클라이언트 쓰레드 혹은 사용자 쓰레드라고도 부른다.\n\n클라이언트의 요청을 모두 처리하면, 쓰레드 캐시로 돌아간다.<br>\n이때, 쓰레드 캐시가 가득 찬 상태라면 쓰레드를 종료시킨다.\n\n데이터를 MySQL의 **데이터 버퍼**나 **캐시**로부터 가져오는 작업을 수행한다.\n\n하지만 버퍼나 캐시에 데이터가 없는 경우, InnoDB 기준으로는 직접 디스크에서 데이터를 가져오는 작업은 **백그라운드 쓰레드**에서 처리된다.\n\n### Background Thread\n\nInnoDB 기준, 아래와 같은 여러 작업들이 백그라운드 쓰레드에 의해 처리된다.\n\n- Insert Buffer를 병합하는 작업\n- 로그를 디스크에 쓰는 작업\n- InnoDB 버퍼 풀의 데이터를 디스크에 쓰는 작업\n- 데이터를 버퍼로 읽어오는 작업\n- 잠금이나 데드락을 모니터링하는 작업\n\n아울러, 주로 디스크에 쓰기 작업을 수행하기 때문에 백그라운드 쓰레드의 개수는 적정 수를 유지하는 것이 중요하다.\n\n쓰기 작업은 버퍼링을 통해 일괄 작업해도 괜찮다. 오히려 효율적이다. 따라서 이러한 방식으로 처리한다.\n\n## 메모리 할당 및 사용 구조\n\n### 글로벌 메모리 영역\n\nMySQL 서버가 실행되면서 운영체제로부터 할당받는 공간이다.\n\n필요에 따라 2개 이상의 메모리 공간을 할당받을 수 있다.\n\n글로벌 메모리 영역이 2개 이상이더라도 모든 쓰레드에 의해 공유된다.\n\n아래의 기능들이 글로벌 메모리 영역에 존재한다.\n\n- 테이블 캐시\n- InnoDB 버퍼 풀\n- InnoDB 어댑티브 해시 인덱스\n- InnoDB 리두 로그 버퍼\n\n### 로컬 메모리 영역\n\n클라이언트 쓰레드가 쿼리를 처리할 때 사용하는 메모리 영역이기에, 클라이언트 메모리 영역이라고도 한다.<br>\n클라이언트와의 커넥션을 *세션*이라고도 하기에, 세션 메모리 영역이라고도 한다.\n\n로컬 메모리는 각 클라이언트 쓰레드 별로 독립적으로 할당되며 사용된다.\n\n대표적인 로컬 메모리 영역은 다음과 같다.\n\n- 정렬 버퍼\n- 조인 버퍼\n- 바이너리 로그 캐시\n- 네트워크 버퍼\n\n## 쿼리 실행 구조\n\n클라이언트로부터 쿼리 요청이 들어오면 다음과 같은 순으로 처리된다.\n\n쿼리 파서 -> 전처리기 -> 옵티마이저 -> 쿼리 실행기\n\n### 쿼리 파서\n\n요청으로 넘어온 SQL문을 MySQL이 인식할 수 있는 최소한의 단위인 토큰으로 분리해 트리 구조로 만드는 작업을 수행한다.\n\nSQL 문법 오류는 쿼리 파서에 의해 감지되고 에러를 발생한다.\n\n### 전처리기\n\n쿼리 파서의 결과인 *파서 트리*를 기반으로 쿼리 문장에 구조적 문제를 파악한다.\n\n테이블이 존재하는지? 칼럼이 존재하는지? 권한이 있는지? 와 같이, 실제 쿼리를 동작했을 때 이상이 없을 지를 판단한다.\n\n### 옵티마이저\n\nSQL문을 가장 효율적으로 실행하기 위한 **실행 계획**을 수립한다.\n\n### 쿼리 실행기\n\n실행 계획을 바탕으로 스토리지 엔진에게 작업을 요청하고, 그 결과를 사용자에게 반환한다. "},{"excerpt":"세 줄 요약 코드의 가독성이 좋아진다.  코드를 객체지향으로 짤 수 있다. 따라서 유지보수가 쉽다. 원시값과 문자열을 포장하지 않았을 때 이 코드를 보고 파라미터로 넘어가는 각 값들이 어떤 값들인지 바로 파악할 수 있을까? 적어도 해당 프로젝트에 대한 이해도가 없다면 절대 불가능할 것이다. 물론 아래 코드처럼 각 파라미터를 변수로 할당하고 변수명으로 유추…","fields":{"slug":"/why-wrap-primitive-type-and-string/"},"frontmatter":{"date":"December 13, 2023","title":"모든 원시값과 문자열을 포장한다. 왜?","tags":["java","oop","clean-code"]},"rawMarkdownBody":"\n## 세 줄 요약\n\n1. 코드의 가독성이 좋아진다. \n2. 코드를 객체지향으로 짤 수 있다.\n3. 따라서 유지보수가 쉽다.\n\n## 원시값과 문자열을 포장하지 않았을 때\n\n```java\nnew RacingGame(List.of(\"doggy\", \"dazzle\", \"0chil\"), 10);\n```\n\n이 코드를 보고 파라미터로 넘어가는 각 값들이 어떤 값들인지 바로 파악할 수 있을까?\n\n적어도 해당 프로젝트에 대한 이해도가 없다면 절대 불가능할 것이다.\n\n물론 아래 코드처럼 각 파라미터를 변수로 할당하고 변수명으로 유추할 수 있다.\n\n```java\nList<String> players = List.of(\"doggy\", \"dazzle\", \"0chil\");\nint playCount = 10;\n\nnew RacingGame(players, playCount);\n```\n\n그렇다면 아래 코드를 한번 보자\n\n```java\nPlayers players = new Players(List.of(\"doggy\", \"dazzle\", \"0chil\"));\nPlayCount playCount = new PlayCount(10);\n\nnew RacingGame(players, playCount);\n```\n\n분명 변수명은 같지만, 타입이 주는 힘이 느껴진다.\n\n## 반복되는 행위를 수행할 수 있다\n\n만약 위 코드에서 `List<String> players`와 `int playCount`가 넓은 범위에서 사용되면서, 동시에 각 변수를 사용하는 데 있어 반복되는 행위가 있다면 어떨까?\n\n가령, 다양한 곳에서 각 Player들에게 playCount만큼 게임을 Play하는 행위가 이뤄져야 한다면 각 변수가 존재하는 메서드마다 실행 코드를 작성해줘야 할 것이다.\n\n하지만 값을 포장하여 새로운 객체를 만든다면, 객체 간의 협력을 통해 충분히 수행할 수 있을 것이다.\n\n이는 유지보수 관점에서 큰 강점이다!\n\n---\n\n추가로, 만약 playCount가 몇 회 이하여야 한다는 요구사항이 있을 경우를 생각해보자.\n\n해당 경우에도 `PlayCount` 객체에서 애초에 객체를 생성할 때 검증을 해주면 된다.\n\n그러면 해당 객체를 사용하는 모든 곳에서 검증을 해주지 않아도 된다!\n\n## 일급 컬렉션\n\n일급 컬렉션은 컬렉션을 감싼 객체이다.\n\n사실, 일급 컬렉션도 값을 포장한 객체이므로 위에서 언급한 장점들이 모두 적용된다고 볼 수 있다."},{"excerpt":"문제 상황 인스턴스 필드로 List만을 가지는 일급 컬렉션 객체가 있습니다. 필드에   키워드가 존재하기 떄문에 항상 초기화를 해줘야 합니다. 따라서 객체를 생성하려면 생성자를 List를 항상 주입받아야 합니다. 이때, 문제가 발생합니다.  위 테스트 코드를 보면 MyList 인스턴스를 생성할 때,  라는 List가 생성자 파라미터로 전달됩니다. 그렇게 …","fields":{"slug":"/cautions-for-using-collections-in-Java/"},"frontmatter":{"date":"December 11, 2023","title":"Java에서 컬렉션을 사용할 때 주의할 점","tags":["java"]},"rawMarkdownBody":"\n## 문제 상황\n\n```java\npublic final class MyList {\n\n    private final List<String> list;\n\n    public MyList(List<String> list) {\n        this.list = list;\n    }\n\n    public void add(String value) {\n        list.add(value);\n    }\n    \n    public int size() {\n        return list.size();\n    }\n    \n    public List<String> list() {\n        return list;\n    }\n}\n```\n\n인스턴스 필드로 List만을 가지는 일급 컬렉션 객체가 있습니다.\n\n필드에 `final`  키워드가 존재하기 떄문에 항상 초기화를 해줘야 합니다.\n\n따라서 객체를 생성하려면 생성자를 List를 항상 주입받아야 합니다.\n\n이때, 문제가 발생합니다.\n\n![문제 상황](problem.png)\n\n위 테스트 코드를 보면 MyList 인스턴스를 생성할 때, `list` 라는 List가 생성자 파라미터로 전달됩니다.\n\n그렇게 `myList` 라는 이름의 인스턴스가 생성된 후, `list`에 \"c\"를 추가합니다.\n\n그런데 검증문을 보면, `myList` 인스턴스의 필드에는 `a, b, c`가 존재합니다.\n\n이 문제는 `myList`가 가지는 필드와, `list` 컬렉션이 **같은 참조 값을 가지기 때문입니다.**\n\n> 참조 값에 대한 내용은 [테코톡 - 히이로의 불변](https://www.youtube.com/watch?v=AjpJS9WrDrs) 추천드립니다\n\n결국 객체 내부의 참조값과 외부의 참조값을 끊어주는 작업이 필요합니다.\n\n이를 **방어적 복사**라고 합니다.\n\n## 방어적 복사 첫 번째 방법: 새로운 객체 할당 (new)\n\n```java\npublic final class MyList {\n\n    private final List<String> list;\n\n    public MyList(List<String> list) {\n        this.list = new ArrayList<>(list);\n    }\n}\n```\n\n메서드 파라미터로 외부의 컬렉션이 들어올 때, **새로운 컬렉션을 만들어** 그 안에 값을 복사하는 방식으로 해결할 수 있습니다.\n\n![새로운 객체 할당](defensive_copy.png)\n\n## List.copyOf()\n\n또 다른 방식으로 `List.copyOf()` 메서드가 있습니다.\n\n```java\npublic final class MyList {\n\n    private final List<String> list;\n\n    public MyList(List<String> list) {\n        this.list = List.copyOf(list);\n    }\n}\n```\n\n![List.copyOf()](list_copyof.png)\n\n하지만 이 메서드의 경우, 위 테스트 코드에서 볼 수 있듯이 값을 쓰거나 수정하거나 삭제하는 메서드에 대해서는 `UnsupportedOperationException`을 발생합니다.\n\n그 이유로는 `copyOf()` 메서드를 까보면 최종적으로 `ImmutableCollections.ListN<>()` 메서드가 호출되는데요.<br>\n`ImmutableCollections.ListN<E>`는 `AbstractImmutableList<E>`를 상속하고 있으며<br>\n`AbstractImmutableList<E>`는 아래와 같이 쓰기 작업에 대해 예외를 던지고 있기 때문입니다.\n\n![AbstractImmutableList의 쓰기 작업 메서드들](AbstractImmutableList.png)\n\n추가로 `List.copyOf()`도 새로운 객체가 생성되고 값을 복사하는 *방어적 복사*가 이루어집니다.\n\n아래 사진은 `ImmutableCollections.ListN<>()`의 생성자입니다.\n\n![List.copyOf()의 최종 실행 메서드](last_method_of_copyof.png)\n\n가변 인자로 값을 받고 새로운 객체를 만들고 모든 요소에 대해 null 체크를 하며 값을 복사합니다.\n\n결과적으로 다른 객체가 생성되어 외부 컬렉션과 참조가 끊어집니다.\n\n## Collections.unmodifiableList()\n\n이 메서드를 사용하면 읽기 전용 컬렉션을 만들 수 있습니다.\n\n```java\npublic final class MyList {\n\n    private final List<String> list;\n\n    public MyList(List<String> list) {\n        this.list = Collections.unmodifiableList(list);\n    }\n}\n```\n\n![Collections.unmodifiableList()](unmodifiable_list.png)\n\n하지만 `List.copyOf()`와 비교해서 한 가지 중요한 점이 있습니다.\n\n아래 테스트 코드처럼, 외부에 존재하는 컬렉션의 값을 수정하면 그대로 `myList`에 반영이 된다는 사실입니다.\n\n![Collections.unmodifiableList()의 약점](weekpoint_of_unmodifialeList.png)\n\n그 원인은 다음과 같습니다.\n\n![Collections.unmodifiableList()의 최종 실행 메서드](last_method_of_unmodifiableList.png)\n\n파라미터로 넘어오는 외부 컬렉션인 `list`를 아무런 조치없이 그대로 할당하는 것을 볼 수 있습니다.\n\n따라서, 내부적으로는 값의 변경이 막혀있지만, 외부의 컬렉션과는 같은 레퍼런스를 참조하고 있다는 것을 확인할 수 있습니다."},{"excerpt":"우아한테크코스의 모든 미션에 \"배열 대신 컬렉션을 사용하라\"는 요구 사항이 항상 존재한다. 그 이유를 알아보자. 두 줄 요약 Java가 이미 잘 만들어놓은 JCF가 존재한다. 제네릭을 지원한다. JCF  Java는 다양한 자료구조와 다양한 알고리즘을 지원한다. 예를 들어 동적 배열은 , 연결 리스트는  처럼 말이다. 게다가 각 자료구조에 필요한 동작들, …","fields":{"slug":"/why-use-collection-instead-of-array/"},"frontmatter":{"date":"December 11, 2023","title":"배열 대신 컬렉션을 쓴다. 왜?","tags":["java","clean-code"]},"rawMarkdownBody":"\n우아한테크코스의 모든 미션에 \"배열 대신 컬렉션을 사용하라\"는 요구 사항이 항상 존재한다.\n\n그 이유를 알아보자.\n\n## 두 줄 요약\n\n1. Java가 이미 잘 만들어놓은 [JCF](https://steady-coding.tistory.com/354)가 존재한다.\n2. 제네릭을 지원한다.\n\n## JCF\n\n![JCF 계층 구조](hierarchy-of-jcf.png)\n\nJava는 다양한 자료구조와 다양한 알고리즘을 지원한다.\n\n예를 들어 동적 배열은 `ArrayList`, 연결 리스트는 `LinkedList` 처럼 말이다.\n\n게다가 각 자료구조에 필요한 동작들, 예를 들어 중간에 값 삽입 혹은 삭제와 같은 동작들 역시 미리 제공한다.\n\n뿐만 아니라, `isEmpty()`, `sort()`, `clear()`, `contains()` 같은 메서드가 존재한다.\n\n실제로 개발하다보면 위 메서드들이 정말 많이 사용되는데, 만약 컬렉션을 사용하지 않고 배열을 사용한다면 위 메서드들을 **재발명**해야 할 것이다. ~~그러면 퇴근이 늦어질 것이다.~~\n\n따라서 상황에 따라 필요한 자료구조와 제공되는 메서드를 사용한다면 배열을 사용하는 것보다 훠어얼씬 **효율적인 개발**을 할 수 있을 것이다.\n\n추가로 `java.util.concurrent` 패키지에서 제공하는 컬렉션들을 통해 **Thread-safe한 자료구조**도 편하게 사용할 수 있다.\n\n또 `Collections` 클래스 내에 `UnmodifiableXXX` 를 통해 불변식 또한 지킬 수 있다.\n\n![Unmodifiable Collections](unmodifable-collections.png)\n\n## 제네릭을 지원한다\n\n제네릭은 JAVA가 제공하는 유연하고 안전한 기술 중 하나이다.\n\n컬렉션은 제네릭을 지원한다. 하지만 배열은 그렇지 않다.\n\n> 제네릭에 대한 이해는 [이 포스팅](https://kdkdhoho.github.io/about-generic) 참고\n\n이 제네릭을 사용한다는 것은, 개발자로 하여금 **컴파일 타임에 타입 안정성을 보장할 수 있다는 것**이다.\n\n이는 어마어마한 메리트이다. 에러가 생겨도 가장 좋은 타이밍은 **컴파일 타임**인데, 제네릭이 이를 보장한다.<br>\n~~일단 이거만으로도 배열 대신 컬렉션을 쓸 이유가 충분해보인다.~~\n"},{"excerpt":"지난 번엔 Garbage Collection과 동작 원리에 대해 알아보았다. 이번에는 어떤 GC 알고리즘이 있고, 각 알고리즘의 특징은 어떤지 알아보자. Serial Collector 단일 쓰레드를 사용하여 모든 GC 작업을 수행하므로 작업 중, 쓰레드 간 통신 오버헤드가 없다. 쓰레드 간 통신은 데이터를 전송하고 조율하는 과정을 의미한다.\n쉽게 말해 쓰…","fields":{"slug":"/garbage-collection-algorithms/"},"frontmatter":{"date":"December 10, 2023","title":"Garbage Collection 알고리즘","tags":["java","garbage-collection"]},"rawMarkdownBody":"\n지난 번엔 [Garbage Collection과 동작 원리](https://kdkdhoho.github.io/about-garbage-collection/)에 대해 알아보았다.\n\n이번에는 어떤 GC 알고리즘이 있고, 각 알고리즘의 특징은 어떤지 알아보자.\n\n## Serial Collector\n\n단일 쓰레드를 사용하여 모든 GC 작업을 수행하므로 작업 중, 쓰레드 간 통신 오버헤드가 없다.\n\n> 쓰레드 간 통신은 데이터를 전송하고 조율하는 과정을 의미한다.<br>\n> 쉽게 말해 쓰레드 간 동기화 작업으로 인한 오버헤드가 발생한다고 이해하면 될 것 같다.\n\n결과적으로 Serial GC 쓰레드의 작업 시간이 프로그램의 응답 시간에 직접적인 영향을 준다.\n\n싱글 프로세서 환경에 적합한 알고리즘이다.<br>\n하지만 처리하는 데이터 셋의 용량이 최대 약 100MB인 경우 멀티 프로세서에서도 유용하게 쓰일 수 있다.\n\nSerial Collector는 특정 하드웨어 및 OS 구성에서 default로 선택되거나,<br>\n`-XX:+UseSerialGC` 옵션을 통해 명시적으로 사용할 수 있다.\n\nJava 8, 11, 17 버전에서 사용 가능하다.\n\n추가로 Major Collection 이후 파편화 된 메모리를 압축해 Hole을 최대화 하는 _Compaction_ 과정이 발생한다.\n\n## Parallel Collector\n\n_Throughput GC_ 라고도 부른다.\n\n*Serial Collector*와 유사하게 동작한다.\n\n하지만 Parallel Collector의 경우 멀티 쓰레드로 GC 작업을 진행한다는 점에서 큰 차이가 있다.\n\n![Serial Collector와 Parallel Collector의 차이 ([출처](https://d2.naver.com/helloworld/1329)](difference_serial_and_parallel.png)\n\n멀티 프로세서 환경에 적합한 알고리즘이다.\n\n`-XX:+UseParallelGC` 옵션을 통해 명시적으로 사용할 수 있다.\n\nJava 8, 11 17 버전에서 사용 가능하다.\n\n## CMS Collector\n\n*Concurrent Mark Sweep*의 약자이다.\n\n이름에서 유추할 수 있듯이, 필요하지 않은 객체를 Mark 하고 제거하는 Sweep 하는 과정을 다른 쓰레드와 동시에 처리한다.\n\n아래 그림을 보면 *Serial Collector* 와 비교하는 그림이 있다.\n\n그림에서 볼 수 있듯이, Initial Mark 와 Remark 시간을 제외하고는 GC 작업이 다른 쓰레드와 동시에 처리된다. \n\n![Serial Collector와 CMS Collector](cms-image.png)\n\nGC 시간이 매우 짧다는 특징이 있다. 따라서 지연 시간이 매우 중요한 시스템의 경우 고려할 수 있다.\n\n하지만, 다른 GC 방식에 비해 더 많은 CPU와 메모리를 사용하게 되고, 무엇보다 Memory Compaction이 이루어지지 않아 더 큰 문제가 발생할 수 있다.\n\nJava 9 버전부터는 Deprecated 되어 Java 8 버전에서만 사용 가능하다.\n\n## G1 Collector\n\n*Garbage First Collector* 라고도 불리는 G1 Collector는 대용량의 메모리와 멀티 프로세스 환경을 위해 탄생한 알고리즘이다.\n\n지금까지 알아본 알고리즘 중, 성능 면에서 가장 뛰어난 알고리즘이다.<br>\n따라서 **Java 9부터 Default GC로 설정되었다.**\n\nG1은 기본적으로 Young Generation 과 Old Generation 으로 영역을 구분짓는 건 동일하다.<br>\n하지만 GC 작업이 효율이 좋은 Young Generation에 집중된다.\n\n처리량 향상을 위해 일부 연산은 항상 Stop-the-world 상태에서 수행한다.<br>\n하지만 *Global Marking*과 같은 힙 전체 영역을 대상으로 하는 작업은 다른 쓰레드와 병렬적으로 수행된다.<br>\n\nG1의 가장 큰 특징은, 이전의 프로그램 동작 및 GC로 인한 Stop-the-world의 정보를 추적한다.<br>\n추적한 정보를 바탕으로 Stop-the-world 상태에서 수행한 작업의 양을 조절한다.<br>\n즉, Garbage가 가장 많이 차있는 영역과 같이 가장 효율적인 영역을 우선적으로 처리한다.<br>\n*Garbage First Collector*라고 불리는 이유가 여기에 있는 것이다.\n\nGC 작업이 수행될 때, 생존한 객체들은 다른 영역으로 복사하고 Compacting 작업이 이뤄진다.<br>\n이동이 끝난 뒤 기존 영역은 재사용된다.<br>\n\nG1 알고리즘은 기존 Heap 메모리 구조를 물리적으로 나누어 각 영역 별로 관리하던 방식과는 다르다.<br>\n아래 그림처럼 Heap 영역을 동일한 크기의 논리적인 *Region*으로 나누어 각 영역을 관리한다.\n\n![힙 영역 레이아웃](garbage_collector_heap_layout.png)\n\n위 그림에서 빨간색으로 칠해진 Region은 Young Genration, 파란색은 Old Generation이다.\n\n빨간색 중에서도 \"S\"가 존재하는 영역은 Survivor 영역이다.\n\n파란색 중에서도 \"H\"가 존재하는 영역은 *Homongous*로, 여러 영역에 걸쳐 존재하는 영역임을 의미한다.<br>\n이 영역은 애플리케이션에서 직접 할당하는 거대한 객체이다.\n\n위 구조에서 G1 Collector의 GC 작업은 크게 *Young-only* 와 *Space Reclamation* 으로 나뉜다.\n\n![G1 Collector의 진행 싸이클](garbage-collection-cycle-overview.png)\n\n- Young-only\n  1. 일반적인 Minor Collection으로 시작한다.\n  2. 이때, Old Generation의 힙 점유율이 임계값을 넘으면 Space Reclamation으로 전환한다.\n  3. 동시에 Minor Collection 대신 *Concurrent Start Minor Collection*을 수행한다.\n\n> _Concurrent Start Minor Collection_<br>\n> Minor Collection 외에도 Marking 작업을 수행한다.<br>\n> 위에서의 Marking 작업은 Space-Reclamation 작업을 위해 **Old Generation**에서 Reachable한 객체들은 유지한다.\n\n- Space-Reclamation\n  1. 이 단계에서는 여러 개의 Collection이 혼합되어 구성된다. 즉, Old Generation에 존재하는 객체들도 GC 작업에 포함된다.\n  2. G1이 더 이상 Old Generation을 작업해도, 가치있는 공간이 확보되지 않는다고 판단하면 종료된다.\n  3. 이 단계가 끝나면 Young-only 단계가 다시 시작된다.\n\n백업의 개념으로, 필요없어진 객체를 찾다가 메모리가 부족해지면 다른 Collector와 마찬가지로 *Full GC*를 수행한다.\n\n`-XX:+UseG1GC` 옵션을 통해 명시적으로 사용할 수 있다.\n\nJava 11, 17에서 사용 가능하다.\n\n> ### 참고\n> - https://youtu.be/FMUpVA0Vvjw?feature=shared <br>\n> - https://youtu.be/vZRmCbl871I?feature=shared <br>\n> - https://mangkyu.tistory.com/118 <br>\n> - https://mangkyu.tistory.com/119 <br>\n> - https://mangkyu.tistory.com/120 <br>\n> - https://docs.oracle.com/en/java/javase/17/gctuning/available-collectors.html#GUID-45794DA6-AB96-4856-A96D-FDE5F7DEE498 <br>\n> - https://stackoverflow.com/questions/70664562/criteria-for-default-garbage-collector-hotspot-jvm-11-17 <br>\n> - https://medium.com/javarevisited/java-17-vs-java-11-exploring-the-latest-features-and-improvements-6d13290e4e1a <br>\n> - https://www.optaplanner.org/blog/2021/09/15/HowMuchFasterIsJava17.html <br>\n> - https://www.baeldung.com/jvm-garbage-collectors <br>"},{"excerpt":"GC에서 고려할 성능 GC의 주요 측정 기준은 처리량과 지연 시간이다. 처리량은 오랜 시간동안 측정하며, (GC에 소요되지 않는 시간 / 전체 시간) * 100을 의미한다. 지연 시간은 애플리케이션의 응답 속도이다. 즉, GC를 얼마나 빠르고 더 적은 횟수로 수행할 것인가가 GC 튜닝에서 고려할 포인트가 되는 것이다. GC 튜닝에 대한 내용은 언젠가 알아…","fields":{"slug":"/garbage-collection-tuning/"},"frontmatter":{"date":"December 09, 2023","title":"Garbage Collection 튜닝에 대해","tags":["java","garbageccollection"]},"rawMarkdownBody":"\n## GC에서 고려할 성능\n\nGC의 주요 측정 기준은 **처리량**과 **지연 시간**이다.\n\n처리량은 오랜 시간동안 측정하며, **(GC에 소요되지 않는 시간 / 전체 시간) * 100**을 의미한다.\n\n지연 시간은 애플리케이션의 응답 속도이다.\n\n즉, **GC를 얼마나 빠르고 더 적은 횟수로 수행할 것인가가 GC 튜닝에서 고려할 포인트**가 되는 것이다.\n\n> ~~GC 튜닝에 대한 내용은 언젠가 알아보자~~\n\n하지만 프로그램 특징에 따라 더 중요하게 여겨지는 기준은 달라질 것이다.\n\n가령 웹서버의 경우 처리량은 네트워크 지연 현상에 가려질 수 있기에 지연 시간은 중요하게 생각하지 않을 수 있다.\n\n하지만 GUI와 같은 실시간성이 아주 중요한 서비스의 경우, 처리량보다 지연 시간이 더 큰 가치가 될 것이다.\n\n따라서 애플리케이션 특징에 따라 우선 순위를 두는 것이 좋다.\n\n## Young, Old Generation 영역의 크기에 따른 비교\n\n처리량과 지연 시간은 Young Genration과 Old Generation 영역의 크기 비율을 어떻게 가져가냐에 따라 달라진다.\n\n*Young Generation*의 비율이 커질수록 GC가 발생하는 빈도는 낮아지기에 처리량은 증가할 수 있다.<br>\n하지만 그만큼 *Old Generation*의 비율이 작아지고, 이는 *Major Collection*의 빈도가 증가하여 지연 시간에 부정적인 영향을 끼칠 것이다.\n\n반대로 *Young Generation*의 비율이 작아질수록 *Minor Collection*의 빈도가 증가하고, 이는 처리량에 부정적 영향을 끼친다.<br>\n하지만 그만큼 *Old Genration*의 비율이 커지고, 이는 *Major Collection*의 빈도가 줄어들어 지연 시간에 긍정적 영향을 끼칠 것이다.\n\n// TODO: 아래 링크 보고 정리해보자\n> 보다 자세한 내용은 [이 링크](https://docs.oracle.com/en/java/javase/17/gctuning/factors-affecting-garbage-collection-performance.html#GUID-5508674B-F32D-4B02-9002-D0D8C7CDDC75)를 참고\n\n// TODO: 아래 링크로부터 이어서 글 작성\nhttps://docs.oracle.com/en/java/javase/17/gctuning/garbage-collector-implementation.html#GUID-A24775AB-16A3-4B86-9963-76E5AC398A3E\n\n## 유의사항\n\n### 1. GC로 인한 Stop-the-world를 최소화하자\n\n### 2. GC는 무적이 아니다.\n\nhttps://techblog.woowahan.com/2628/\nhttps://www.baeldung.com/jvm-garbage-collectors\n\n### 2. 무적의 GC는 없다.\n\nhttps://www.youtube.com/watch?v=FMUpVA0Vvjw\nhttps://stackoverflow.com/questions/70664562/criteria-for-default-garbage-collector-hotspot-jvm-11-17\n\n## 번외\nJava 17 vs Java 11의 GC 성능 차이: https://medium.com/javarevisited/java-17-vs-java-11-exploring-the-latest-features-and-improvements-6d13290e4e1a, https://www.optaplanner.org/blog/2021/09/15/HowMuchFasterIsJava17.html#:~:text=Java%2017%20is%208.66%25%20faster,than%20the%20G1%20Garbage%20Collector\n\n> ### 참고\n> - https://youtu.be/FMUpVA0Vvjw?feature=shared <br>\n> - https://youtu.be/vZRmCbl871I?feature=shared <br>\n> - https://mangkyu.tistory.com/118 <br>\n> - https://docs.oracle.com/en/java/javase/17/gctuning/available-collectors.html#GUID-45794DA6-AB96-4856-A96D-FDE5F7DEE498 <br>\n> - https://stackoverflow.com/questions/70664562/criteria-for-default-garbage-collector-hotspot-jvm-11-17 <br>\n> - https://medium.com/javarevisited/java-17-vs-java-11-exploring-the-latest-features-and-improvements-6d13290e4e1a <br>\n> - https://www.optaplanner.org/blog/2021/09/15/HowMuchFasterIsJava17.html <br>\n> - https://www.baeldung.com/jvm-garbage-collectors <br>"},{"excerpt":"들어가며 5주 간 JSCODE에서 진행한 운영체제 모의면접 스터디를 마치고 회고록을 작성하려고 한다. JSCODE 란? 👈 진행 방법 매 주차별로 질문 리스트가 제공된다. 스터디 참여자들은 질문 리스트에 대한 자기주도 학습을 진행한다. 매주 목요일 20시에 모여 팀별로 모의면접을 진행한다.\n답변자 1명, 질문자 2명, 관찰자 1명으로 역할 나누어 진행한다…","fields":{"slug":"/os-interview-study-retrospection/"},"frontmatter":{"date":"December 08, 2023","title":"[JSCODE] - OS 면접 스터디 회고록","tags":["retrospection"]},"rawMarkdownBody":"\n## 들어가며\n\n5주 간 JSCODE에서 진행한 운영체제 모의면접 스터디를 마치고 회고록을 작성하려고 한다.\n\n> [JSCODE 란?](https://jscode.kr/) 👈\n\n## 진행 방법\n\n1. 매 주차별로 질문 리스트가 제공된다.\n\n2. 스터디 참여자들은 질문 리스트에 대한 자기주도 학습을 진행한다.\n\n3. 매주 목요일 20시에 모여 팀별로 모의면접을 진행한다.<br>\n답변자 1명, 질문자 2명, 관찰자 1명으로 역할 나누어 진행한다.\n\n## 신청한 계기\n\n우아한테크코스가 끝나고 본격적인 취업 준비를 시작하는 과정에서 면접 준비의 필요성을 느꼈다.\n\n특히 신입으로 준비하다보니 CS 지식도 같이 준비해야 했다.\n\n준비해야 할 CS 지식에는 네트워크, DB, 운영체제가 있었는데 아무래도 혼자서 준비하기에 운영체제는 다소 힘들 것이라 생각했다.\n\n그래서 학습에 강제성을 줄 겸 면접 연습할 겸 여러모로 이득이라고 생각해서 바로 지원했다.\n\n## 좋았던 점\n\n### 1. 운영체제 지식을 전체적으로 돌아볼 수 있어 좋았다.\n\n학부생 2학년 때 처음 공부하고 한번도 공부하지 않았다. 그러다보니 가물가물했었고 사실상 운영체제 지식이 없다 해도 과언이 아니었다.\n\n그래도 이번 스터디를 통해 다시 학습을 진행했고, 특히 우아한테크코스를 통해 터득한 나만의 학습 방법과 더 넓어진 지식을 바탕으로 다시 공부하니 전엔 **보이지 않던 내용들이 보이기 시작했다.**\n\n특히 [Tomcat 구현하기 미션](https://github.com/kdkdhoho/jwp-dashboard-http)에서 쓰레드 관련 내용이 많이 나왔었는데 당시엔 잘 이해하지 못하고 넘어갔다.\n\n하지만 쓰레드 관련 내용을 학습하고 다시 보니 무슨 내용이었는지 이해하는 경험을 할 수 있었다.\n\n### 2. 말하기 연습\n\n누구에게 아는 것을 설명할 때 종종 장황하게 설명하는 버릇이 있다.\n\n그만큼 전달하고 싶은 말이 많다는 것이지만 대부분 듣는 사람 입장에선 전부 귀기울여 듣기 힘들다.\n\n그래서 핵심만 딱! 말하는 게 중요한데, 이번 면접 스터디를 통해 이를 연습할 수 있었다.\n\n특히 남에게 말로써 전달할 때 어떤 구조로 이야기할 지에 대해 조금은 터득했다.\n\n> **터득한 구조** 👇<br>\n> 질문에 대한 답을 먼저 한다.<br>\n> 그 답에 대한 과정, 방법, 이유 등이 포함되지 않아도 괜찮다.<br>\n> 위 내용들은 뒤에서 이야기해도 충분하다!\n\n## 팀에 기여한 부분\n\n### 1. 너무 식상하게 진행되지 않도록 했다.\n\n사실 같은 질문 리스트에 대해 4명이 돌아가며 질답을 하다보면, 굉장히 지루하고 식상하다.\n\n게다가 스터디를 통해 얻어가는 것도 미비할 것이라 생각했다.\n\n그래서 조금 주제넘을 수 있지만 깊은 내용에 대해 학습한 것이 있으면 이에 대해 질문했다.\n\n초반에는 팀원들이 당황하는 것이 잘 느껴졌다. 실제로 당황스럽다고도 이야기 해 주셨다. 😂\n\n그런데 시간이 갈수록 오히려 더 좋다는 말씀을 해주셨다.\n\n**실제 면접에서는 어떤 질문이 나올지 모르기 때문이다.**\n\n어쩌면 지루하고 크게 얻어가는 게 없는 면접 스터디가 될 수 있었지만 이를 막았다?고 할 수 있을 것 같다.\n\n### 2. 학습한 내용 지식 공유\n\n이제는 개념에 대해 학습할 때, 탄생하게 된 배경부터, 어떤 장단점이 있고, 어떤 대안이 있는 지에 대한 내용도 반드시 학습하고 넘어가게 되었다.\n\n스터디를 준비하면서도 마찬가지였다.\n\n그러다보니 다른 팀원들에 비해 다소 깊은 내용들까지 학습을 했고, 이를 스터디 시간에 질문까지 하다보면 자연스레 학습한 내용을 공유까지 하게 되었다.\n\n공유..! 드디어 **주도적으로 타인에게 나의 지식을 공유하여 도움이 되는 경험을 했다고 할 수 있다.**\n\n## 팀원들에게 받은 긍정적 피드백\n\n### 1. 답변이 차분하고 전달이 잘된다\n\n나는 면접볼 때 최대한 유연한 태도를 가지려고 한다.\n\n너무 딱딱하게 마치 방어하듯이 답변하는 것은 나도 힘들고 듣는 사람도 힘들다고 생각하기 때문이다.\n\n그래서 항상 답변할 땐 대화하듯이, 이야기하듯이하는데 이 점이 듣는 사람으로 하여금 듣기 편안했고<br>\n전달이 잘 되었다는 피드백을 받았다.\n\n## 팀원들에게 받은 부정적 피드백\n\n### 1. 설명이 장황하거나 너무 짧다\n\n1주차에 너무 많은 것을 학습하고 이를 설명하려다보니, 스스로 느끼기에도 설명이 너무 장황했다.\n\n어쩔 땐 너무 장황해지는 걸 막기 위해 굉장히 짧게 대답을 했다.\n\n역시나 이에 대해 많은 피드백이 날라왔다..\n\n사실 예전부터 인지하던 문제인데 오랜만에 인터뷰 자리에서 대답하려니 안좋은 습관이 다시 나왔던 것 같다.\n\n그래서 2주차를 준비할 때는 미리 어떤 식으로 답변할 지 미리 시뮬레이션하며 준비했고,\n\n이를 바탕으로 **두괄식으로 답변하기를 계속해서 연습했다.**\n\n연습한 덕인지 2주차때는 **\"1주차랑 다른 사람같다\"는 말을 들을 정도로 답변이 많이 깔끔해졌다는 피드백을 받았다.** 👍\n\n### 2. 시선처리\n\n한번은 시선처리가 좋았다는 피드백이 있었지만 한번은 시선처리가 좋지 않다는 피드백을 받았다.\n\n지금 생각해보면 자신있게 답변할 수 있는 내용에 대해 답변할 때는 시선처리가 좋았던 것 같고,\n\n자신이 없거나 조금 생각이 필요한 답변의 경우에는 시선처리가 불안하다고 생각한다.\n\n아무래도 준비를 얼마나 했냐에 따라 달라질 것 같다.\n\n## 마치며\n\n기대 이상으로 좋은 시간이었다.\n\n운영체제 스터디 뿐만 아니라 인성 면접이나, 멘토님들에게 개인적인 질문도 할 수 있었다.\n\n면접이나 이력서, 그리고 프로젝트 기반 면접 팁 같은 것들도 얻어갈 수 있었다.\n\n취준생인 나에게 투자 대비 효율이 좋았던 스터디라 생각한다.\n\n5주 간 함께 해주신 팀원들, 멘토님들 감사합니당 🙇‍♂️"},{"excerpt":"Garbage Collection를 알아야 하는 이유 Garbage Collection(이하 GC)을 공부하다보니, GC를 너무 믿어서는 안되겠다고 생각됐다. 우형에서 발생한 일도 그렇고 토스에서 발생한 일을 보니 모두 애플리케이션의 메모리 관련 치명적 에러는 모두 이 GC와 관련이 있었기 때문이다. 더군다나 네이버 D2 글에서도 GC에 대해 잘 알고 있…","fields":{"slug":"/about-garbage-collection/"},"frontmatter":{"date":"December 08, 2023","title":"JVM의 Garbage Collection과 동작 원리","tags":["java","garbage-collection"]},"rawMarkdownBody":"\n## Garbage Collection를 알아야 하는 이유\n\nGarbage Collection(이하 GC)을 공부하다보니, **GC를 너무 믿어서는 안되겠다**고 생각됐다.\n\n[우형에서 발생한 일](https://techblog.woowahan.com/2628/)도 그렇고 [토스에서 발생한 일](https://www.youtube.com/watch?v=w4fWgLgop5U)을 보니\n\n모두 애플리케이션의 메모리 관련 치명적 에러는 모두 이 *GC*와 관련이 있었기 때문이다.\n\n더군다나 [네이버 D2 글](https://d2.naver.com/helloworld/1329)에서도 **GC에 대해 잘 알고 있을수록 실력이 좋은 Java 개발자**라고 하셨다.\n\n실력이 좋은 Java 개발자가 되기 위해 이 GC에 대해 알아보자.\n\n## GC란?\n\nGC는 **JVM 기반 애플리케이션에서 동적으로 할당되는 메모리를 알아서 관리해주는 기술**이다.\n\n> JVM의 메모리 구조는 [static을 더 잘 사용하기](https://kdkdhoho.github.io/static-with-memory-structure/) 글 참고\n\n프로세스가 실행하다보면 필연적으로 새로운 메모리를 malloc 및 free 하게 된다.\n\nC, C++의 경우 Native하게 Memory를 직접 관리할 수 있다.\n\n하지만 우리의 JAVA는 탄생 배경부터 개발자로 하여금 OS에 직접 의존하지 않아도 되도록, 중간 계층인 JVM을 통해 시스템 자원에 접근하게 된다.\n\n따라서 malloc 되었지만 더이상 사용하지 않아 필요없어진 쓰레기 메모리들을 대신 처리해주는 역할이 필요하다.\n\n그 작업을 GC가 해주는 것이다.\n\n## GC의 장단점\n\n### 장점\n\n1. 개발자가 개발 중간중간 메모리를 관리할 필요가 없어진다.\n2. (완벽하진 않지만) 자체적으로 메모리 누수를 관리해준다.\n\n### 단점\n\n1. GC가 동작할 때 ❗️ **STOP THE WORLD** ❗️가 발생한다.\n> Stop the world: GC 쓰레드가 동작하는 순간에, 다른 쓰레드들은 동작하지 않는 현상이다.<br>\n> GC를 튜닝한다는 말은 주로 이 Stop the world 시간이 짧아지도록 한다는 말이다.\n\n## GC의 기본 동작 원리\n\n이 GC는 다양한 알고리즘이 존재한다.\n\n하지만 뒤에 나올 ZGC를 제외한 모든 알고리즘은 **Generational Collection**이라는 기술을 사용한다.\n\nGenerational Collection이란, ~~직역하면 세대로 수집한다는 의미~~ 대부분의 애플리케이션에서 경험적으로 관찰된 몇 가지 속성을 활용하여, 사용하지 않는 객체를 회수하는 데 필요한 작업을 최소화하는 기술이다.\n\n관찰된 몇 가지 속성 중 가장 핵심적인 것은 **Weak Generational Hypothesis**이다.\n\nWeak Generational Hypothesis이란 대부분의 객체는 잠깐 동안만 필요하다는 가설이다.\n\n아래 그림은 객체의 수명에 대한 분포도인데, 간단하게 X축은 객체의 수명, Y축은 분포도이다.\n\n즉, **애플리케이션에서 수명이 짧은 객체들이 훨씬 많이 분포한다**는 의미이다.\n\n![객체의 수명에 대한 분포도](distribution_for_lifetimes_of_objects.png)\n\n따라서 이러한 경험적 관찰 결과를 통해, Heap 영역에 있는 모든 객체에 대해 검사하지 않고 객체의 수명에 기반한 Collecting을 통해 보다 효율적인 방법을 모색한 것이다.\n\n> ~~객체 세상은 요절의 비율이 높다~~\n\n그렇다면 이 나이는 어떻게 측정해서 수집한다는 건지 보다 자세히 알아보자.\n\n일단 기본적으로 메모리 구조는 다음과 같다.\n\n![Serial GC의 기본적인 메모리 공간](default_arrangement_of_generations_int_the_serial_collector.png)\n\n자세히 살펴보면 Young, Old로 크게 한번 나뉜다.<br>\n쉽게 유추할 수 있듯이 상대적으로 젊은 객체는 Young, 늙은 객체는 Old에 저장된다.\n\n이제 Young을 자세히 들여다보자.<br>\n1개의 `Eden`과 2개의 `Survivor` 영역으로 나뉜다.\n\n> 각 영역 별 특징으로는 다음과 같다.\n> - Eden 영역에는 처음 생성되는 대부분의 객체가 저장된다.\n> - Survivor 영역은 적어도 하나는 항상 비어있다.\n\n이 구조를 기반으로 나이를 증가하는 과정은 다음과 같다.\n\n1. 처음 생성된 객체는 Eden 영역에 쌓인다.\n2. 쌓이다보면 모두 차게 되는데, 이때 Eden에 존재하는 객체 중 유효한 객체만이 Survivor 영역으로 이동한다.\n3. 이때!! 이동하는 객체에 나이를 증가시킨다.\n4. 마찬가지로 Survivor 영역이 모두 차게 되면, 유효한 객체만이 반대편의 Survivor 영역으로 이동한다. 바로 이때!! 나이가 증가한다.\n\n자, 그럼 위 과정을 계속해서 반복해서 GC의 동작 원리를 상세히 알아보자.\n\n1. Eden 혹은 Survivor 영역이 모두 차면 Young Generation 영역만 수집하는 **Minor Collection**이 발생한다.<br>\n   (적은 양의 객체만 참조하기에 Minor Collection은 비교적 빠르게 수행된다.)\n2. Minor Collection이 진행되면서 특정 나이가 된 객체는 Old 영역으로 이동한다.\n3. 진행되다 보면 Old Generation이 모두 차게 된다. 이때는 전체 영역을 Collecting하는 **Major Collection**이 발생한다.<br>\n   (전체 객체를 확인해야 하기에 Minor에 비해 상당한 시간이 걸린다.)\n\n## 이어서\n\n다음으로는 GC 알고리즘의 종류와 각 특징에 대해 알아보자.\n\n> ### 참고\n> - https://youtu.be/FMUpVA0Vvjw?feature=shared <br>\n> - https://youtu.be/vZRmCbl871I?feature=shared <br>\n> - https://mangkyu.tistory.com/118 <br>\n> - https://docs.oracle.com/en/java/javase/17/gctuning/available-collectors.html#GUID-45794DA6-AB96-4856-A96D-FDE5F7DEE498 <br>\n> - https://stackoverflow.com/questions/70664562/criteria-for-default-garbage-collector-hotspot-jvm-11-17 <br>\n> - https://medium.com/javarevisited/java-17-vs-java-11-exploring-the-latest-features-and-improvements-6d13290e4e1a <br>\n> - https://www.optaplanner.org/blog/2021/09/15/HowMuchFasterIsJava17.html <br>\n> - https://www.baeldung.com/jvm-garbage-collectors <br>"},{"excerpt":"JVM의 메모리 구조 우선 static에 대해 이해하기 전에 JVM이 실행될 때 메모리에 어떻게 적재되는 지부터 살펴보자.  크게 세 공간으로 나뉜다. Heap 영역 new 키워드를 사용해 동적으로 생성한 객체의 인스턴스가 담기는 공간이다. Garbage Collection에 의해 관리된다. Stack 영역 메서드 콜 스택이나, 지역 변수 혹은 변수 파라…","fields":{"slug":"/static-with-memory-structure/"},"frontmatter":{"date":"December 06, 2023","title":"static을 더 잘 사용하기","tags":["java","operating-system"]},"rawMarkdownBody":"\n## JVM의 메모리 구조\n\n우선 static에 대해 이해하기 전에 JVM이 실행될 때 메모리에 어떻게 적재되는 지부터 살펴보자.\n\n![메모리 구조](memory.jpeg)\n\n크게 세 공간으로 나뉜다.\n\n1. Heap 영역\n   - new 키워드를 사용해 **동적으로 생성한 객체의 인스턴스**가 담기는 공간이다.\n   - **Garbage Collection**에 의해 관리된다.\n\n\n\n2. Stack 영역\n   - 메서드 콜 스택이나, 지역 변수 혹은 변수 파라미터가 저장되는 공간이다.\n   - 주로 메서드와 관련이 있으며 메서드가 호출될 때 새로운 영역이 할당되고 반환될 때 영역도 반환된다.\n   - `StackOverflow`가 발생하는 이유도 메모리 관점에서 생각해보면 알 수 있다.\n\n3. Static 영역\n   - 운영체제에서의 Code, Data 영역이다.\n     - 동일한 프로세스 내에서 언제 어디서든 접근할 수 있다.\n   - 우리가 작성한 Class나 정적 변수(클래스 변수)가 이 영역에 저장된다.\n   - 이 곳에 담긴 데이터들은 프로그램이 종료되기 전까지 유지된다.\n\n## Java의 static 키워드\n\n이제 Java의 static 키워드에 대해 알아보자\n\nJava에서 static 키워드를 변수, 메서드, 클래스에 추가할 수 있다.\n\nstatic을 붙여 변수를 생성하거나 메서드나 클래스를 선언하게 되면, **프로그램 실행 시**에 **딱 한 번 생성**되어 위 메모리 구조의 **`Static` 영역에 포함**된다.\n\n따라서, 우리가 객체를 생성하지 않고도 static 변수를 참조하거나 메서드를 호출할 수 있는 것이다.\n\n## static을 많이 사용하게 된다면?\n\n이러한 static 변수, 메서드, 클래스를 많이 사용하게 되면 어떤 일이 발생할까?\n\n결론부터 말하면 **프로그램에 부작용을 초래할 확률이 증가**한다.\n\n메모리 관점에서 생각해보자.\n\n운영체제는 메모리에 Java 프로그램의 공간을 내어줄 때 무한정으로 제공하지 않는다.<br>\n따라서 제한된 영역 내에서 프로그램을 실행해야 한다.\n\n그런데 Static 영역이 Heap과 Stack 영역에 비해 더 많은 공간을 차지하게 된다면, `StackOverflow`나 `OutOfMemoryError` 가 발생할 확률이 증가한다.\n\n아무리 GC가 Heap 영역을 관리해 준다 한들, 사용할 수 있는 공간 자체가 적다면 GC의 역할에도 한계가 있을 것이다.\n\n추가로 사용되지 않음에도 static으로 선언되었다는 이유로, 불필요하게 메모리를 차지하게 될 경우도 분명 존재할 것이다.\n\n## 어느 상황에 static을 사용하면 좋을까?\n\n그렇다면 어느 상황에서 static을 적절히 사용하면 좋을까?\n\n단순히 생각해서 객체를 매번 만들 필요가 없는 경우에만 사용하면 좋을 것이다.\n\n즉, 프로그램을 실행할 때 딱 한 번만 메모리에 올려놓고 계속해서 사용하는 경우이다.\n\n다시 말해, Java 프로그램이 실행되면 딱 한 번만 메모리에 올려놓고 사용하는 경우가 될 것이다.\n\n### 상수\n\n상수는 변하지 않는 값을 의미한다.\n\n이러한 상수는 런타임으로 새로운 변수가 계속해서 만들어질 필요가 없다.\n\n가령, 회원가입 시에 1,000 포인트를 기본적으로 제공한다는 예시가 있다고 가정해보자.\n\n```java\npublic class Member {\n   private static final int WELCOME_POINT = 1_000; // 상수\n   \n   public Member(...) {\n      // ...\n      this.point = WELCOME_POINT;\n      // ...\n   }\n}\n```\n\n위 코드의 `WELCOME_POINT` 처럼, Member 객체를 만들 때마다 새로 만들어 줄 필요도 없는 값의 경우 사용하면 효율적일 것이다.\n\n#### 번외: 상수의 접근 제어자의 범위를 어떤 기준으로 지정하면 좋을까?\n\n위 예시에서는 `private`로 선언을 했는데, `public`으로 한다면 어떨까?\n\n메모리 영역에서 생각해 봤을 때 `Static` 영역으로 올라가고, 이 영역은 같은 프로세스 내의 모든 곳에서 접근이 가능하다.\n\n그렇지만 위 상수가 다른 클래스에서 범용적으로 쓰이지 않는 한, 불필요하게 public으로 열어둘 필요는 없어보인다.\n\n필요하다면 private으로 선언 후, getter를 통해 값을 반환하는 게 코드를 작성하고 이해하는 데에 훨씬 도움이 될 것이라고 생각한다.\n\n최대한 작은 범위로 설정하자.\n\n### util성 메서드 (static 메서드)\n\n흔히 **유틸성 클래스**라고 하는 곳에 존재하는 메서드들에 `static` 키워드를 붙여 사용한다.\n\n이는 인스턴스를 만들지 않고도 사용할 수 있고, 어디에서든 호출할 수 있다.\n\n다만, 인스턴스로 쓰이는 클래스에 static 메서드가 존재할 수 있는데, 이때 조심해야 할 점이 있다.\n\n바로 static 메서드는 재정의가 불가능 하다는 것이다.\n\n![클래스 상속 시](cannot-extends.png)\n\n![인터페이스 구현 시](cannot-implements.png)\n\n추가로 광범위하게 사용되는 특징을 가진 Util성 메서드인 만큼, 클래스의 변경 가능성이 증가한다는 특징이 있다.\n\n가급적 Util성 클래스에 static 메서드를 모아서 사용하도록 하자.\n\n> ## Reference\n> - [망나니 개발자 블로그](https://mangkyu.tistory.com/47)\n> - [Java의 정석](https://www.yes24.com/Product/Goods/24259565)\n> - [AT&T Israel Tech Blog](https://medium.com/att-israel/should-you-avoid-using-static-ae4b58ca1de5)"},{"excerpt":"가상 주소와 물리 주소(실주소)에 대해 설명해주세요. 가상 주소를 물리 주소(실주소)로 어떻게 변환할까요? 절대 주소 지정과 상대 주소 지정의 차이점은 뭘까요? 메모리 분할에 대해 설명해주세요. 메모리 배치 기법(메모리 관리 전략)에 대해 설명해주세요. 외부 단편화와 내부 단편화의 차이가 뭔가요? 메모리 배치 기법중 하나인 colaescing(통합)에 대…","fields":{"slug":"/os-interview-study-5week/"},"frontmatter":{"date":"December 06, 2023","title":"[JSCODE] - OS 면접 스터디 5주차","tags":["operating-system","interview"]},"rawMarkdownBody":"\n## 가상 주소와 물리 주소(실주소)에 대해 설명해주세요.\n```text\n가상 주소의 경우 프로세스마다 독립적으로 가지는 주소 공간입니다. 모두 0번지부터 시작합니다.\n물리 주소의 경우 프로세스가 실제 메모리에 올라가는 위치입니다.\n```\n\n## 가상 주소를 물리 주소(실주소)로 어떻게 변환할까요?\n```text\nMMU라는 하드웨어에 의해 변환됩니다.\nCPU가 가진 논리 주소를 MMU로 전달하게 되고, 메모리 매핑 전략에 따라 달라지는 방법을 통해 물리 주소로 변환하게 됩니다.\n\n추가)\n이 MMU는 Relocation Register와 Limit Register를 이용해 계산합니다.\nRelocation Register는 현재 사용 가능한 물리 주소의 최하위 주소 값을 가집니다.\nLimit Register는 논리 주소의 범위를 가집니다. 이 값을 통해 해당 프로세스의 물리적 주소를 벗어나지 않도록 막을 수 있습니다.\n\n추가)\nOS가 이를 계산하여 변환하지 않는 이유는, HW를 이용하는 것보다 훨씬 느리기 때문입니다.\nHW는 값만 넣어주면 빠르게 계산되어 결과를 얻을 수 있는 반면,\nOS에게 위임하게 된다면 컨텍스트 스위칭으로 인한 추가적인 오버헤드가 발생하기 때문입니다. \n```\n\n## 절대 주소 지정과 상대 주소 지정의 차이점은 뭘까요?\n```text\n절대 주소 지정은 컴파일러에 의해 만들어진 논리 주소가, 실제 메모리에 적재될 때 그대로 사용되어 적재되는 것을 의미합니다.\n상대 주소 지정은 이 논리 주소와는 상관없이 메모리에 적재될 때, 그리고 적재된 이후에 동적으로 물리 주소를 할당하는 것을 의미합니다.\n```\n\n## 메모리 분할에 대해 설명해주세요.\n```text\n프로세스를 동일한 크기의 메모리 블록으로 나누는 것을 의미합니다.\n이 페이지 분할을 통해 프로세스에서 사용되는 메모리만을 메인 메모리에 적재하여 사용할 수 있게 되어\n메모리를 효율적으로 사용할 수 있게 됩니다.\n```\n\n## 메모리 배치 기법(메모리 관리 전략)에 대해 설명해주세요.\n```text\n프로세스를 메인 메모리에 적재할 때 어떤 방식으로 적재할 것인가에 대한 방법입니다.\n\n방법으로 크게 나누면 연속 메모리 할당과 불연속 메모리 할당으로 나눌 수 있습니다.\n```\n\n```text\n[연속 메모리 할당]\n단일 프로그래밍 환경 -> 프로세스 1개를 통채로 메인 메모리에 적재\n다중 프로그래밍 환경 -> 고정 분할 방법: 메인 메모리를 여러 개의 고정된 크기로 나누고 해당 크기에 프로세스를 적재하는 방식. 내부 단편화 발생\n                -> 가변 분할 방법: 고정된 크기를 없애고 프로세스가 필요한 만큼만 메모리에 적재. 외부 단편화 발생. 최초 적합, 최적 적합, 최악 적합\n\n[불연속 메모리 할당]\n고정 분할: 페이징 ->\n가변 분할: 세그멘테이션 -> \n```\n\n## 외부 단편화와 내부 단편화의 차이가 뭔가요?\n```text\n외부 단편화는 프로세스가 적재될 수 있는 공간이지만, 그 크기가 작아 들어가지 못하는 공간입니다.\n내부 단편화는 메인 메모리 상에 일정 크기의 공간 안에 그 공간보다 더 작은 프로세스가 할당되어 생기는 공간을 의미합니다.\n\n예를 들어 설명해보면, 외부 단편화는 음식을 담을 그릇이 있지만 이 그릇에 비해 음식의 크기가 더 커서 담지 못하는 현상을 의미합니다.\n내부 단편화는 음식을 그릇에 담았지만, 음식의 크기가 그릇에 비해 매우 작아서 빈공간이 생기는 개념입니다.\n```\n\n## 메모리 배치 기법중 하나인 colaescing(통합)에 대해 설명해주세요.\n```text\n메인 메모리에 할당된 프로세스가 종료되면 해당 프로세스가 차지한 공간과 인접한 빈 공간이 있는지 확인하고\n이 공간들을 하나로 통합하는 과정입니다.\n이를 통해 외부 단편화를 최소화할 수 있습니다.\n```\n\n## 메모리 배치 기법중 하나인 compaction(압축)에 대해 설명해주세요.\n```text\n메인 메모리에 적재된 프로세스들을 최대한 한 쪽으로 모두 모으는 기법입니다.\n이를 통해 외부 단편화를 최소화할 수 있습니다.\n\n하지만 압축하는 시간동안 시스템은 모든 일을 중지해야 한다는 심각한 단점이 있습니다.\n```\n\n## 메모리 배치 기법중 하나인 버디 시스템에 대해 설명해주세요.\n```text\n메인 메모리를 항상 2^N 크기로 할당합니다. \n그 후 프로세스를 적재할 때, 가능한 공간을 절반씩 쪼개면서 들어갈 수 있는 최적의 공간을 찾아 넣습니다.\n프로세스를 적재할 때는 최대한 메모리의 낮은 위치에 적재하는 식으로 진행됩니다.\n\n외부 단편화를 최소화할 수 있습니다.\n\n만약 프로세스가 할당 해제되고 자신의 Buddy가 존재하고 비어있다면 Merge합니다.\n```\n\n## 가상 메모리에 대해 설명해주세요.\n```text\n메인 메모리에 프로세스의 모든 메모리를 올리지 않고 필요한 메모리만 올릴 수 있도록,\n필요하지 않은 부분의 경우 디스크에 일부 공간에 저장하는 개념을 의미합니다.\n\n가상 메모리를 사용하는 기법으로는 페이징과 세그멘테이션이 있습니다.\n```\n\n## 메모리 배치 기법중 하나인 페이징에 대해 설명해주세요.\n```text\n프로세스의 메모리 영역을 크기가 동일한 페이지로 나눕니다. 동시에 메인 메모리도 동일한 크기의 프레임으로 나눠 놓습니다.\n그 후 프로세스의 일부 필요한 페이지를 프레임에 적재하고 이를 페이지 테이블에 기록합니다.\n물리 주소는 이 페이지 테이블을 통해 찾을 수 있습니다.\n\n빈 프레임이 존재만 한다면 메모리 공간 어디에든 적재할 수 있어 공간을 효율적으로 사용할 수 있으며,\n외부 단편화는 발생하지 않지만 내부 단편화가 발생할 수 있다는 특징이 있습니다.\n```\n\n## 메모리 배치 기법중 하나인 세그멘테이션에 대해 설명해주세요.\n```text\n프로그램을 구성하는 논리적인 단위로 메모리를 나누게 됩니다. 이를 세그먼트라고 합니다.\n이 세그먼트가 메인 메모리에 적재되면 세그먼트 테이블에 이를 기록하고, 이 정보를 통해 물리 주소를 조회할 수 있습니다.\n보통 최초 적합이나 최소 적합 알고리즘을 사용합니다.\n```\n\n## 페이징과 세그멘테이션의 비교 (개인적으로 추가)\n```text\n페이징은 용량이 작은 메인 메모리 크기가 동시에 많이 실행될 수 있다. \n세그멘테이션은 페이징 기법의 장점에 추가로 프로그램을 논리적으로 독립된 주소 공간으로 나누고 쉽게 공유 및 보호할 수 있다.\n```\n\n## Swapping이란 무엇인가요?\n```text\n메인 메모리에 새로운 프로세스를 적재할 공간이 없을 경우, 메인 메모리에 있는 특정 프로세스를 디스크의 가상 메모리에 잠시 저장해뒀다가\n공간이 생기면 다시 가져와 실행하는 개념을 의미합니다.\n```\n\n## Swapping의 과정을 설명해 주세요.\n```text\n우선 CPU가 요청하는 논리 주소를 MMU를 통해 물리 주소로 변환하게 됩니다.\n이때 물리 주소에 원하는 메모리 블록이 없다면, 디스크에서 해당 메모리 블록을 메인 메모리에 적재해야 합니다.\n만약 메인 메모리에 공간이 없다면, 메인 메모리에 있는 특정 메모리 블록을 디스크로 이동을 하고 요구되는 메모리 블록을 메인 메모리에 적재합니다.\n만약 메인 메모리에 공간이 있다면, 바로 디스크에서 메모리 블록을 가져와 적재합니다.\n그 후 페이지 테이블을 업데이트 하는 식으로 스와핑이 진행됩니다.\n```\n\n## Swapping의 장단점을 설명해 주세요.\n```text\n장점으로는, 가상 메모리를 활용하여 필요한 메모리 블록만 메인 메모리에 올린다는 점에서 공간을 효율적으로 사용한다는 장점이 있습니다.\n하지만 단점으로, 스와핑하는 과정은 디스크 I/O 작업이 포함되기 때문에 오버헤드가 발생한다는 단점이 있습니다.\n```\n\n## 페이지 교체에 대해서 설명해주세요.\n```text\n디스크에서 페이지를 메인 메모리에 적재하려고 할 때, 빈 공간이 없어서 기존에 적재되어 있는 페이지를 디스크로 이동시키고\n디스크에 있는 페이지를 메인 메모리에 적재하는 과정을 의미합니다.\n\n대표적인 페이지 교체 알고리즘으로는, FIFO, LRU, LFU, Clock 알고리즘이 있습니다.\n```\n\n## 페이지 부재를 최소화하려면 어떻게 해야 하나요?\n```text\n상식적으로 생각을 해봤을 때, CPU가 원하는 페이지가 최대한 메인 메모리에 적재가 되어 있는 상태여야 합니다.\n이를 위해서는 적절한 페이지 교체 알고리즘을 선택하거나, 페이지의 크기도 적절히 조절을 하는 것이 도움이 될 수 있습니다.\n```\n\n## 페이지 교체 알고리즘 FIFO에 대해 설명 해주세요.\n```text\n메인 메모리에 올라간 페이지의 순서대로 스와핑 아웃을 하는 방식입니다.\n이는 아무래도 구현은 쉽지만 페이지 교체를 최소화하는 방법이 아닐 확률이 큽니다.\n```\n\n## 페이지 교체 알고리즘 LRU에 대해 설명 해주세요.\n```text\nLeast Recently Used의 약자로, 가장 오랫동안 사용되지 않은 페이지를 스와핑 아웃시키는 방식입니다.\n\n페이지의 실제 패턴을 고려했기에 더 적은 페이지 교체가 일어날 수 있습니다.\n하지만 알고리즘을 위한 추가 연산이 들어가기에 오버헤드가 발생합니다.\n```\n\n## 페이지 교체 알고리즘 LFU에 대해 설명 해주세요.\n```text\nLeast Frequently Used의 약자로, 가장 적게 사용된 페이지를 스와핑 아웃하는 방식입니다.\n\n실제 페이지 사용 패턴을 고려했기에 페이지 교체가 적게 일어날 수 있습니다.\n하지만 알고리즘을 위한 추가적인 연산이 들어가기에 오버헤드가 발생합니다.\n```\n\n## 페이지 교체 알고리즘 클럭 알고리즘에 대해 설명해주세요.\n```text\n페이지들을 시계처럼 원형 큐에 저장합니다.\n이 큐에 프레임을 저장할 때, 참조 여부를 나타내는 비트도 함께 저장합니다.\n만약 페이지가 참조되면 이 비트를 1로 바꾸게 됩니다.\n\n그리고 시계침처럼 이 원형큐를 계속해서 순회하며 바라보는 인덱스가 있는데요.\n만약 한 바퀴를 도는 동안 참조되지 않았다면 비트를 1에서 0으로 수정하고, 그 후에도 한 바퀴 도는 동안 참조되지 않으면\n해당 페이지를 교체하는 방식으로 이루어집니다.\n\n이 알고리즘은 구현이 간단하면서 동시에 페이지 교체가 적게 일어나는 방식으로 알고 있습니다.\n```\n\n## 쓰레싱에 대해 설명해주세요.\n```text\n실제 프로세스 동작보다 페이지 교체에 더 많은 시간을 쏟게 되는 현상을 의미합니다.\n\n이를 해결하기 위한 알고리즘으로 워킹 알고리즘과 페이지 부재 빈도 알고리즘이 있습니다.\n```\n\n## 워킹 알고리즘에 대해 설명해주세요.\n```text\n현재 프로세스가 실행 중인 상황에서 가장 중점적으로 사용되는 메모리 영역을 워킹 셋이라고 합니다.\n이 워킹 셋을 메모리에 동시에 적재할 수 있도록 보장하여 페이징 부재 빈도를 낮추는 방식의 알고리즘 입니다. \n```\n\n## 페이지 부재 빈도 알고리즘에 대해 설명해주세요.\n```text\n프로세스의 페이지 부재율을 주기적으로 조사하고 이 값을 근거로 해당 프로세스에 할당할 메모리 양을 동적으로 조절하는 알고리즘입니다.\n\n해당 프로세스의 페이지 부재율이 높으면, 그만큼 이 프로세스에 할당된 페이지의 양이 적다는 의미이므로 할당할 수 있는 페이지의 양을 증가합니다.\n이때 페이지 부재율이 높은 프로세스의 페이지 할당률을 낮춰 그 수를 조절한다.\n```\n\n------------------------\n\n## 이화여대 강의 정리\n\n```text\n프로세스가 메모리에 적재될 때, 메모리의 어느 위치에 적재할 지에 대한 방법(주소 바인딩)이 3가지 있다.\n\n1. Compile Time Binding\n프로세스의 가상 주소를 실제 주소로 그대로 사용한다.\n이는 프로세스 2개 이상을 동시에 메모리에 적재하기에 위험성이 있다.\n컴파일러는 절대 코드(Absolute Code)를 생성한 경우 사용되는 방법이다.\n\n2. Load Time Binding\n프로세스가 메모리에 적재될 때(Load time) 무작위로 적재가 되고, 변경되지 않는다.\n컴파일러는 재배치가능코드(Relocatable Code)를 생성한 경우 가능한 방법이다.\n\n3. Run Time Binding\n프로세스가 메모리에 적재될 때 무작위로 적재되지만, 동적으로 변경이 가능하다.\n현 시대에 흔히 사용되는 방법이다.\n```\n\n```text\n프로그래밍 언어로 변수나 메서드를 선언한다.\n그리고 컴파일하면 실행 파일(프로그램)이 생성된다.\n이 프로그램은 자체적으로 0번지부터 시작하는 메모리 주소가 있다.\n컴파일러가 이 실행 파일을 만들 때 알아서 메모리를 할당한다.\n\n프로그램이 실행되어 프로세스가 되어도 자체적인 주소 공간을 사용한다.\n이를 가상 주소라고 한다.\n실제 주소는 실제 메모리의 주소 공간을 의미한다.\n\nCPU는 PCB에 있는 Program Counter가 가리키는 주소를 바라본다.\n이때 PC가 가리키는 주소는 가상 주소이다.\n(왜? 물리 주소는 항상 바뀔 수 있고, 잘못하면 다른 프로세스의 명령어를 실행한다. 논리주소는 바뀌지 않고 매번 동일한 곳을 가리키기 때문)\n\n따라서 이 가상 주소를 통해 실제 주소를 가리키려면 변환 과정이 필요하다.\n\n이는 MMU(Memory Management Unit)라는 하드웨어에 의해 실행된다.\n(왜 운영체제가 아닌 하드웨어? 빠른 속도로 메모리에 접근해야한다. 그런데 운영체제에 의해 관리되면 오버헤드가 발생한다.)\n\n(연속 할당에서의) MMU는 논리 주소를 relocation register와 limit register를 통해 물리 주소로 변환한다.\nrelocation register는 실행하려는 프로세스의 물리적 주소의 시작 위치를 가리킨다.\nlimit register는 실행하려는 프로세스가 가지는 주소의 전체 범위를 가진다.\n따라서 논리주소 + relocation register의 값 = 물리 주소가 된다.\n이떄, limit register는 해당 프로세스가 가지는 물리적 주소의 범위를 벗어나서 명령을 실행하는 경우를 막기 위해 존재한다. \n```\n\n```text\n[Dynamic Loading]\n프로세스 전체의 메모리 공간을 실제 메모리 영역에 적재하지 않고, 해당 메모리 공간이 필요로 할 때 메모리에 적재하는 것을 의미한다.\n\n[Swapping]\n메모리에 적재되어 있는 프로세스의 일부 페이지를 디스크로 쫓아내는 것을 의미한다.\n```\n\n```text\n[Allocation of Physical Memory]\n1. 연속 할당\n하나의 프로세스가 메모리에 적재될 때, 통채로 올라가는 것을 의미한다.\n\n2. 불연속 할당\n하나의 프로세스에 대해서도 페이징 단위로 나누어 메모리에 적재하는 것을 의미한다.\n더군다나 같은 프로세스의 페이지라고 해도 메인메모리 상에서 연속적이지 않다.\n\n2-1. 페이징 기법\n프로세스의 논리 주소를 동일한 사이즈의 페이지로 나눈다.\n일부 페이지는 스왑 메모리에, 일부 페이지는 메인 메모리에 저장한다.\n주소 변환은 '페이지 테이블'을 통해 이뤄진다.\n\n페이지 테이블을 통한 주소 변환은 다음의 과정으로 이뤄진다.\n1. CPU가 가진 논리 주소로 페이지 테이블에서 물리 주소로 변환한다.\n2. CPU가 같이 가지고 있는 offset 값으로 물리 주소로부터 offset 만큼의 주소를 추가한다.\n\n그런데 이 페이지 테이블은 4byte로 나누고, 실행되는 프로세스가 32비트인 경우, 각 프로세스마다 100만 행의 페이지 테이블을 가진다.\n이때 중요한 점은 컨텍스트 스위칭이 발생할 때 이 페이지 테이블도 함께 초기화된다.\n따라서 효율측면에서 좋지 않다. 따라서 캐싱 기법을 통해 성능을 개선한다.\n```"},{"excerpt":"왜 값의 변경을 막아야 할까?  자동차 경주 피드백 중 하나이다. 왜 이런 피드백이 있는지 알아보자 final 키워드를 붙이면 좋은 점 1. 변경 가능성을 최소화한다. 즉, 코드의 예측이 쉬워진다. 사실 알잘딱깔센으로 객체나 값의 변경은 정말 필요한 곳에서 필요한 만큼 수행해야 한다. 하지만 final 키워드를 붙이지 않으면 분명한 것은 변경 가능성이 존…","fields":{"slug":"/why-use-final/"},"frontmatter":{"date":"December 05, 2023","title":"final 키워드를 사용해 값의 변경을 막아라. 왜?","tags":["java","clean-code"]},"rawMarkdownBody":"\n## 왜 값의 변경을 막아야 할까?\n\n![피드백 사진](why-use-final.png)\n\n자동차 경주 피드백 중 하나이다.\n\n왜 이런 피드백이 있는지 알아보자\n\n## final 키워드를 붙이면 좋은 점\n\n### 1. 변경 가능성을 최소화한다.\n\n즉, 코드의 **예측이 쉬워진다**.\n\n사실 알잘딱깔센으로 객체나 값의 변경은 정말 필요한 곳에서 필요한 만큼 수행해야 한다.\n\n하지만 final 키워드를 붙이지 않으면 분명한 것은 변경 가능성이 존재하는 것이고, 모종의 이유로 사이드 이펙트 발생으로 버그를 낳을 수 있다.\n\n그럼 퇴근이 늦어질 것이다.\n\n### 2. Thread-Safe한 값이 된다.\n\n동시성 이슈이 발생하는 과정에는 반드시 값이 **변경**하는 과정이 존재한다.\n\n그런데 애초에 값을 변경하지 못하게 한다면 동시성 이슈는 일어날 일이 없다.\n\n그럼 퇴근이 빨라질 것이다.\n\n> **유의해야 할 점** ❗️ <br>\n> `StringBuilder`나 `List`, `Map`, `Set` 과 같은 컬렉션들은 값이 **변경**한다.\n> ```java\n>>public String foo() {\n>    final StringBuilder sb = new StringBuilder();\n>    \n>    sb.append(\"a\");\n>    sb.append(\"b\");\n>    \n>    return sb.toString();\n> }\n> ```\n> \n> 위 객체들은 내부적으로 쓰기 작업을 모두 허용한다.<br>\n> **final 키워드는 사실 메모리 Level에서 값의 재할당을 불가능하게 막아줄 뿐이다.**\n\n### 3. 성능의 사소한 이점\n\n이건 우테코에서 배운건데 신기해서 공유한다.\n\n아래 두 코드는 바이트 코드 레벨에서 차이가 존재한다.\n\n```java\npublic void example1() {\n    String a = \"a\";\n    String b = \"b\";\n    System.out.println(a + b);\n}\n\npublic void example2() {\n    final String a = \"a\";\n    final String b = \"b\";\n    System.out.println(a + b);\n}\n```\n\n![바이트 코드단의 결과](result_of_bytecode.png)\n\n왼쪽 빨간 영역은 `example1()`의 결과, 오른쪽은 `example2()`의 결과이다.\n\n프로그래밍 언어 관점에서는 아무 것도 아닌 것처럼 보이지만, 컴퓨터 관점에서 실행하는 코드의 양은 꽤 차이가 난다.\n\n### 4. GC 성능을 높일 수 있다.\n\nGC 성능을 높일 수 있다고 하는데, 이는 GC에 대해 알아보고 작성하자.\n\n## final 키워드를 붙이면 안좋은 점\n\n### 1. 값을 변경하려면 새로운 객체를 생성해야 한다.\n\n이는 메모리를 추가적으로 사용하게 된다.\n\n근데 사실 GC가 있으니 크게 문제되어 보이지 않는다.\n\nGC를 믿어보자.\n\n### 2. 코드가 다소 지저분해진다.\n\nfinal 키워드를 통해 불변을 지키려면 가급적 불변을 지켜야하는 곳에는 모두 붙여줘야 할 것이다.\n\n어디는 해주고 어디는 안해주면 그게 더 혼란스러울테니까.\n\n그러면 인스턴스 변수 뿐만 아니라, 메서드 파라미터나 메서드 내의 지역변수에도 붙여줘야 할 것이다.\n\n하지만 이는 코드를 작성하는 과정에서도 그렇고 읽는 데에도 은근 거슬린다.\n\n아래는 진행했던 프로젝트의 메서드 중 하나이다.\n\n![final 키워드 붙이기 전](before_final.png)\n\n![final 키워드 붙인 후](after_final.png)\n\n은근 차이가 있어보인다.\n\n> 사실 셀럽잇 프로젝트할 때 이거 때문에 안붙였는데 불변을 지키지 않아 생기는 문제는 없었다. 😶\n\n## 결론\n\n프로그래밍에서 불변을 진리처럼 여기는 것 같다.\n\n하지만 늘 그랫듯 개발에는 **정답**은 없다.\n\nfinal 키워드를 통한 불변식을 지키는 것도 마찬가지이다.\n \n조금 더 넓은 범위에서 이해해보면, 사실 프로그래밍 언어가 제공하는 기능을 통해 프로그램의 안정성을 도모하는 것이다.\n\n프로그램이 안정적이면 개발자는 조금 더 퇴근을 빨리하거나 꿀잠을 잘 수 있다.\n\n하지만 개발하는 과정에서 final 키워드로 인해 눈이 조금이라도 피곤하고 머리가 아프다면 붙이지 않는 것이다.\n\n근데 나라면 더 빠른 퇴근과 편안한 잠을 위해 final 키워드를 붙이는 데에 손을 들고 싶다."},{"excerpt":"들어가며 흔히 Domain과 View는 분리해야 한다고 한다. 대표적인 디자인 패턴으로 MVC 패턴이 있다. 같이 작성하면 개발도 빨리 할 수 있고 코드 작성에도 편한데, 왜 분리하라고 할까? 결론 결론부터 말하자면, Domain 객체를 잦은 변경으로부터 보호하기 위해서이다. 왜 Domain을 보호해야 할까? 이를 알아보기 위해서는 우선 Domain이 무…","fields":{"slug":"/why-seperate-domain-and-view/"},"frontmatter":{"date":"December 04, 2023","title":"Domain과 View를 분리한다. 왜?","tags":["design-pattern","mvc-pattern","clean-code"]},"rawMarkdownBody":"\n## 들어가며\n\n흔히 Domain과 View는 분리해야 한다고 한다.\n\n대표적인 디자인 패턴으로 MVC 패턴이 있다.\n\n같이 작성하면 개발도 빨리 할 수 있고 코드 작성에도 편한데, 왜 분리하라고 할까?\n\n## 결론\n\n결론부터 말하자면, Domain 객체를 잦은 변경으로부터 보호하기 위해서이다.\n\n## 왜 Domain을 보호해야 할까?\n\n이를 알아보기 위해서는 우선 Domain이 무엇을 의미하는 **용어**인지 이해해보자.\n\n### Domain이란?\n\n흔히 **소프트웨어로 해결하고자 하는 문제 영역**이라고 한다.\n\n우리에게 친숙한 서비스로 생각해봤을 때<br>\n카카오톡이 해결하고자 하는 문제 영역은 채팅이다.<br>\n배달의 민족이 해결하고자 하는 문제 영역은 배달 주문이다.\n\n우리가 개발하는 이유에 대해서 생각해 봤을 때에도 결국 어떠한 문제를 해결을 위해서이다.\n\n그렇기에 **Domain은 프로그램에 있어 가장 본질적인**, 그리고 **우리의 밥줄이 되는 코드**라고 할 수 있다.\n\n## View\n\n이제 View에 대해 알아보자.\n\nView는 쉽게 말해 사용자에게 보여지는 코드이다.\n\nView는 어떤 특징을 가질까?\n\n개인적인 경험으로 View는 Domain에 비해 엄청난 변경이 발생한다.\n\n페이지를 추가하거나,<br>\n메인 페이지 개편을 하거나,<br>\n처음엔 웹페이지로만 만들었다가 웹앱으로 만들었다가..\n> 심지어 웹페이지랑 웹앱은 따로 관리해야 한다고도 한다 😂\n\n더 다양한 상황의 변경이 일어난다.\n\n혹여나 웹 서비스가 성공해서 앱으로도 제공하려면, 앱 전용 View도 작성해야 할 것이다.\n\n## 다시 돌아와서\n\n이제 다시 돌아와서 \"왜 Domain과 View를 분리해야 할까?\" 라는 질문보다,\n\n반대로 \"Domain과 View가 공존한다면? 혹은 Domain이 View에 강력하게 결합되어 있다면?\" 이라는 질문을 하고 싶다.\n\n이에 대한 답으로, 변경 가능성이 매우 높은 View가 변할 때마다 Domain도 그에 따라 변화에 대응해야 한다.\n\n따라서 결론으로, **우리의 밥줄인 Domain을 변경하는 상황을 최소화하는 노력** 중 하나라고 이해하면 될 것 같다.\n\n## 이해를 돕는 좋은 예시\n\n사실 위 내용은 어느 블로그에서나 할 수 있는 이야기이다.\n\n그렇기에 개인적으로 Domain과 View의 분리가 와닿았던 사례에 대해 소개하고자 한다.\n\n아래 그림을 보자.\n\n![recommend-example.png](recommend-example.png)\n\n*RacingCar* 라는 이름의 게임이다.\n\n패키지 구조를 보면 `android`, `console`, `domain`, `ios`, `web` 으로 크게 나뉘어져있다.\n\n그리고 위 사진을 보면, `domain` 패키지 안에는 자동차 경주 게임을 진행할 때 정말 꼭 있어야 하는 필수 객체들이 존재한다.\n\n그리고 동시에 `console`과 `web` 패키지를 보자.\n\n각 View의 속성에 따라 객체들이 다르게 존재하는 것을 볼 수 있다.\n\n따라서 위 게임은 콘솔 환경에서도, 웹 환경에서도 **Domain 객체의 변경이 일어나지 않는다.**\n\n만약, 패키지 구분없이 `console`과 `web`의 실행 환경을 동시에 해야 하는 요구 사항이 있다면 패키지는 어떻게 됐을까?\n\n아마 아래와 같은 형태일 것이다.\n\n![not-seperate.png](not-seperate.png)\n\n보기 싫다. 게다가 지금은 간단한 프로그램이라 그렇지 대규모 프로젝트였다면?\n\n이제 그만 알아보자."},{"excerpt":"들어가며 MySQL에서의 계정 식별 방식, 권한, 역할에 대한 기본적인 내용을 알아보자 왜 알아야 할까? 데이터베이스 서버의 보안을 위해 ! 괜히 보안떄문에 퇴근이 늦어지면 안되니까 1. 사용자 식별 방법 사용자 아이디 뿐만 아니라 접속 지점(호스트명이나 도메인 또는 IP주소)도 함께 확인한다.\n예시)   따라서 계정의 HOST와 다른 지점에서 접속하면 …","fields":{"slug":"/03-user-identification/"},"frontmatter":{"date":"December 04, 2023","title":"MySQL에서의 사용자 식별에 대해","tags":["database"]},"rawMarkdownBody":"\n## 들어가며\n\nMySQL에서의 계정 식별 방식, 권한, 역할에 대한 기본적인 내용을 알아보자\n\n## 왜 알아야 할까?\n\n데이터베이스 서버의 **보안**을 위해 !\n\n> ~~괜히 보안떄문에 퇴근이 늦어지면 안되니까~~\n\n## 1. 사용자 식별 방법\n**사용자 아이디** 뿐만 아니라 **접속 지점(호스트명이나 도메인 또는 IP주소)**도 함께 확인한다.<br>\n예시) `root@localhost`\n\n![예시 사진](example.png)\n\n따라서 계정의 HOST와 다른 지점에서 접속하면 실패한다.\n\n> 💡 계정을 언급할 때는 **항상 아이디와 호스트를 함께** 명시하자 !\n\n### 1-1. 만약 모든 지점에서 접속을 허용하고 싶다면\n---\n계정의 HOST를 `%` 로 설정하자. (와일드카드 느낌)\n\n### 1-2. 계정의 HOST를 이용해 보안을 신경 쓴 사례\n---\n**[상황]**<br>\n주어진 자원: 1대의 MySQL 서버, 1대의 개발용 서버 1대, 1대의 운영용 서버 1대\n\n개발용, 운영용 서버에서 1대의 MySQL 서버로 요청을 보낸다.\nMySQL 서버에는 개발용, 운영용 데이터베이스를 따로 구축하여 관리하고 있었다.\n\n아래는 이해를 돕기 위한 사진이다.\n\n![인프라 아키텍처의 일부](infra_architecture.png)\n\n**[문제]**<br>\n혹여나 실수로 인해 개발용 서버에서 운영용 DB에 쿼리를 날리거나, 그 반대의 상황을 막고자 했다.\n\n**[해결]**<br>\n계정을 각 환경에 맞는 서버로 나누어 생성 및 관리를 함으로써 의도치 않은 상황을 예방했다.<br>\n(`'celuveat_prod'@{ec2_prod_ip}`, `'celuveat_dev'@{ec2_dev_ip}`)\n\n### 1-3. 계정 식별에서 주의할 점: 아이디는 같고 호스트가 다른 경우\n---\n만약 `user` 이라는 계정이 두 개가 있다.<br>\n한 계정의 HOST는 `192.168.1.01`이고 나머지는 `%`라고 가정해보자.<br>\n(`'user'@'192.168.1.01'`, `'user'@'%'`)\n\n이때 클라이언트의 IP가 `192.168.1.01`인 곳에서 `user` 계정으로 접속을 시도하면 어떻게 될까?<br>\n그리고 `192.168.1.01`이 아닌 곳에서 `user` 계정으로 접속을 시도하면 어떻게 될까?\n\n정답은, `user@192.168.1.01` 계정으로 로그인을 시도한다.<br>\nMySQL은 기본적으로 **더 좁은 범위의 HOST 계정으로 인증을 시도**하기 때문이다.\n\n> 애초에 이런 식으로 설정하는 경우는 가급적 자제하는 게 좋아보인다.<br>\n> 만약 불가피한 경우에는 `--host` 속성으로 계정을 명시해 접속 시도할 수 있을 것이다.\n\n이때 주의할 점은, 인증에 실패했다고 더 넓은 범위의 계정으로 접속을 시도하지 않는다.\n\n### 1-4. 실험1: 만약 `localhost`와 현재 IP 주소로 설정한다면?\n---\n`create user 'kdkdhoho'@localhost identified by abc;` 명령어와<br>\n`create user 'kdkdhoho'@'192.168.0.9' identified by 123;` 명령어로 상황을 세팅해보자.\n\n![실험 환경](test_environment.png)\n\n그리고 kdkdhoho 계정으로 로그인을 시도했을 때, 성공하는 계정을 보자.\n\n![로그인 결과](after_test.png)\n\nHOST를 `localhost`로 설정한 계정으로 로그인이 성공한다.\n\n### 1-5. 실험2: 실험1에 `%`이 추가된다면?\n---\n![실험2 환경](test2_environment.png)\n\n만약 위와 같이 `kdkdhoho@%` 계정이 추가된다면 어떤 계정으로 접속하게 될까?<br>\n(참고로 위 계정의 암호는 `xyz`)\n\n아마 예상하기 쉽겠지만 `localhost`로만 접속이 가능하다.\n\n> 💡결론: HOST의 범위는 `localhost` < 지정 IP < `%` 인 것 같다.<br>\n> 참고) 추가 실험 결과, `127.0.0.1`도 `localhost`에 순위가 밀린다.\n\n## 2. 사용자 계정 관리\n\n### 2-1. 시스템 계정과 일반 계정\n\nMySQL 8.0부터는 `SYSTEM_USER` **권한**을 가지고 있냐에 따라 `시스템 계정`과 `일반 계정`으로 구분된다.\n\n> `SYSTEM_USER` 권한은 `ALL PRIVILEGES` 안에 포함된다.\n\n### 2-2. 두 계정의 차이\n\n#### 시스템 계정\n- MySQL \n- 시스템 계정과 일반 계정을 관리(생성, 삭제, 변경) 할 수 있다.\n- DB 서버 관리와 관련된 작업은 시스템 계정으로만 수행할 수 있다.\n  - 계정 관리\n  - Connection(세션) 강제 종료\n- DBA를 위한 계정\n\n#### 일반 계정\n- 시스템 계정을 관리할 수 없다.\n- 개발자나 응용 프로그램을 위한 계정\n\n### 2-3. 내장된 계정들\n\nMySQL 서버에는 다음과 같은 내장된 계정들이 존재한다. (`root@localhost` 제외)\n\n- `mysql.sys@localhost`: 8.0부터 기본으로 내장된 **sys 스키마의 객체들의 DEFINER**로 사용하는 계정\n  > sys 스키마: Performance Schema를 편하게 볼 수 있도록 별도로 만든 뷰의 모음\n- `mysql.session@localhost`: **MySQL 플러그인이 서버로 접근**할 때 사용하는 계정\n- `mysql.infoschema@localhost`: **information_schema에 정의된 뷰의 DEFINER**로 사용하는 계정\n  > information_schema: DB의 메타 정보(테이블 칼럼, 인덱스 등의 스키마 정보)의 모음\n\n위 세 계정은 처음부터 잠겨있는 상태이다. `account_locked` 칼럼을 통해 확인할 수 있다.\n\n![계정 잠김 확인](account_lock.png)\n\n## 3. 계정 생성\n\n`CREATE USER` 명령으로 계정을 생성할 수 있다.<br>\n`GRANT` 명령으로 권한 부여를 할 수 있다.\n\n> 5.7v 까지는 `GRANT` 명령으로 생성과 동시에 권한 부여를 할 수 있었다.\n\n### 3-1. 다양한 옵션 설정\n\n계정을 생성할 때, 아래와 같은 다양한 옵션들을 설정할 수 있다.\n\n- 인증 방식과 비밀번호\n- 비밀번호 관련 옵션(유효 기간, 이력 개수, 재사용 불가 기간)\n- 기본 역할(Role)\n- SSL 옵션\n- 계정 잠금 여부\n\n위 옵션을 모두 적용한 임의의 쿼리문을 통해 하나씩 살펴보자.\n\n```sql\nCREATE USER 'user'@'{HOST}'\n    IDENTIFIED WITH 'mysql_native_password' BY 'password'\n    REQUIRE NONE\n    PASSWORD EXPIRE INTERVAL 30 DAY\n    ACCOUNT UNLOCK\n    PASSWORD HISTORY DEFAULT\n    PASSWORD REUSE INTERVAL DEFAULT\n    PASSWORD REQUIRE CURRENT DEFAULT;\n```\n\n### 3-2. IDENTIFIED WITH\n\n계정 인증 방식과 비밀번호를 설정한다.<br>\n예) `IDENTIFIED WITH {계정 인증 방식} BY {비밀번호}`\n\n계정 인증 방식은 플러그인 형태로 제공되며, 아래 4가지 방식이 가장 대표적이다.\n\n- `Native Pluggable Authentication`: v5.7까지 기본으로 사용되던 방식이다. 단순하게 비밀번호에 대한 해시(SHA-1 알고리즘) 값을 저장하고, 클러이언트가 보낸 값과 해시값이 일치하는지 비교한다.\n- `Caching SHA-2 Pluggable Authentication`: v8.0의 기본 인증 방식. 암호화 해시값 생성을 위해 SHA-2(256비트) 알고리즘을 사용한다. `Native` 방식은 입력과 해시값이 매번 동일하지만, `SHA-2` 방식은 내부적으로 Salt 키를 활용해 수천 번의 해시 계산을 수행해 계산을 만들고 결국 동일한 입력일지라도 매번 결과가 달라진다. 이는 속도가 매우 느린 점을 고려하여, MySQL 서버는 해시 결과값을 메모리에 캐시해서 사용한다. 따라서 이름에 `Caching`이 포함되었다. 이 방식을 사용하려면 SSL/TLS 또는 RSA 키페어를 반드시 사용해야하고, 이를 위해 접속할 때 SSL 옵션을 활성화해야 한다.\n- PAM Pluggable Authentication\n- LDAP Pluggable Authentication\n\n이때, `IDENTIFIED BY {비밀 번호}`만 입력한다면 MySQL 서버의 기본 인증 방식으로 사용된다.\n\n> MySQL 5.7에서 8.0으로 넘어오면서 기본 인증 방식의 변경으로 인해, SSL/TLS 또는 RSA 키페어가 필요해질 수 있다.\n> 따라서 보안 수준은 낮아지더라도, 하위 호환성을 고려하면 `Native` 인증 방식으로 계정을 생성해야 할 수 있다.<br>\n> <br>\n> MySQL 8.0에서 `Native Authentication`을 기본 인증 방식으로 설정하려면 다음과 같이 설정을 변경하거나 my.cnf 설정 파일에 추가하면 된다.<br>\n> `SET GLOBAL default_authentication_plugin=\"mysql_native_password`\n\n### 3-2. REQUIRE\n\n서버에 접속할 때 암호화된 SSL/TLS 채널을 사용할지 여부를 설정한다.\n\nDEFAULT 값은 비암호화 채널로 설정된다.\n\n하지만 `REQUIRE` 옵션을 `SSL`로 설정하지 않아도, `Caching SHA-2 Authentication` 인증 방식을 사용하여 계정을 생성하면 해당 계정으로 접속할 때 암호화된 채널만으로 서버에 접속하게 된다.\n\n### 3-3. PASSWORD EXPIRE\n\n비밀번호의 유효 기간을 설정한다.\n\nDEFAULT 값은 `default_password_lifetime` 시스템 변수에 저장된 기간으로 유효 기간이 설정된다.\n\n> 스프링 접근용 계정에 유효 기간을 설정하면 큰일이 날 수 있다!\n\n설정 가능한 옵션은 다음과 같다.\n\n- `PASSWORD EXPIRE`: 계정 생성과 동시에 비밀번호의 만료 처리\n- `PASSWORD EXPIRE NEVER`: 비밀번호의 만료 기간 없음\n- `PASSWORD EXPIRE DEFAULT`\n- `PASSWORD EXPIRE INTERVAL n DAY`: 유효기간을 오늘부터 n일로 설정\n\n### 3-4. PASSWORD HISTORY\n\n비밀번호 이력을 얼마나 저장할 지에 대한 옵션이다.\n\n설정 가능한 옵션은 다음과 같다.\n\n- `PASSWORD HISTORY DEFAULT`: `password_history` 시스템 변수에 저장된 개수만큼 비밀번호 이력을 저장하며, 저장된 이력에 남아있는 비밀번호는 재사용할 수 없다.\n- `PASSWORD HISTORY n`: 최근 사용한 n개의 비밀번호를 저장한다.\n\n> 시스템 변수에 저장된 값으로 DEFAULT 값이 설정된다고 한다.\n> 만약 설정하지도 않았으면 어떻게 될까?\n> ![시스템 변수에도 설정하지 않았을 때.png](dont-setting.png)\n> 위와 같이 `NULL` 값으로 존재한다.\n\n### 3-5. PASSWORD REUSE INTERVAL\n\n한 번 사용했던 비밀번호의 재사용 금지 기간을 설정한다.\n\n기본적으로 `password_reuse_interval` 시스템 변수에 저장된 기간으로 설정된다.\n\n설정 가능한 옵션은 다음과 같다.\n\n- `PASSWORD REUSE INTERVAL DEFAULT`\n- `PASSWORD REUSE INTERVAL n DAY`: n일 이후에 비밀번호를 재사용할 수 있다.\n\n### 3-6. PASSWORD REQUIRE\n\n비밀번호가 만료되어 새로운 비밀번호로 변경할 때 현재 비밀번호를 필요로 할지 말지를 결정한다.\n\n기본적으로 `password_require_current` 시스템 변수의 값으로 설정된다.\n\n설정 가능한 옵션은 다음과 같다.\n\n- `PASSWORD REQUIRE CURRENT`: 비밀번호를 변경할 때 현재 비밀번호를 먼저 입력하도록 한다.\n- `PASSWORD REQUIRE OPTIONAL`: 비밀번호를 변경할 때 현재 비밀번호를 입력하지 않아도 되도록 설정한다.\n- `PASSWORD REQUIRE DEFAULT`\n\n### 3-7. ACCOUNT LOCK / UNLOCK\n\n계정을 사용하지 못하게 한다.\n\n- `ACCOUNT LOCK`: 계정을 사용하지 못하게 잠금\n- `ACCOUNT UNLOCK`: 잠긴 계정을 다시 사용 가능 상태로 잠금 해제\n\n## 4. 비밀번호 관리\n\n### 4-1. 고수준 비밀번호\n\n비밀번호의 규칙을 강제할 수 있는 방법이 있다.\n\n`validate_password` 컴포넌트를 활용하면 된다.\n\n이는 크게 중요하지 않은 부분인 것 같아 pass\n\n### 4-2. 비밀번호 이중화\n\n**[문제]**<br>\n스프링이 DB 서버의 하나의 계정을 사용중이다.<br>\n근데 만약 계정의 비밀번호를 바꾼다면 서비스에 장애가 발생할 것이다.<br>\n이로 인해 결국 하나의 암호를 몇 년 동안 사용하게 될 수도 있다.\n\n**[해결책]**<br>\n이를 해결하기 위해 비밀번호로 2개의 값을 **동시에** 사용할 수 있도록 하는 기능이 존재한다.\n\n**[설명]**<br>\n2개의 비밀번호는 Primary, Secondary로 구분된다.\n\n가장 최근 설정한 비밀번호는 Primary, 기존 비밀번호는 Secondary가 된다.\n\n이중 비밀번호를 사용하려면 다음과 같이 기존 비밀번호 변경 문법에 `RETAIN CURRENT PASSWORD` 옵션만 추가하면 된다.\n\n```sql\n-- 비밀번호를 \"abcd\"로 설정\nmysql> ALTER USER 'root'@'localhost' IDENTIFIED BY 'abcd';\n\n-- 비밀번호를 \"1234\"로 변경하면서 기존 비밀번호 \"abcd\"를 세컨더리 비밀번호로 설정\nmysql> ALTER USER 'root'@'localhost' IDENTIFIED BY '1234' RETAIN CURRENT PASSWORD;\n```\n\n그리고 root 계정으로 접속을 시도할 때 비밀번호를 \"abcd\" 또는 \"1234\"로 해도 접근이 가능하다.\n\n이렇게 설정한 후, 스프링의 접속 설정을 새로운 비밀번호로 변경하고 배포 및 재시작한다.\n\n이제 기존 비밀번호은 Secondary 비밀번호는 보안을 위해 삭제해주자.\n\n## 5. 권한\n\nv5.7까지 **글로벌 권한**과 **객체 권한**으로 구분됐다.\n\n- **글로벌 권한**: DB 혹은 테이블 이외의 객체에 적용되는 권한\n  - `GRANT` 명령에서 특정 객체를 명시하지 말아야 한다.\n- **객체 권한**: DB나 테이블을 제어하는 데 필요한 권한\n  - `GRANT` 명령으로 권한을 부여할 때 반드시 특정 객체를 명시해야 한다.\n- **ALL or (ALL PRIVILEGES)**: 글로벌과 객체 권한 두 가지 용도로 사용할 수 있다.\n  - 특정 객체에 ALL 권한이 부여되면 해당 객체에 적용될 수 있는 모든 권한을 부여한다.\n  - 글로벌로 ALL이 사용되면 글로벌 수준의 모든 권한이 부여된다.\n\n![정적 권한 1.png](static_privilege_1.png)\n![정적 권한 2.png](static_privilege_2.png)\n\nv8.0부터는 아래의 동적 권한이 추가됐다.\n\n> 정적 권한: MySQL 서버의 소스코드에 고정적으로 명시돼 있는 권한\n> 동적 권한: MySQL 서버의 컴포넌트나 플러그인이 설치되면 그때 등록되는 권한\n\n![동적 권한.png](dynamic_privilege.png)\n\n### 5-1. 사용자에게 권한 부여하기\n\n기본적으로 `GRANT` 명령을 사용한다.\n\n이때, 부여하려는 권한의 특성에 따라 `GRANT` 명령의 `ON` 절에 명시되는 객체의 내용이 바뀌어야 한다.\n\n```sql\n-- 권한 부여하는 문법의 기본적인 틀\n-- 부여하려는 권한을 추가한다. \",\" 를 구분자로 하여 여러 개를 동시에 명령할 수 있다.\nmysql> GRANT {권한} ON db.table TO 'user'@'host';\n\n-- 참고: GRANT OPTION 권한은 마지막에 WITH GRANT OPTION을 추가한다.\nmysql> GRANT OPTION ON db.table TO 'user'@'host' WITH GRANT OPTION;\n\n-- 글로벌 권한은 ON 절에 항상 *.* 을 사용한다.\n-- *.*은 모든 DB의 객체를 포함해 MySQL 서버 전체를 의미한다.\nmysql> GRANT SUPER ON *.* TO 'user'@'localhost';\n\n-- DB 권한은 특정 DB에 대해서만 권한을 부여하거나 서버에 존재하는 모든 DB에 대해 권한을 부여할 수 있다.\n-- 여기서 DB라 함은 DB 내부에 존재하는 테이블뿐만 아니라 스토어드 프로그램들도 모두 포함이다.\n-- 하지만 DB 권한만 부여하는 경우 employees.department와 같이 테이블까지 명시할 수 없다.\nmysql> GRANT EVENT ON *.* TO 'user'@'localhost';\nmysql> GRANT EVENT ON employees.* TO 'user'@'localhost';\n\n-- 테이블 권한은 서버의 모든 DB에 대해, 특정 DB의 모든 오브젝트에 대해, 특정 DB의 특정 테이블에 대해서 권한을 부여할 수 있다.\nmysql> GRANT SELECT,INSERT,UPDATE ON *.* TO 'user'@'localhost';\nmysql> GRANT SELECT,INSERT,UPDATE ON employees.* TO 'user'@'localhost';\nmysql> GRANT SELECT,INSERT,UPDATE ON employees.department TO 'user'@'localhost';\n```\n\n### 5-2. 실제 권한을 사용한 사례\n\n우아한테크코스 프로젝트 요구사항 중, 운영 DB 서버를 `DROP`하지 못하도록 하는 요구사항이 있었다.\n\n이를 `celuveat_prod` 계정의 권한에서 `DROP`을 제거하는 식으로 요구사항을 만족했다.\n\n### 5-3. 권한 확인하기\n\n```sql\n# 사용자별 권한 확인\nmysql> SHOW GRANTS FOR '사용자계정'@'호스트';\n# 접속된 계정 권한 확인\nmysql> SHOW GRANTS FOR CURRENT_USER;\n```\n\n## 6. 역할(Role)\n\nv8.0부터 권한을 묶어 역할(Role)을 사용할 수 있게 됐다.\n\n```sql\n-- 역할 생성\nmysql> CREATE ROLE role_emp_read, role_emp_write;\n\n-- 역할에 권한 부여\nmysql> GRANT SELECT ON employees.* TO role_emp_read;\nmysql> GRANT INSERT, UPDATE, DELETE ON employees.* TO role_emp_write;\n\n-- 계정 생성\nmysql> CREATE USER reader@localhost IDENTIFIED BY 'abcd';\nmysql> CREATE USER writer@localhost IDENTIFIED BY '1234';\n\n-- 계정에 역할 부여\nmysql> GRANT role_emp_reader TO reader@localhost;\nmysql> GRANT role_emp_writer TO writer@localhost;\n\n-- 역할 활성화\n-- 주의! 계정이 로그아웃됐다가 로그인하면 역할이 비활성화된다. 아래 명령어를 통해 활성화를 시켜주자.\n-- 자동으로 활성화를 시켜주려면 activate_all_roles_on_login 시스템 변수를 ON 으로 설정해주자.\nmysql> SET ROLE role_emp_reader;\nmysql> SET ROLE role_emp_writer;\n```"},{"excerpt":"📝 질문 리스트 병행성(동시성)과 병렬성에 대해 설명해주세요. 프로세스 동기화에 대해 설명해 주세요. Critical Section(임계 영역)에 대해 설명해주세요. Race Condition(경쟁 조건)이 무엇인가요? Mutual Exclusion(상호 배제)에 대해 설명해주세요. 뮤텍스(Mutex)에 대해 설명해주세요. 세마포어에 대해 설명해주세요. …","fields":{"slug":"/os-interview-study-4week/"},"frontmatter":{"date":"November 30, 2023","title":"[JSCODE] - OS 면접 스터디 4주차","tags":["operating-system","interview"]},"rawMarkdownBody":"\n## 📝 질문 리스트\n\n### 병행성(동시성)과 병렬성에 대해 설명해주세요.\n\n```\n[병행성]\n병행성은 코어 1개를 가진 CPU가 2개 이상의 작업을 처리하는 것을 생각하면 됩니다.\n\n이는 마치 사용자 입장에서 동시에 실행되는 것처럼 보이는데요.\n실제로 두 작업을 동시에 실행하는 것이 아닌, 두 작업을 매우 빠른 속도로 번갈아가면서 처리를 합니다. 이 속도가 매우 빠르기에 사용자 입장에서는 동시에 처리되는 것처럼 보입니다.\n\n[병렬성]\n코어 2개를 가진 CPU가 2개 이상의 작업을 병렬적으로 처리한다고 생각하면 됩니다.\n\n두 방식 모두 여러 작업이 공유 자원에 접근할 때 생길 수 있는 여러 문제가 존재합니다.\n```\n\n### 프로세스 동기화에 대해 설명해 주세요.\n\n```\n여러 작업이 공유 자원을 사용할 때 생길 수 있는 동시성 문제를 방지하기 위한 개념입니다.\n이를 적용하는 방법으로는 스핀락, 뮤텍스, 세마포어가 있습니다.\n```\n\n### Critical Section(임계 영역)에 대해 설명해주세요.\n\n```\n여러 프로세스가 공유 자원을 사용하려고 할 때, 그 공유 자원에 접근해서 사용하는 과정까지의 영역입니다. 프로세스 동기화를 위해 사용되는 개념입니다.\n```\n\n### Race Condition(경쟁 조건)이 무엇인가요?\n\n```\n여러 프로세스/쓰레드가 동시에 같은 자원에 접근하여 조작할 때, 작업들의 타이밍이나 작업 순서에 따라 결과가 기대했던 것과 달라지는 상황을 의미합니다.\n\n<자세한 예시>\n두 개의 쓰레드가 하나의 전역 변수를 수정하는 상황을 가정해보겠습니다.\n전역 변수를 +1 하려는 작업은 CPU 레벨에서 값 조회, 연산, 갱신의 순으로 동작합니다.\n만약 쓰레드 1번이 값 조회를 하고, 값을 더한 상태에서 쓰레드 2번으로 컨텍스트 스위칭이 일어난다면, 쓰레드 2번은 아직 변경되지 않은 전역 변수 값을 읽고, 더하게 됩니다. 이 상태에서 두 쓰레드 모두 메모리에 값을 갱신하다면 어떤 순서든 간에 1로 갱신됩니다.\n이러한 상황을 경쟁 조건이라고 합니다.\n\n이 경쟁 조건은, 자원에 접근하여 사용하는 영역을 임계 영역으로 설정하고 이 임계 영역에 오직 하나의 쓰레드만이 접근할 수 있도록 구현함으로써 해결할 수 있습니다.\n대표적인 예로는 스핀락, 뮤텍스, 세마포어 방식이 있습니다.\n```\n\n### Mutual Exclusion(상호 배제)에 대해 설명해주세요.\n\n```\n한국말로 '상호 배제' 라는 말입니다.\n즉, 하나의 쓰레드가 공유 자원을 사용할 때 다른 쓰레드가 접근하지 못하도록 한다는 개념입니다.\n```\n\n### 뮤텍스(Mutex)에 대해 설명해주세요.\n\n```\nMutual Exlcusion의 약자로, 상호 배제를 위한 동기화 기법 중 하나입니다.\n**하나의 임계 영역**에 **하나의 프로세스나 쓰레드**만 들어가 작업을 할 수 있다는 개념입니다.\n\n이 기법은 뮤텍스 락이라는 개념을 사용합니다.\n\n보통 임계 영역에 하나의 공유 자원이 존재합니다.\n\n**상호 배제를 할 수 있는 방법**으로는 **스핀락 방식과 뮤텍스 방식**이 존재합니다.\n두 방법 모두 **Mutex Lock이라는 개념을 사용**합니다. 이 락은 1개만 존재합니다.\n임계 영역에 접근할 때 이 락을 획득할 수 있다면 락을 획득하고 임계 영역에 접근합니다.\n만약 이때 다른 프로세스가 임계 영역에 접근하려고 할 때 이 락을 획득할 때까지 임계 영역에 접근할 수 없습니다. 락을 획득하고 나서야 비로소 임계 영역에 접근 가능합니다.\n\n여기서 락을 획득하는 방식의 차이가 스핀락 방식과 뮤텍스 방식의 차이입니다.\n\n스핀락 방식의 경우 락을 획득할 때까지 계속해서 확인합니다.\n이와 다르게 뮤텍스 방식의 경우, 락을 획득하지 못하면 본인을 대기 큐에 넣어두고 임계 영역에서 처리 중인 프로세스가 락을 반환할 때, 대기 큐에 있는 프로세스를 깨우는 방식입니다.\n\n스핀락 방식의 경우 락을 획득할 때까지 CPU를 무한정 사용할 수 있는 문제점이 있지만, 멀티 코어 환경이며 임계 영역에서의 작업 시간이 컨텍스트 스위칭보다 더 빨리 끝난다면 뮤텍스에 비해 더 이점이 있다.\n뮤텍스는 대기 큐로 갔다가, 깨움을 당해 다시 CPU를 할당받는 과정에서 컨텍스트 스위칭 작업이 필요하다. 하지만, 임계 영역에서의 작업이 빨리 끝나면 굳이 대기 큐로 갔다오는 작업과 컨텍스트 스위칭 작업이 불필요해지기 때문이다.\n게다가 싱글 코어에서 스핀락 방식은 컨텍스트 스위칭이 필연적으로 발생하기 때문이다. 멀티 코어에서 스핀락 방식을 적용한다면 컨텍스트 스위칭없이 Core1은 Lock을 점유하고 Core2는 Lock에 대해 계속해서 대기하는 상황이라면 컨텍스트 스위칭없이 더 빠르게 Lock을 획득할 수 있는 것이다.\n```\n\n### 세마포어에 대해 설명해주세요.\n\n```\n여러 개의 쓰레드가 공유 자원에 동시에 접근하는 것을 **제한하기 위한 정수**의 개념입니다.\n\n뮤텍스와 달리 하나의 임계 영역에 여러 개의 공유 자원이 존재할 수 있습니다.\n따라서 임계 영역에 존재하는 공유 자원의 수만큼 정수가 초기화되고, 쓰레드가 세마포어에 접근 요청을 하면 정수를 1 감소시킵니다.\n만약 이 값이 0이라면 현재 임계 영역 내의 모든 공유 자원을 각각의 쓰레드가 접근하고 있는 것이므로 더 이상 접근할 수 없습니다.\n이때, 다른 쓰레드가 접근 요청을 하게 되면 이 정수값은 -1로 감소하고 대기 큐에 들어갑니다.\n이후에 공유 자원을 모두 사용한 쓰레드가 임계 영역을 나올 때, 세마포어 값을 1 증가하고 대기 큐에 있는 쓰레드를 깨웁니다. 그리고 해당 쓰레드가 다시 한번 세마포어에 접근하게 됩니다.\n```\n\n### 뮤텍스(Mutex)와 이진 세마포어의 차이에 대해 설명해주세요.\n\n```\n세마포어는 하나의 임계 영역안에 접근할 수 있는 공유 자원의 수를 조절할 수 있는 점에서, 이진 세마포어는 뮤텍스를 대신할 수 있습니다.\n\n하지만 뮤텍스는 이진 세마포어를 대신할 수 없습니다.\n\n추가로 세마포어는 작업 간 실행 순서를 동기화할 수 있지만, 뮤텍스는 불가능합니다. <### 추가 학습 필요\n```\n\n### 모니터에 대해 설명해주세요.\n\n```\n세마포어를 이용하다보면 개발자의 실수로 인해 경쟁 조건이나, 데드락 또는 기아상태가 발생한다.\n\n하지만 이는 특정 상황에만 발생하는 문제들이므로 디버깅하기 매우 어렵다는 문제가 있다.\n\n따라서 프로세스 동기화를 더 쉽고 확실하게 사용하기 위한 방법으로 모니터 방식이 탄생했다.\n\n모니터 방식은 기본적으로 뮤텍스 방식을 사용해서 구현한다.\n\n모니터 방식을 설명하기 위해선 생산자-소비자 문제가 대표적이다.\n\n> 자세한 내용은 [테코톡](https://youtu.be/4t8BennljZA?si=lEfu-OTT2cMYlgrB)을 참고하자.\n\n간단히 설명하자면, 생산자와 소비자 사이에는 함께 사용할 수 있는 공간이 있고 생산자는 이 공간에 데이터를 저장하고, 소비자는 데이터를 소비한다.\n이때 공간이 꽉 차 있으면 생산자는 추가할 수 없고, 반대로 공간이 비어있으면 소비자는 소비할 수 없다.\n\n이를 모니터 방식을 통해 구현할 수 있다.\n\n각 메서드(생산자 or 소비자)는 락을 획득하여 임계 영역에 들어가면, 본인이 수행할 수 있는 조건인지를 먼저 확인한다. (공간이 비어있는지? 꽉찼는지?) 수행할 수 없는 조건이라면 본인을 대기 큐에 넣는다.\n\n수행할 수 있는 조건이라면 정해진 동작을 하고 모두 끝이 나면 대기 큐에 있는 무작위의 쓰레드를 깨우고 락을 반환하는 형식이다.\n```\n\n### 데드락이 무엇인가요?\n\n```\n여러 프로세스가 필요한 자원을 요청하는 과정에서 그 어떤 프로세스도 더 이상 진행할 수 없는 상태입니다.\n자원을 대기하는 blocked 상태에서 영구히 대기한다.\n```\n\n### 데드락 발생 조건 4가지를 설명해 주세요.\n\n```\n1. 상호 배제: 리소스는 한번에 하나의 쓰레드만이 접근할 수 있다.\n2. Hold and wait: 쓰레드가 이미 하나 이상의 리소스를 획득한 상태에서 다른 리소스를 획득하기 위해 대기하는 상태\n3. No preemption: 리소스 반환은 해당 리소스를 취득한 리소스만이 할 수 있다.\n4. Circular wait: 쓰레드들이 순환 형태로 \n```\n\n### 데드락 회피 방법은 무엇이 있나요?\n\n발생 조건 4가지 중 1가지만이라도 해결하면 된다.\n1) 리소스를 공유 가능하게 한다. -> 이는 경쟁 조건 문제 발생\n2) 사용할 리소스를 모두 획득한 뒤에 시작하도록 하거나, 리소스를 획득하지 않은 상태에서만 리소스를 요청하게끔 한다. -> 리소스 사용 효율이 떨어질 수 있다. 또는 인기 많은 리소스를 계속해서 기다려서 기아 상태에 발생할 수 있다.\n3) 추가 리소스를 획득하려면, 가지고 있는 리소스를 다른 리소스가 선점할 수 있도록한다.\n4) 모든 리소스에 순서 체계를 부여하고 오름차순으로 리소스를 요청한다.\n\n하지만 모두 약간의 문제들이 존재한다.\n따라서 데드락 회피(뱅커 알고리즘) 또는 데드락 감지 및 복구\n\n## 🤔 개인적인 궁금증\n\n### 세마포어의 정수값은 어떻게 동기화를 할 수 있을까?\n\n세마포어의 wait(), signal()에서 lock을 획득하기 위한 메서드는 CPU 레벨에서 Atomic한 연산을 지원한다.\n다시 말해, 하나의 쓰레드가 lock을 획득하기 위한 과정 도중에 다른 쓰레드가 연산 중간에 끼지 못한다.\n\n이를 통해 \n\n## 🎯 피드백\n\n임계 영역을 설명할 때 다소 설명이 장황해졌다.\n그 외에는 두괄식으로 답변하려고 노력하는 것이 잘 보였다."},{"excerpt":"TDD란? Test Driven Development. 테스트 주도 개발이다. 보통 '~~ 주도 개발' 이라고 이름이 붙으면, 개발할 때 '~~' 를 1순위로 여기며 개발하는 방법론을 의미한다. 즉, 테스트 주도 개발은 테스트를 1순위로 여기며 개발하는 개발 방법론이다. 개발 방법론! 어떻게 하나? 결론: 테스트 코드 작성 -> 프로덕션 코드 작성 -> …","fields":{"slug":"/about-tdd/"},"frontmatter":{"date":"November 29, 2023","title":"TDD 이해하기","tags":["tdd","test"]},"rawMarkdownBody":"\n## TDD란?\n\nTest Driven Development. 테스트 주도 개발이다.\n\n보통 '_~~ 주도 개발_' 이라고 이름이 붙으면, 개발할 때 '_~~_' 를 1순위로 여기며 개발하는 **방법론**을 의미한다.\n\n즉, 테스트 주도 개발은 테스트를 1순위로 여기며 개발하는 **개발 방법론**이다.\n\n**개발 방법론!**\n\n## 어떻게 하나?\n\n결론: _테스트 코드 작성 -> 프로덕션 코드 작성 -> 리팩터링_ 순으로 진행한다.\n\n### 1. 테스트 코드 작성\n\n우선 구현하려는 기능에 대해 단위 테스트를 작성한다.\n\n작성하고 돌리면 당연히 실패한다. 왜? 아직 프로덕션 코드가 없으니까.<br>\n어쩌면 컴파일 조차 안될 것이다.\n\n### 2. 프로덕션 코드 작성\n\n이제 기능을 구현한다.\n\n테스트 코드와 프로덕션 코드를 이상없이 작성했다면 테스트가 통과할 것이다.\n\n이때 중요한 점은, 코드 품질은 신경쓰지 않아도 된다.\n\n### 3. 리팩터링\n\n코드 품질은 이제 신경쓴다.\n\n돌아가는 쓰레기를 만들었다면, 이제 이쁜 쓰레기로 만들 차례이다.\n\n테스트 코드, 프로덕션 코드 모두 리팩터링을 진행한다.\n\n## 왜 하나?\n\nTDD를 해본 경험으로 이야기하자면, 버그가 생길 확률이 현저하게 떨어진다. <br>\n(높은 테스트 커버리지는 덤)\n\n확실히 테스트 코드로 내가 구현하려는 기능에 대해 확실한 **안전 장치**를 걸고 기능을 추가하니까, 아무래도 안전하다.\n\n구현하려는 기능에 대한 도메인 이해도가 어느 정도 있고, 주어진 시간이 충분하다면 나는 TDD를 가급적 할 것이다.\n\n주변 지인들에게도 추천할 것이다.\n\n## 단점은?\n\n기능을 작성하는 데까지 시간이 오래 걸린다.\n\n아무래도 돌다리르 건널 때 두들겨보고 건너는 개념인데, 이 두들기는 시간이 상당하다.\n\n체감 상 바로 프로덕션 코드를 작성할 때에 비해 2~3배는 걸리는 것 같다.\n\n또, 내가 구현하려는 기능에 대해 설계 방법이나 감이 잘 오지 않거나 개발 초기 단계라서 변화가 매우 빠른 상황이라면 나는 비추한다.\n\nTDD는 테스트 코드를 필연적으로 낳게 되는데, 이 테스트 코드는 위 상황에서는 걸림돌이기 때문이다.\n\nTDD는 개발 **방법론**일 뿐이니 너무 맹신하지말자.\n\n무엇이든 **상황에 맞게 판단**하자.\n\n## 착각하기 쉬운 것\n\n프로젝트에 TDD를 적용한다고 했을 때, 항상 모든 기능에 대해 테스트 코드를 먼저 작성할 수 없다.\n\n물론 구현하려는 기능에 대해 분명하고 확실한 상황이라면 TDD를 적용할 수 있지만, 개발이라는 게 처음 생각한대로 구현하다보면 생각지도 못한 기능을 함께 구현해야 하는 경우가 종종 있다.\n\n따라서 TDD를 처음 시도해보려는 사람들이 혹여나 \"테스트 코드를 먼저 작성하기 전까지는 기능 구현 안할거야 !!\" 라는 생각을 가질 수 있다고 생각한다.\n\n본인이 구현하려는 기능이 무엇인지 분명히 아는 선에서, TDD를 적용하는 것이 바람직해보인다.  "},{"excerpt":"이 글의 목적 JUnit과 AssertJ의 학습 테스트 코드를 통해 단위 테스트를 처음 해보려는 사람에게 어떻게 쓰는지 간단한 가이드 라인 제시 단위 테스트하면서 잘 사용하지 않는 기능에 대해 나만의 치트 시트 역할 전체 코드 자세한 전체 코드는 이 곳에 올려놓았습니다. 위 레포지토리를 로컬에 clone하고 step3 브랜치로 이동하면, 학습 테스트가 존…","fields":{"slug":"/using-junit-and-assertJ/"},"frontmatter":{"date":"November 28, 2023","title":"JUnit과 AssertJ 활용법","tags":["unit-test","JUnit","AssertJ","test"]},"rawMarkdownBody":"\n## 이 글의 목적\n\n- JUnit과 AssertJ의 학습 테스트 코드를 통해 단위 테스트를 처음 해보려는 사람에게 어떻게 쓰는지 간단한 가이드 라인 제시\n- 단위 테스트하면서 잘 사용하지 않는 기능에 대해 나만의 치트 시트 역할\n\n## 전체 코드\n\n자세한 전체 코드는 [이 곳](https://github.com/kdkdhoho/java-racingcar)에 올려놓았습니다.\n\n위 레포지토리를 로컬에 clone하고 step3 브랜치로 이동하면, 학습 테스트가 존재합니다.\n\n## 코드 바로 보기\n\n### 1. 배열 또는 리스트를 테스트하기\n```java\npublic class AssertJWithIterableStudyTest {\n\n    @Test\n    void filteredOnTest() {\n        List<Member> members = members();\n\n        assertThat(members).filteredOn(Member::job, \"woowa developer\")\n                .hasSize(2)\n                .containsOnly(\n                        new Member(2, \"woowa developer\"),\n                        new Member(4, \"woowa developer\")\n                );\n    }\n\n    @Test\n    void containsExactlyVScontainsOnly() {\n        List<Member> members = members();\n\n        // contains: 순서 상관없이 주어진 요소가 있기만 하면 된다.\n        assertThat(members).extracting(\"id\")\n                .contains(5, 3, 2, 4, 1);\n\n        // containsOnly: 순서 상관없이 주어진 요소만 있어야 한다.\n        assertThat(members).extracting(\"id\")\n                .containsOnly(5, 4, 3, 2, 1);\n\n        // containsExactly: 순서, 주어진 요소 모두 동일해야 한다.\n        assertThat(members).extracting(\"id\")\n                .containsExactly(1, 2, 3, 4, 5);\n    }\n\n    @Test\n    void extractingTest1() {\n        List<Member> members = members();\n\n        assertThat(members).extracting(\"id\")\n                .containsExactly(1, 2, 3, 4, 5);\n    }\n\n    @Test\n    void extractingTest2() {\n        List<Member> members = members();\n\n        assertThat(members).extracting(\"id\", \"job\")\n                .containsExactly(\n                        tuple(1, \"job seeker\"),\n                        tuple(2, \"woowa developer\"),\n                        tuple(3, \"student\"),\n                        tuple(4, \"woowa developer\"),\n                        tuple(5, \"delivery hero\")\n                );\n    }\n\n    /**\n     * [실행 결과]\n     * Multiple Failures (3 failures)\n     * -- failure 1 --\n     * expected: 2\n     * but was: 1\n     * at StringTest$IterableTestUsingAssertJ.lambda$softAssertionsTest$0(StringTest$IterableTestUsingAssertJ.java:103)\n     * -- failure 2 --\n     * expected: 3\n     * but was: 1\n     * at StringTest$IterableTestUsingAssertJ.lambda$softAssertionsTest$0(StringTest$IterableTestUsingAssertJ.java:104)\n     * -- failure 3 --\n     * expected: 4\n     * but was: 1\n     */\n    @Test\n    @Disabled\n    void softAssertionsTest() {\n        SoftAssertions.assertSoftly(softly -> {\n            softly.assertThat(1).isEqualTo(1);\n            softly.assertThat(1).isEqualTo(2);\n            softly.assertThat(1).isEqualTo(3);\n            softly.assertThat(1).isEqualTo(4);\n        });\n    }\n\n    public static List<Member> members() {\n        return List.of(\n                new Member(1, \"job seeker\"),\n                new Member(2, \"woowa developer\"),\n                new Member(3, \"student\"),\n                new Member(4, \"woowa developer\"),\n                new Member(5, \"delivery hero\")\n        );\n    }\n}\n\nclass Member {\n    private final int id;\n    private final String job;\n\n    public Member(int id, String job) {\n        this.id = id;\n        this.job = job;\n    }\n\n    public String job() {\n        return job;\n    }\n\n    @Override\n    public boolean equals(Object o) {\n        if (this == o) return true;\n        if (o == null || getClass() != o.getClass()) return false;\n        Member member = (Member) o;\n        return id == member.id;\n    }\n\n    @Override\n    public int hashCode() {\n        return Objects.hash(id);\n    }\n}\n```\n\n### 2. 예외 검증하기\n\n```java\npublic class ExceptionAssertionStudyTest {\n\n    private void throwIllegalStateException() {\n        throw new IllegalStateException(\"예외 1번\");\n    }\n\n    @Test\n    void exceptionTest_1() {\n        assertThatThrownBy(this::throwIllegalStateException)\n                .isInstanceOf(IllegalStateException.class)\n                .hasMessage(\"예외 1번\")\n                .hasMessageContaining(\"1번\")\n                .hasMessageEndingWith(\"1번\");\n    }\n\n    @Test\n    void exceptionTest_2() {\n        assertThatExceptionOfType(IllegalStateException.class).isThrownBy(() -> throwIllegalStateException())\n                .withMessage(\"예외 1번\")\n                .withMessageContaining(\"예외\")\n                .withMessageContaining(\"1번\")\n                .withMessageStartingWith(\"예외\")\n                .withMessageEndingWith(\"1번\");\n    }\n\n    /**\n     * 아래 Exception들 미리 제공\n     * assertThatNullPointerException\n     * assertThatIllegalArgumentException\n     * assertThatIllegalStateException\n     * assertThatIOException\n     */\n    @Test\n    void exceptionTest_3() {\n        assertThatIllegalStateException().isThrownBy(this::throwIllegalStateException)\n                .withMessage(\"예외 1번\")\n                .withMessageContaining(\"예외\")\n                .withMessageContaining(\"1번\")\n                .withMessageStartingWith(\"예외\")\n                .withMessageEndingWith(\"1번\");\n    }\n\n    @Test\n    void notExceptionTest() {\n        assertThatNoException().isThrownBy(AssertJWithIterableStudyTest::members);\n    }\n}\n```\n\n### 3. 테스트 메서드에 파라미터 전달하여 테스트하기\n\n```java\npublic class ParameterizedStudyTest {\n\n    /**\n     * [ValueSource가 지원하는 타입]\n     * short (with the shorts attribute)\n     * byte (bytes attribute)\n     * int (ints attribute)\n     * long (longs attribute)\n     * float (floats attribute)\n     * double (doubles attribute)\n     * char (chars attribute)\n     * java.lang.String (strings attribute)\n     * java.lang.Class (classes attribute)\n     */\n    @ParameterizedTest\n    @ValueSource(ints = {2, 4, 6})\n    void parameterizedTest_with_valueSource(int number) {\n        assertThat(number % 2 == 0).isTrue();\n    }\n\n    @ParameterizedTest\n    @NullSource\n    void parameterizedTest_with_nullSource(String input) {\n        assertThat(input == null).isTrue();\n    }\n\n    @ParameterizedTest\n    @EmptySource\n    void parameterizedTest_with_emptySource_Array(int[] numbers) {\n        assertThat(numbers.length).isZero();\n    }\n\n    @ParameterizedTest\n    @EmptySource\n    void parameterizedTest_with_emptySource_List(List<Integer> numbers) {\n        assertThat(numbers.size()).isZero();\n    }\n\n    @ParameterizedTest\n    @EmptySource\n    void parameterizedTest_with_emptySource_String(String input) {\n        assertThat(input).isEqualTo(\"\");\n    }\n\n    @ParameterizedTest\n    @NullAndEmptySource\n    @ValueSource(strings = {\" \", \"\\t\", \"\\n\"})\n    void parameterizedTest_with_NullAndEmptySource(String input) {\n        assertThat(Strings.isNullOrEmpty(input) || input.isBlank()).isTrue();\n    }\n\n    private enum Direction {\n        NORTH, EAST, SOUTH, WEST;\n    }\n\n    // pass all 4 directions\n    @ParameterizedTest\n    @EnumSource(value = Direction.class)\n    void parameterizedTest_with_EnumSource_All_Passing(Direction direction) {\n        assertThat(direction).isIn(Direction.NORTH, Direction.EAST, Direction.SOUTH, Direction.WEST);\n    }\n\n    @ParameterizedTest\n    @EnumSource(value = Direction.class, names = {\"NORTH\", \"SOUTH\"})\n    void parameterizedTest_with_EnumSource_names_filter(Direction direction) {\n        assertThat(direction).isIn(Direction.NORTH, Direction.SOUTH);\n        assertThat(direction).isNotIn(Direction.EAST, Direction.WEST);\n    }\n\n    @ParameterizedTest\n    @EnumSource(\n            value = Direction.class,\n            names = {\"NORTH\", \"SOUTH\", \"EAST\"}, // Enum의 value 이름과 매칭한다. 대소문자 구분한다.\n            mode = EnumSource.Mode.EXCLUDE\n    )\n    void parameterizedTest_with_EnumSource_names_filter_exclude(Direction direction) {\n        assertThat(direction).isNotIn(Direction.NORTH, Direction.SOUTH);\n        assertThat(direction).isIn(Direction.EAST, Direction.WEST);\n    }\n\n    @ParameterizedTest\n    @CsvSource({\"test,TEST\", \"java,JAVA\"})\n    void parameterizedTest_with_CsvSource(String input, String expected) {\n        assertThat(input.toUpperCase()).isEqualTo(expected);\n    }\n\n    @ParameterizedTest\n    @CsvSource(value = {\"test:TEST\", \"java:JAVA\"}, delimiter = ':')\n    void parameterizedTest_with_CsvSource_Using_Delimiter(String input, String expected) {\n        assertThat(input.toUpperCase()).isEqualTo(expected);\n    }\n\n    @ParameterizedTest\n    @CsvFileSource(resources = \"/csvFile.txt\", numLinesToSkip = 1)\n    void parameterizedTest_with_CsvSource_Using_CsvFile(String input, String expected) {\n        assertThat(input.toUpperCase()).isEqualTo(expected);\n    }\n\n    @ParameterizedTest\n    @MethodSource(\"provideStringForIsBlank\")\n    void parameterizedTest_with_MethodSource(String input, boolean expected) {\n        assertThat(StringUtils.isBlank(input)).isEqualTo(expected);\n    }\n\n    private static Stream<Arguments> provideStringForIsBlank() {\n        List<String> inputs = new ArrayList<>();\n        for (int i = 0; i < 4; i++) {\n            if (i == 3) {\n                String input = \"not Empty\";\n                inputs.add(input);\n                continue;\n            }\n\n            String input = \" \".repeat(i);\n            inputs.add(input);\n        }\n\n        return Stream.of(\n                Arguments.of(inputs.get(0), true),\n                Arguments.of(inputs.get(1), true),\n                Arguments.of(inputs.get(2), true),\n                Arguments.of(inputs.get(3), false)\n        );\n    }\n\n    private static class BlankStringArgumentsProvider implements ArgumentsProvider {\n\n        @Override\n        public Stream<? extends Arguments> provideArguments(ExtensionContext context) throws Exception {\n            return Stream.of(\n                    Arguments.of((String) null),\n                    Arguments.of(\"\"),\n                    Arguments.of(\" \")\n            );\n        }\n    }\n\n    @ParameterizedTest\n    @ArgumentsSource(BlankStringArgumentsProvider.class)\n    void parameterizedTest_with_MethodSource_Using_ArgumentsProvider(String input) {\n        assertThat(StringUtils.isBlank(input)).isTrue();\n    }\n\n    static Stream<Arguments> arguments = Stream.of(\n            Arguments.of(null, true),\n            Arguments.of(\"\", true),\n            Arguments.of(\" \", true),\n            Arguments.of(\"Not Empty\", false)\n    );\n\n    private static class SlashyDateConverter implements ArgumentConverter {\n\n        @Override\n        public Object convert(Object source, ParameterContext context) throws ArgumentConversionException {\n            if (!(source instanceof String)) {\n                throw new IllegalArgumentException(\"문자열이 아닙니다. 입력값:\" + source);\n            }\n\n            try {\n                String[] parts = ((String) source).split(\"/\");\n                int year = Integer.parseInt(parts[0]);\n                int month = Integer.parseInt(parts[1]);\n                int day = Integer.parseInt(parts[2]);\n\n                return LocalDate.of(year, month, day);\n            } catch (NumberFormatException e) {\n                e.printStackTrace();\n                throw new IllegalArgumentException(\"숫자가 아닙니다. 입력값:\" + source);\n            }\n        }\n    }\n\n    @ParameterizedTest\n    @CsvSource({\"2023/11/28\", \"2023/12/28\"})\n    void parameterizedTest_with_CustomConverter(@ConvertWith(SlashyDateConverter.class) LocalDate date) {\n        assertThat(date.getYear()).isEqualTo(2023);\n        assertThat(date.getMonth()).isGreaterThan(Month.OCTOBER);\n        assertThat(date.getDayOfMonth()).isEqualTo(28);\n    }\n\n    @ParameterizedTest(name = \"{index} - Parameter is {0}\")\n    @EnumSource(value = Direction.class, names = {\"SOUTH\", \"NORTH\"})\n    void displayNameTest(Direction direction) {\n    }\n}\n```\n\n### 4. 두 객체 리스트의 필드 비교하기\n\n```java\npublic class UsingRecursiveComparisonStudyTest {\n\n    @Test\n    void usingRecursiveComparisonTest_1() {\n        List<Member> members = List.of(\n                new Member(1, \"backend developer\"),\n                new Member(2, \"DBA\"),\n                new Member(3, \"frontend developer\")\n        );\n\n        List<Member> other = List.of(\n                new Member(1, \"DBA\"),\n                new Member(2, \"frontend developer\"),\n                new Member(3, \"backend developer\")\n        );\n\n        assertThat(members).usingRecursiveComparison()\n                .comparingOnlyFields(\"job\")\n                .ignoringCollectionOrder()\n                .isEqualTo(other);\n    }\n\n    @Test\n    void usingRecursiveComparisonTest_2() {\n        Member member = new Member(1, \"DBA\");\n        Member other = new Member(2, \"DBA\");\n\n        assertThat(member).usingRecursiveComparison()\n                .comparingOnlyFields(\"job\")\n                .isEqualTo(other);\n    }\n}\n```\n\n## 마치며\n\n더 나은 방법이 있거나 제안이 있다면 언제든 댓글 달아주시면 감사하겠습니다!"},{"excerpt":"📝 질문 리스트 SQL에 대해서 설명해주세요. C언어와 같은 프로그래밍 언어와 어떤 차이가 있나요? 개발자가 작성한 SQL이 어떤 과정을 통해 실행 되는지 설명해주세요. DML은 무엇인가요? 어떤 구문이 있는지도 설명해주세요. DDL은 무엇인가요? 어떤 구문이 있는지도 설명해주세요. DCL은 무엇인가요? 어떤 구문이 있는지도 설명해주세요. 참조 무결성에 …","fields":{"slug":"/db-interview-study-2week/"},"frontmatter":{"date":"November 28, 2023","title":"DB 면접 스터디 2주차","tags":["database","interview"]},"rawMarkdownBody":"\n## 📝 질문 리스트\n\n### SQL에 대해서 설명해주세요. C언어와 같은 프로그래밍 언어와 어떤 차이가 있나요?\n\n```\nSQL은 DBMS에게 명령하는 언어입니다.\n\n반면 C언어는 프로그래머가 작성한 코드가 C언어 컴파일러에 의해 기계어로 번역이 됩니다.\n이 기계어는 OS에 의해 코드가 작성된대로 실행됩니다. \n\n즉, SQL은 DMS에게 요청을 보내는 반면 프로그래밍 언어는 미리 작성된 대로 수행된다는 차이점이 있습니다.\n```\n\n### 개발자가 작성한 SQL이 어떤 과정을 통해 실행 되는지 설명해주세요.\n\n```text\n크게 나눠 파싱, 최적화, 실행의 단계로 실행됩니다.\n\nDBMS는 개발자가 작성한 SQL문을 파싱하고 문법 오류가 없는지 검사합니다.\n오류가 없다면 파싱한 SQL문을 통해 가장 효율적인 실행 계획을 수립합니다.\n그 다음 최적화된 실행 계획에 따라 쿼리문을 실행합니다.\n```\n\n### DML은 무엇인가요? 어떤 구문이 있는지도 설명해주세요.\n\n```text\n데이터를 조작하는 쿼리문입니다.\n테이블에 저장된 데이터와 관련된 쿼리문으로 이해하고 있습니다.\n\nINSERT, SELECT, UPDATE, DELETE 문이 있습니다.\n```\n\n### DDL은 무엇인가요? 어떤 구문이 있는지도 설명해주세요.\n\n```text\n데이터를 정의하는 언어입니다.\n테이블과 관련된 쿼리문으로 이해하고 있습니다.\n\n보통 CREATE, ALTER, DROP, TRUNCATE 문이 있습니다.\n```\n\n### DCL은 무엇인가요? 어떤 구문이 있는지도 설명해주세요.\n\n```text\n주로 보안과 관련된 언어업니다.\n\nGRANT, REVOKE, COMMIT, ROLLBACK 문이 있습니다.\n```\n\n### 참조 무결성에 대해서 설명해주세요.\n\n```text\n테이블 간의 관계를 정의할 때 반드시 지켜야하는 제약 조건입니다.\n\n외래키를 설정할 때 반드시 관계를 맺으려는 테이블의 기본키 값으로 해야 한다는 것입니다.\n만약 값이 없을 경우 외래키 값은 NULL이 될 수도 있습니다.\n```\n\n### CASCADE 설정에 대해서 설명해주세요.\n\n```text\n외래키를 참조하고 있는 행에 삭제 또는 변경이 발생할 때, 그에 따라 참조하고 있는 행을 동시에 관리하는 옵션입니다.\n\nUPDATE CASCADE의 경우 참조되는 기본키가 변경될 때, 참조하는 행의 외래키도 함께 변경합니다.\nDELETE CASCADE의 경우 참조되는 기본키의 행이 삭제될 때, 참조하는 행도 함께 삭제한다는 것입니다. \n```\n\n### VIEW에 대해서 설명해주세요.\n\n```text\n기존 테이블에서 사용자가 필요로한 데이터만을 따로 추출하여 만든 임의의 가상 테이블입니다.\n\nVIEW는 실제로 물리적으로 저장하지 않고 기존 테이블의 데이터를 참조하여 가상으로 생성됩니다.\n이를 통해 대규모거나 복잡한 데이터 조회의 경우 VIEW 테이블로 추출하여 쉽고 빠르게 조회할 수 있습니다.\n\n하지만 VIEW가 참조하고 있는 데이터가 변경될 경우, VIEW도 함께 변경해줘야 한다는 특징이 있습니다. \n```\n\n### SELECT 절의 처리순서에 대해서 설명해주세요.\n\n```text\nFROM -> WHERE -> GROUP BY -> HAVING -> SELECT -> ORDER BY 순으로 처리됩니다.\n```\n\n### SELECT ~ FOR UPDATE 구문에 대해서 설명해주세요.\n\n```text\nSELECT 절에 해당하는 행들에 접근할 때 LOCK을 설정하는 구문입니다.\n\n이 구문을 사용하면 해당 행에는 하나의 트랜잭션 만이 값을 변경할 수 있습니다.\nLOCK이 걸린 도중에 다른 트랜잭션이 동일한 행을 수정하려 한다면 LOCK이 해제될 때까지 대기해야합니다.\n이 LOCK은 커밋 또는 롤백을 할 때까지 유지됩니다.\n\n이를 통해 동시성 문제를 해결할 수 있지만, 성능에 악영향을 줄 수 있습니다.\n```\n\n### GROUP BY 절에 대해서 설명해주세요.\n\n```text\nGROUP BY 절을 통해 특정 칼럼의 데이터를 그룹화할 수 있습니다.\n\n보통 집계 함수와 함께 사용하여 유의미한 결과를 도출할 수 있습니다.\n```\n\n### ORDER BY 절에 대해서 설명해주세요.\n\n```text\n쿼리의 결과를 특정 열을 기준으로 정렬하기 위해 사용하는 절입니다.\n\n기본적으로 오름차순으로 정렬을 하지만 내림차순으로 정렬할 수 있으며,\n정렬의 기준이 되는 열을 2개 이상 설정할 수 있습니다. \n```\n\n### INNER JOIN과 OUTER JOIN에 대해서 설명해주세요.\n\n```text\nINNER JOIN은 JOIN 하려는 두 테이블의 특정 열을 기준으로 동일한 행들만 반환하는 명령어입니다.\n\nOUTER JOIN은 두 테이블의 동일한 행 뿐만 아니라, 조건에 일치하지 않는 특정 테이블의 행들도 함께 반환하는 명령입니다.\n```\n\n### LEFT OUTER JOIN, RIGHT OUTER JOIN에 대해서 설명해주세요.\n\n```text\nLEFT OUTER JOIN은 JOIN 시에 왼쪽, 즉 기준이 되는 테이블의 모든 행과, JOIN 조건에 일치하는 행들을 모두 반환합니다.\n\nRIGHT OUTER JOIN은 LEFT와 반대로 오른쪽, 즉 JOIN이 되는 테이블의 모든 행과 JOIN 조건에 일치하는 행들을 모두 반환합니다.\n```\n\n### CROSS JOIN에 대해서도 설명해주세요.\n\n```text\n첫 번째 테이블의 모든 각 행이 두 번째 테이블의 전체 행과 결합하는 방식의 JOIN입니다.\n\n두 테이블의 각 전체 데이터 수를 곱한 크기의 JOIN 결과가 반환됩니다.\n```\n\n### 서브쿼리에 대해서 설명해주세요.\n\n```text\n쿼리문 안에서 실행되는 또 다른 쿼리문입니다.\n\n주로 쿼리문 안에서 추가적인 쿼리문이 필요할 때 사용합니다.\n```\n\n### DROP, TRUNCATE, DELETE에 각각에 대해 설명해주세요. 어떤차이가 있나요?\n\n```text\nDROP은 테이블 자체를 삭제하는 명령입니다.\nTRUNCATE는 테이블은 그대로 두고 테이블 안에 있는 모든 데이터를 삭제하는 명령입니다.\nDELETE는 테이블 안에 있는 특정 행을 삭제하는 데 사용합니다.\n\nDROP, TRUNCATE는 롤백을 할 수 없지만 DELETE는 롤백이 가능합니다.\nDROP, TRUNCATE는 빠른 속도로 실행되지만 DELETE는 조건을 비교하는 과정이 필요하기에 시간이 오래 걸릴 수 있습니다.\n```\n\n### DISTINCT에 대해서 설명해주세요. 사용해본 경험도 설명해주세요.\n\n```text\n쿼리 결과에서 특정 칼럼에 해당하는 데이터의 중복을 제거한 결과를 반환하는 명령어입니다.\n\n셀럽잇 프로젝트에서 음식점 테이블과 비디오 테이블을 음식점 ID로 JOIN하는 과정이 존재했습니다.\n이때 음식점 테이블과 비디오 테이블은 1:다 관계였기에 JOIN 결과로 같은 음식점이 2개 이상 존재하는 상황이 발생했습니다.\n따라서 이때 생기는 중복을 제거하기 위해 사용한 경험이 있습니다.\n```\n\n### SQL Injection 공격이 무엇인지 어떻게 공격을 예방할 수 있는지 설명해 주세요.\n\n```text\n악의적인 사용자가 입력 필드를 통해 서버에 SQL 쿼리를 주입하여 DB에 대한 공격을 시도하는 행위입니다.\n\n이를 방지하기 위해 입력값을 직접 쿼리문에 넣지 않고, 쿼리를 파라미터화하여 공격을 예방할 수 있습니다.\n```\n\n### 알고 있는 SQL 안티패턴이 있다면 설명해주세요.\n\n```text\n'SELECT *' 을 사용하는 것입니다.\n\n이는 불필요한 열의 데이터까지 모두 가져오기에 불필요한 오버헤드가 발생할 수 있습니다.\n실제로 셀럽잇 프로젝트에서 음식점 테이블과 행정구역 테이블을 통해 공간 인덱스를 활용하려는 시도에서,\n실수로 SELECT * 을 사용하여 항상 테이블 풀 스캔의 결과가 발생했고\n이를 뒤늦게 발견하여 의도했던 결과가 나오지 않았던 것을 알게 된 경험이 있습니다.\n\n추가로 비효율적인 JOIN, 서브쿼리 남용 등이 있습니다.\n```\n\n### 페이지네이션을 구현한다고 했을때 쿼리를 어떻게 작성해야할까요?\n\n```text\nMySQL을 기준으로 LIMIT과 OFFSET을 통해 구현할 수 있습니다.\n\nOFFSET을 통해 쿼리 결과에서 n번 째부터 LIMIT의 수만큼의 데이터만을 가져옴으로써 페이지네이션을 구현할 수 있습니다.\n```"},{"excerpt":"단위 테스트란? 애플리케이션에서 동작하는 기능 또는 메서드를 실행시키는 독립적인 단위의 테스트이다. 이때 기능이라는 말이 포함되어있다. 즉, 단위 테스트라고 해서 무조건 메서드만 테스트하는 건 아니다. 왜 단위 테스트를 하나? 잘 작성한 단위 테스트는 개발자가 작성한 단위(기능 또는 메서드)를 매우 빠르게 검증할 수 있다. 단위 테스트를 하지 않았을 때의…","fields":{"slug":"/unit-test/"},"frontmatter":{"date":"November 27, 2023","title":"단위 테스트 이해하기","tags":["unit-test","test"]},"rawMarkdownBody":"\n## 단위 테스트란?\n\n애플리케이션에서 동작하는 **기능 또는 메서드를 실행**시키는 **독립적인 단위의 테스트**이다.\n\n이때 기능이라는 말이 포함되어있다. 즉, 단위 테스트라고 해서 무조건 메서드만 테스트하는 건 아니다.\n\n## 왜 단위 테스트를 하나?\n\n잘 작성한 단위 테스트는 **개발자가 작성한 단위(기능 또는 메서드)를 매우 빠르게 검증**할 수 있다.\n\n### 단위 테스트를 하지 않았을 때의 문제점\n\n1. 테스트 코드를 프로덕션 코드에 작성해야한다.<br>\n   이는 배포하는 jar 파일의 크기에 영향을 줄 것이고 배포 시간에도 영향을 줄 것이다.\n\n2. 기능을 개발할 때마다 직접 실행시켜 동작을 확인해야한다.<br>\n   이는 작성한 기능에 대해 검증받는 시간의 주기가 길어질 것이다.<br>\n   즉, 버그를 낳을 확률이 증가한다.\n\n이 외에도 문제점이 다양하게 있겠지만, 이만하면 단위 테스트를 할 가치가 충분히 있어보인다.\n\n## JUnit\n\n이렇게 좋은 단위테스트를 어떻게 할까?<br>\n바로 [JUnit](https://junit.org/junit5/) 프레임워크를 사용한다.\n\n> 우테코 5기 최고 미남 [제리의 프레임워크 vs 라이브러리 vs API 테코톡](https://youtu.be/yKEwNVbAFC0?feature=shared)을 보면 프레임워크에 대해 알 수 있다.\n\nJUnit 홈페이지에 있는 소개글에 의하면 \"JVM 기반이며 Java 8 이상에 초점을 맞춰 다양한 스타일의 테스트를 지원한다\"고 한다.\n\n자세한 사용법은 [User Guide](https://junit.org/junit5/docs/current/user-guide/)를 참고하자.\n\n## 특징\n### 1. 메서드 실행 순서\n\n[공식 문서](https://junit.org/junit5/docs/current/user-guide/#writing-tests-test-execution-order)에 의하면 테스트 실행 순서를 이렇게 설명한다.\n\n\"_기본적으로, 테스트 클래스와 메서드는 결정론적이지만 의도적으로 명확하지 않은 순서로 정렬이 된다._\"\n\n무슨 소리일까? 🤔\n\n결정론적 알고리즘에 대해 검색해보니 아래와 같이 설명한다.\n\n\"_예측한 그대로 동작하는 알고리즘이다. **어떤 특정한 입력이 들어오면 언제나 똑같은 과정을 거쳐서 언제나 똑같은 결과를 내놓는다**.\"_\n\n즉, 순서에 대한 보장은 할 수 없지만 입력과 출력은 매번 동일한 것이 보장된다. 라고 이해하면 될 것 같다.\n\n> 왜 이렇게 구현했을까 고민했을 땐, 테스트 순서로부터 독립성을 보장하기 위함이지 않을까한다.<br>\n> 좋은 단위 테스트는 독립적이어야한다. 테스트 순서에도 영향을 받아서는 안된다.\n\n```java\nclass TestOrderTest {\n\n    private static final AtomicInteger number = new AtomicInteger(1);\n\n    @Test\n    void C() {\n        System.out.println(\"Test C - \" + number.getAndIncrement());\n    }\n\n    @Test\n    void B() {\n        System.out.println(\"Test B - \" + number.getAndIncrement());\n    }\n\n    @Test\n    void A() {\n        System.out.println(\"Test A - \" + number.getAndIncrement());\n    }\n\n    @Test\n    void E() {\n        System.out.println(\"Test E - \" + number.getAndIncrement());\n    }\n\n    @Test\n    void D() {\n        System.out.println(\"Test D - \" + number.getAndIncrement());\n    }\n}\n```\n\n실제 위 예제 코드로 순서를 바꿔도 보고 그룹 단위로 변경해가며 실행하면 위에서 설명하는 특징을 실제로 보이는 것을 확인할 수 있다.\n\n> 만약 클래스에 작성한 테스트 코드의 순서대로 동작하고 싶다면 `@TestInstance(value = TestInstance.Lifecycle.PER_CLASS)`를 추가하자<br>\n> 혹은 `@TestMethodOrder(OrderAnnotation.class)`를 클래스 레벨에 추가하고, 각 메서드에 `@Order(int value)`로 조절하자\n\n> 클래스 간의 테스팅 순서도 조절할 수 있다고 한다. 이는 필요할 때 알아보자.\n\n### 2. AssertJ 활용\n\n[AssertJ](http://joel-costigliola.github.io/assertj/assertj-core-quick-start.html)는 \"_능수능란한 검증문 자바 라이브러리_\" 라고 설명한다.\n\n단순히 말해서 유용한 Assertion문을 선언할 수 있도록 도와주는 라이브러리라고 생각하자.\n\nAssertJ는 메서드 체이닝 가능한 assert문을 지원하는 라이브러리이다.<br>\n이를 이용해서 더욱 가독성 좋은 단위 테스트를 작성할 수 있다.<br>\n아래 코드로 비교해보자.\n\n```java\nimport static org.junit.jupiter.api.Assertions.assertEquals;\nimport static org.assertj.core.api.Assertions.assertThat;\n\n@Test\nvoid assertJTest() {\n    // junit 메서드 사용\n    assertEquals(expected, actual);\n\n    // assertJ 메서드 사용\n    assertThat(actual).isEqualTo(expected);\n}\n```\n\n> 자세한 사용법은 [Java Doc](http://javadoc.io/doc/org.assertj/assertj-core) 참고\n\n추가적인 장점으로는 실패 메시지가 자세하다는 점과 다양한 검증 메서드를 지원한다고 한다.<br>\n심지어 [JUnit 공식 사이트](https://junit.org/junit5/docs/current/user-guide/#writing-tests-assertions-third-party)에서도 써드파티 Assertion 라이브러리로 AssertJ를 권장한다.\n\n## Junit vs AssertJ\n\nJunit은 프레임워크고 AssertJ는 라이브러리이다.\n\nAssertJ를 이용해 단위 테스트 코드를 작성하고, JUnit에게 `@Test`와 같은 어노테이션으로 메서드를 위임하여 실행시킨다. "},{"excerpt":"📝 질문 리스트 기아 상태가 무엇인가요? CPU에 의해 처리가 된 준비가 된 프로세스가, 다른 프로세스의 우선 순위에 밀려 오랜 시간 대기하는 상태입니다. 이는, 프로세스 스케줄링에 의해 발생할 수 있습니다. 보통 데드락과 헷갈릴 수 있는데, 데드락은 여러 프로세스가 동시에 자원들을 점유하려는 과정에서 모든 프로세스의 상태가 blocked가 되는 현상입니…","fields":{"slug":"/os-interview-study-3week/"},"frontmatter":{"date":"November 23, 2023","title":"[JSCODE] - OS 면접 스터디 3주차","tags":["operating-system","interview"]},"rawMarkdownBody":"\n## 📝 질문 리스트\n\n### 기아 상태가 무엇인가요?\n\nCPU에 의해 처리가 된 준비가 된 프로세스가, 다른 프로세스의 우선 순위에 밀려 오랜 시간 대기하는 상태입니다.\n\n이는, 프로세스 스케줄링에 의해 발생할 수 있습니다.\n\n보통 데드락과 헷갈릴 수 있는데, 데드락은 여러 프로세스가 동시에 자원들을 점유하려는 과정에서 모든 프로세스의 상태가 blocked가 되는 현상입니다. 기아 상태는 프로세스의 상태는 ready인데, 프로세스 스케줄링에 의해 계속해서 대기하는 상태를 의미합니다.\n\n### 기아 상태를 어떻게 해결할 수 있나요?\n\nRR처럼 기아 상태가 발생하지 않는 스케줄링 알고리즘을 선택하거나,\n\n레디 큐에 대기하는 프로세스들이 처리가 되지 않을 때마다 우선 순위를 증가하는 방식으로 하면, 해결할 수 있다고 생각한다.\n\n### CPU 스케줄링에 대해 설명해주세요.\n\nCPU 효율을 극한으로 끌어올리기 위한 일종의 알고리즘\n\n다양한 알고리즘이 있고 FCFS, Priority, Round-Robin 등이 있다.\n\n각 알고리즘에 따라 장단점이 다르고, 발생할 수 있는 문제점이 다르다\n\n### 스케줄러의 종류는 무엇이 있나요?\n\n장기 스케줄러: 새로운 프로세서가 시스템에 들어올 때, 이를 메모리에 할당하거나 대기 큐에 넣는 역할\n\n단기 스케줄러: 현재 메모리에 적재된 프로세스 중 어떤 프로세스가 CPU에 할당될 지 결정하는 역할.\n여기서 주로 CPU 스케줄링 알고리즘이 수행된다.\n\n중기 스케줄러: 프로세스를 메모리에서 디스크로 스왑하는 역할.\n메모리 부족 상태를 완화하거나, 프로세스의 우선순위를 조절하는 목적\n\n### 선점형 스케줄링과 비선점형 스케줄링의 차이가 무엇인가요?\n\n비선점형 스케줄링: CPU에 의해 처리되는 프로세스가 자발적으로 CPU를 반환하기 전까지는 아무런 개입을 하지 않는 방식.\n직관적이고 단순한 방법이지만, 이 경우 처리시간이 긴 프로세스가 CPU에 할당되면 다른 프로세스는 처리하지 못할 수 있다.\n\n선점형 스케줄링: 비선점형 방식을 적용하지만, 동시에 모종의 이유로 프로세스가 자발적으로 CPU를 반환하지 않았음에도 개입하는 방식.\n이 경우 처리되던 프로세스를 Ready Queue로 옮기는 작업과 다음 처리할 프로세스를 결정하고 할당하는 작업이 필요하다. 하지만, 높은 효율을 가진다.\n\n### 선입선출 스케줄링(FCFS)에 대해 설명해주세요.\n\nReady 큐에 들어간 순서대로 처리하는 방식이다.\n\n단순한 방법이지만, 만약 가장 먼저 들어간 프로세스의 처리 시간이 매우 긴 경우 뒤에 들어간 프로세스에 영향을 받는다.\n\n### 최단 작업 우선 스케줄링(SJF)에 대해 설명해주세요.\n\n프로세스가 CPU를 점유하는 시간이 가장 짧은 프로세스부터 처리하는 방식이다.\n이 경우 프로세스를 선택할 때마다 가장 작은 값을 선택한다. 이 말은 즉, 계속해서 시간이 짧은 프로세스가 레디 큐에 들어오면 영구히 처리되지 못하는 프로세스가 발생한다. 이를 기아 상태라고 한다.\n\n### 최소 잔류 시간 우선 스케줄링(SRTF) 방식에 대해 설명해주세요.\n\n최단 작업 우선 스케줄링과 선점형 스케줄링이 동시에 적용됐을 때, 레디 큐에 존재하는 프로세스중, 처리 시간이 가장 짧은 프로세스를 우선적으로 처리한다.\n\n이도 마찬가지로 기아 상태가 발생할 수 있다.\n\n### 우선순위 스케줄링에 대해 설명해주세요.\n\n각 프로세스에 우선순위를 매기고, 그 우선순위가 가장 높은 프로세스부터 처리하는 방식이다.\n\n기아 상태가 발생할 수 있다.\n\n### 라운드 로빈 스케줄링에 대해 설명해주세요.\n\n가장 간단하며 널리 사용되는 방식으로, 모든 프로세스는 동일한 Quantam을 가지고 이 시간동안만 CPU를 사용하는 방식이다.\n\n자연스레 모든 프로세스나 쓰레드가 공정하게 실행되며, 이 Qunatam을 적절한 시간으로 설정한다면 가장 높은 응답성을 보일 수 있다.\n\n### 멀티 레벨 큐 스케줄링에 대해 설명해주세요.\n\n프로세스들을 특징에 따라 그룹화하고, 그 그룹에 따라 레디 큐를 나누는 방식이다. 이 큐들에서도 우선순위가 존재하며, 각 그룹의 레디 큐에서는 서로 다른 방식으로 스케줄링된다.\n\n다양한 프로세스들이 서로 다른 특성을 가질 때, 각 프로세스에 적합한 스케줄링 방식을 적용할 수 있도록 할 수 있다.\n\n그룹 사이에서 우선순위가 낮은 큐에 있는 프로세스가 기아 상태에 빠질 수 있다.\n\n### 멀티 레벨 피드백 큐 스케줄링에 대해 설명해주세요.\n\n멀티 레벨 큐는 프로세스가 특정 큐에 들어가게 되면, 해당 큐의 우선순위에서 변경되지 못하는 점을 개선하기 위해 탄생한 방식이다.\n\n그래서 동적으로 프로세스의 우선순위를 결정한다.\n\n프로세스가 큐 간에 이동을 할 수 있고, 그에 따라 기아 상태도 해결할 수 있다.\n\n낮은 우선순위에 있는 큐에 있는 프로세스가 잦은 빈도로 실행되면 높은 우선순위 큐로 이동한다.\n\n## 🤔 개인적인 궁금증\n\n### 데드락 vs 기아상태\n\n데드락은, 여러 개의 프로세스가 필요한 자원을 요청하는 과정에서 더이상 진행될 수 없는 상태이다. 자원을 대기하는 blocked 상태에서 영구히 대기한다.\n\n반면 기아상태는, 한 프로세스가 처리될 상태이지만 다른 프로세스의 우선순위에 밀려 영구히 대기하는 상태이다.\n\n## 🎯 피드백\n\n개선할 점 찾기 힘들었다."},{"excerpt":"📝 질문 리스트 파일 시스템과 데이터베이스의 차이점에 대해서 설명해주세요. 파일 시스템은 물리적 액세스만 관리하고, 데이터베이스는 물리적 액세스와 논리적 액세스 모두 관리합니다. 둘 다 결국엔, 디스크에 물리적으로 저장된다. 특히 파일 시스템의 경우 디스크에 저장된 파일을 직접 다루는 시스템이다. 데이터베이스는 이 디스크에 저장된 파일과, 사용자 사이에서…","fields":{"slug":"/db-interview-study-1week/"},"frontmatter":{"date":"November 21, 2023","title":"DB 면접 스터디 1주차","tags":["database","interview"]},"rawMarkdownBody":"\n## 📝 질문 리스트\n\n### 파일 시스템과 데이터베이스의 차이점에 대해서 설명해주세요.\n\n파일 시스템은 물리적 액세스만 관리하고, 데이터베이스는 물리적 액세스와 논리적 액세스 모두 관리합니다.\n\n둘 다 결국엔, 디스크에 물리적으로 저장된다.\n\n특히 파일 시스템의 경우 디스크에 저장된 파일을 직접 다루는 시스템이다.\n\n데이터베이스는 이 디스크에 저장된 파일과, 사용자 사이에서 데이터를 논리적인 구조로 관리하는 시스템\n\n### 데이터베이스의 특징에 대해 설명해주세요.\n\n실시간 사용성, 지속적인 변화, 동시에 공유, 값 참조가 있습니다.\n\n### DBMS는 뭘까요? 특징에 대해 설명해주세요.\n\n데이터베이스를 관리하는 시스템으로, 소프트웨어\n\nMySQL, Oracle, Redis 등이 있다.\n\n특징으로는, 데이터 일관성 유지, 복구, 동시 요청 제어 등이 있다.\n\n### 스키마가 뭘까요? 3단계 데이터베이스 구조에 대해 설명해주세요.\n\n스키마란, 데이터베이스에 저장된 데이터 구조를 정의하는 개념입니다.\n\n이 스키마는 데이터베이스 구조에서 외부 스키마, 개념 스키마, 내부 스키마로 나뉩니다.\n\n외부 스키마는 사용자가 보는 스키마입니다. 흔히 테이블이나 뷰를 의미한다고 보면 될 것 같습니다.\n\n개념 스키마는 전체 데이터베이스의 정의를 의미합니다. 쉽게 말해 전체 테이블을 의미한다고 할 수 있습니다.\n\n내부 스키마는 디스크에 실제 데이터가 저장되는 방법의 표현입니다. 인덱스, 자료형이 포함된다고 할 수 있습니다.\n\n### 데이터 독립성에 대해서 설명해주세요. ← 이거 잘 이해가 안감\n\nDB의 구조를 변경해도, 사용자에게 영향이 가지 않도록 하는 개념입니다.\n\n두 가지의 데이터 독립성이 존재합니다.\n\n논리적 데이터 독립성입니다. 이는, 논리적인 구조나 스키마를 변경해도 사용자는 동일하게 DML을 사용할 수 있어야 한다는 점입니다. 쉽게 말해 테이블 구조가 변경이 되어도 사용자는 논리적으로 동일한\n\n물리적 데이터 독립성은, 물리적인 구조가 변경되어도 사용자에게는 영향을 주지 않습니다.\n\n### RDBMS(관계형 데이터베이스 관리시스템)는 뭘까요?\n\n데이터를 테이블 간의 정의된 관계를 통해 데이터를 조회하고 쓸 수 있는 시스템입니다.\n\n### 릴레이션 스키마와 릴레이션 인스턴스에 대해서 설명해주세요.\n\n릴레이션 스키마는, 쉽게 말해 테이블의 제약 조건입니다. 테이블의 이름, 각 칼럼의 이름과 자료형을 의미합니다. 자바로 비유하자면 클래스\n\n릴레이션 인스턴스는, 자바로 비유하자면 인스턴스가 될 것 입니다. 즉, 스키마가 생성되고 어느 특정 시점에 테이블에 저장되어 있는 상태가 바로 릴레이션 인스턴스이다.\n\n정적이냐 동적이냐의 관점으로 봐도 될 것 같다.\n\n### 릴레이션의 차수와 카니덜리티에 대해 설명해주세요.\n\n차수: 특정 테이블의 칼럼 수\n\n카디널리티: 특정 칼럼에 해당하는 행들의 중복도입니다. 중복도가 높으면 카디널리티는 낮다라고 표현을 하고, 반대로 중복도가 낮으면 카디널리티는 높다고 합니다.\n\n### 키(Key)에 대해서 설명해주세요. (슈퍼키, 후보키, 기본키, 대리키, 외래키)\n\n슈퍼키: 테이블 내에서 고유성을 보장하는 데 사용되는 하나 이상의 칼럼의 집합\n\n후보키: 슈퍼키의 성질을 포함하면서 동시에 최소성을 만족하는 키\n\n기본키: ID 필드에 속하는 값들. 각 행을 구분짓는 값. Not Null이며 값이 중복되서는 안된다.\n\n대리키: 사용자가 정의한 실제 데이터와는 상관없이 데이터베이스가 자동으로 생성하는 키를 의미. 주로 기본키로 사용하며, 데이터의 변경이나 실제 값의 변경에 대한 영향을 최소화한다.\n\n외래키: 하나의 특정 테이블에서 다른 테이블과 관계를 맺기 위한 키. 주로 외래키는 관계를 맺어지는 테이블의 기본키이다.\n\n### 무결성 제약조건에 대해서 설명해주세요. (도메인 무결성, 개체 무결성, 참조 무결성)\n\n데이터 무결성: DB에 저장된 데이터가 결함이 없는지? 이 결함은, 일관성과 정확성\n\n도메인 무결성(도메인 제약): 칼럼의 속성에 해당하는 값만 올 수 있다.\n\n개체 무결성(기본키 제약): 테이블은 기본키를 설정해야 하고, 기본키는 Not Null, Unique 해야 한다.\n\n참조 무결성(외래키 제약): 테이블 간의 참조 관계를 선언하는 제약 조건. 자식 테이블의 외래키는 부모 테이블의 기본키와 도메인이 동일해야 하며, 자식 릴레이션의 값이 변경될 때 부모 릴레이션의 제약을 받는다.\n\n### 사용했던 데이터베이스에 대해서 설명해주세요. (오라클DB, MySQL, MariaDB, MongoDB 등)\n"},{"excerpt":"📝 질문 리스트 프로그램에 대해 설명해주세요. 프로그램은 개발자가 프로그래밍 언어로 작성한 명령어들의 집합입니다. 프로그램의 특징으로는 디스크에 실행 파일 형식으로 저장되어 있다가, 사용자가 실행을 하면 운영체제에 의해 해당 파일을 메모리에 적재됩니다. 이를 프로세스라고 합니다. 프로세스에 대해 설명해주세요. 프로그램의 인스턴스입니다. 쉽게 말해, 디스크…","fields":{"slug":"/os-interview-study-2week/"},"frontmatter":{"date":"November 16, 2023","title":"[JSCODE] - OS 면접 스터디 2주차","tags":["operating-system","interview"]},"rawMarkdownBody":"\n## 📝 질문 리스트\n\n### 프로그램에 대해 설명해주세요.\n\n프로그램은 개발자가 프로그래밍 언어로 작성한 명령어들의 집합입니다.\n\n프로그램의 특징으로는 디스크에 실행 파일 형식으로 저장되어 있다가, 사용자가 실행을 하면 운영체제에 의해 해당 파일을 메모리에 적재됩니다.\n\n이를 프로세스라고 합니다.\n\n### 프로세스에 대해 설명해주세요.\n\n프로그램의 인스턴스입니다.\n\n쉽게 말해, 디스크에 저장되어 있던 프로그램이 사용자에 의해 실행되면, 운영체제가 프로그램의 일부 명령어를 메모리에 적재를 하게 되고, 적재된 명령어들이 CPU에 의해 처리가 됩니다.\n\n이렇게 해서 사용자가 실행한 프로그램이 프로세스로서 실행이 됩니다.\n\n### 프로세스의 메모리 공간에 대해 설명해주세요.\n\n프로세스의 메모리 공간은 크게 4가지로 나뉩니다.\n\n**텍스트(또는 코드) 영역**, **데이터 영역**, **힙 영역**, **스택 영역**으로 나뉩니다.\n\n텍스트 영역은 프로그램의 명령어들이 저장되는 영역입니다. CPU는 이 부분의 명령어를 읽어 처리합니다.\n\n데이터 영역은 주로 전역 변수나 정적 변수들이 저장되는 영역입니다.\n\n스택 영역은 지역 변수나 함수 호출과 관련된 데이터가 저장되는 영역입니다.\n\n힙 영역은 프로세스가 실행 중에 동적으로 메모리를 할당하고 해제하게 되는데, 그때 사용하게 되는 영역입니다.\n자바 기준으로 말씀드리자면 객체의 인스턴스가 힙 영역에 저장된다고 할 수 있습니다.\n\n### 쓰레드에 대해 설명해주세요.\n\n프로세스의 실행 단위인 동시에, CPU가 처리할 수 있는 하나의 작업 단위입니다.\n\n실행 중인 프로세스는 꼭 1개 이상의 쓰레드를 가집니다.\n\n한 프로세스에 여러 쓰레드가 존재할 수 있으며, 이 쓰레드들은 같은 프로세스의 자원을 공유해서 사용할 수 있습니다.\n\n쓰레드가 많을수록 처리하는 작업이 많아져서 효율이 증가한다고 생각할 수 있는데요.\n사실, 이 쓰레드 간의 컨텍스트 스위칭이 빈번하게 일어나면 그 과정에서 오버헤드가 발생합니다.\n따라서 많다고 해서 좋은 건 아닙니다.\n\n### 프로세스와 쓰레드가 어떤 차이가 있는지 예시를 들어 설명해주세요.\n\n프로세스가 식당이라고 한다면, 쓰레드는 요리사 또는 서버 또는 계산원이 될 수 있을 것 같습니다.\n\n한 식당에서 고객의 요청에 의해 수행되어야 하는 작업이 주문받기, 요리, 음식 제공, 계산의 순이 될 것 같은데요. 이러한 하나의 작업 단위를 수행하는 사람이 분명히 존재해야 합니다.\n따라서 이 하나의 작업 단위를 수행하는 사람을 쓰레드로 비유를 들 수 있을 것 같습니다.\n\n### 쓰레드의 메모리 공간(스택, 데이터, 코드)에 대해 설명해주세요.\n\n쓰레드는 각기 다른 작업 영역을 가집니다. 이 작업 영역은 스택 영역에 해당합니다.\n이외에 나머지인, 코드, 데이터, 힙 영역에 경우는 같은 프로세스 내에서 함께 사용하게 됩니다.\n\n이로 인해 발생할 수 있는 문제점이 있습니다. 바로, 동시성 문제인데요.\n특히 데이터 영역의 경우 정적 변수들이 저장되어 있는데, 이 변수가 가변성을 띈다면 동시성 문제가 충분히 발생할 수 있습니다.\n\n따라서 멀티 쓰레드 환경이라면 동시성 문제를 항상 고려해야 합니다.\n\n### 프로세스 제어블록(PCB)에 대해 설명해주세요.\n\n특정 프로세스에 대해 중요한 정보를 담고 있는 자료구조입니다.\n운영체제가 프로세스를 관리하기 위해 프로세스 생성과 동시에 고유한 PCB를 생성합니다.\n\n프로세스가 CPU에 할당되어 처리되는 도중에, CPU의 점유가 다른 프로세스로 넘어가게 된다면, 실행 과정에 대한 정보를 어딘가에 저장하고 있어야 합니다. 이때 그 어딘가가 PCB에 해당합니다.\n\n- PCB에는 어떤 정보가 담겨있을까요?\n    - 프로세스 ID\n    - 프로세스의 상태: new, ready, running, waiting, terminated\n    - 프로그램 카운터: 프로세스가 다음에 실행할 명령어의 주소\n    - CPU 레지스터\n    - CPU 스케줄링 정보: 프로세스 우선순위, 스케줄 큐에 대한 포인터 등\n    - 메모리 관리 정보: 페이지 테이블 또는 세그먼트 테이블 등과 같은 정보를 포함\n    - 입출력 상태 정보: 프로세스에 할당된 입출력 장치들과 열린 파일 목록\n    - 어카운팅 정보 : 사용된 CPU 시간, 시간제한, 계정번호 등\n\n### 쓰레드 제어블록(TCB)에 대해 설명해주세요.\n\n쓰레드에 대한 정보를 저장하는 자료구조입니다.\n\n쓰레드가 생성될 때, 운영체제는 각 쓰레드에 대한 TCB를 생성하고 업데이트하여 쓰레드의 상태를 추적합니다.\n\n특정 쓰레드가 CPU를 점유하여 처리되는 도중에, 다른 쓰레드로 점유가 넘어가게 되면, 기존 처리되던 쓰레드의 상태를 저장해야 합니다. 이때 저장하는 공간이 TCB입니다.\n\n- TCB에 포함되는 정보\n- 쓰레드 식별자\n- 쓰레드 상태\n- 프로그램 카운터\n- 스택 포인터\n- 레지스터 상태\n- 쓰레드 우선순위\n- 쓰레드의 할당된 자원 정보\n- 쓰레드의 스케줄링 정보\n\n### 멀티 프로세스에 대해서 설명해주세요.\n\n하나의 컴퓨터에서 여러 개의 독립적인 프로세스가 실행되는 것을 말합니다.\n\n이때, 각 프로세스는 메모리에 각각의 독립적인 메모리 영역을 할당받게 됩니다.\n\n프로세스가 많을수록 처리되는 프로그램의 수가 많아져서 좋은 게 아닐까하는 생각이 들 수 있지만, 쓰레드가 많을수록 좋은 게 아닌 것처럼 실행 결과가 좋지 않을 수 있습니다.\n\n### 멀티 쓰레딩에 대해서 설명해주세요.\n\n하나의 프로세스 내에 여러 개의 쓰레드가 동시에 실행되는 것을 의미합니다.\n\n적절한 수의 쓰레드는 여러 작업을 이상없이 동시에 처리할 수 있지만,\n너무 많은 수의 쓰레드는 컨텍스트 스위칭으로 인해 오히려 성능 저하가 발생할 수 있습니다.\n\n### 프로세스 수행 상태 변화 과정에 대해 설명해주세요.\n\n처음 프로세스가 메모리에 적재되기 전의 상태는 new입니다.\n\n해당 프로세스가 메모리에 적재가 되고 실행대기 큐에 적재가 된 상태는 ready입니다.\n\n해당 프로세스가 CPU에 할당되어 작업이 수행되면 running 상태입니다.\n\n만약 해당 프로세스가 IO 작업을 요청하게 되면, 해당 IO 작업이 완료되기 전까지 대기하는 상태가 block 입니다.\n\n해당 프로세스가 종료될 때, terminated 상태를 거쳐 완전히 종료하게 됩니다.\n\n### 문맥교환(context switch)에 대해 설명해주세요.\n\nA 쓰레드와 B 쓰레드가 동시에 처리될 때, CPU가 A를 처리하다가 B를 처리하게 될 때 발생하는 개념이 컨텍스트 스위칭입니다.\n\n- 문맥교환은 언제 발생하나요?\n\n일반적으로 한 쓰레드의 타임 슬라이스가 끝난 경우 발생합니다.\n\n또는 타이머 인터럽트나 IO 요청 시스템 콜이 들어올 때 발생합니다.\n\n- 문맥 교환 발생 과정에 대해서 조금 더 상세히 설명해주세요.\n\n현재 처리하는 CPU가 쓰레드 A에서 B로 컨텍스트 스위칭이 일어난다고 가정해보겠습니다.\n\n이때, 쓰레드 A의 진행 상태를 PCB 내지 TCB에 저장합니다.\n그리고 운영체제는 쓰레드 B의 진행 상태를 PCB 또는 TCB에서 불러오고, 이를 CPU에게 할당합니다.\n\n### fork() 명령어에 대해 설명해주세요.\n\n유닉스에서의 fork 명령어는 새로운 프로세스를 생성하도록 하는 시스템 콜입니다.\n\nfork 시스템콜을 통해 부모 프로세스의 내용을 그대로 복제한 자식 프로세스를 생성할 수 있습니다.\n\n이때 부모 프로세스와 자식 프로세스는 독립적으로 수행합니다.\n\n### 프로세스끼리 협력하는 방법에 대해서 설명해주세요.\n\n1. 메시지 전달 방식\n   프로세스는 다른 프로세스에게 전달할 메시지를 운영체제에게 시스템 콜 방식으로 요청해 전달한다.\n2. 공유 메모리 방식\n   시스템 콜 방식을 통해 각 프로세스 간 공유하는 공간을 만든다.\n   이 공유 공간을 다른 프로세스에서 접근할 수 있도록 한다.\n\n## 🤔 개인적인 궁금증\n\n### 스택 메모리 영역 관련\n\n자바의 정석에 따르면, “각 메서드를 위한 메모리상의 작업공간은 서로 구별된다”고 합니다.\n동시에 쓰레드에 대해 학습했을 때, 각 쓰레드는 각기 다른 스택 영역을 가진다고 학습했습니다.\n\n둘 다 결국, 메모리 관점에서 스택 영역에 각기 다른 작업 영역을 할당한다고 이해하면 될까요?\n\n### 컨텍스트 스위칭 설명에 주로 프로세스 간의 교환이 설명되어 있는데, 쓰레드 간의 교환으로 나와있지 않은 이유? 그리고 사실상 쓰레드 간의 교환이 일어날텐데? 그리고 같은 프로세스의 쓰레드 간의 교환이 일어나면 PCB 말고 TCB에 저장하나?\n\n## 🎯 피드백\n\n- 저번 주랑 많이 달라졌다. **두괄식이 좋았다.**\n- 예시도 구체적이고 자세해서 굿\n- 소통도 많이 늘었다.\n- 저번 주에 비해 다 좋았다.\n- 래비덴큐\n- 멀티 쓰레드 적용 → 대용량 데이터, 배치\n  이때 멀티 쓰레드의 장점을 많이 느낀다.\n- 시선 처리"},{"excerpt":"📝 질문 리스트 운영체제는 무엇이고 어떤 역할을 수행하는가? 운영체제는 컴퓨팅 시스템을 운영하는 시스템이다. 약간 말장난 같지만, 실제로 운영체제는 사용자와 하드웨어 사이에서 자원을 어떻게 사용할 지를 결정한다. 대표적인 역할로는 프로세스 스케줄링, 프로세스 동기화 방식 등이 있다. 시분할 시스템 하나의 프로세서가 2개 이상의 프로세스의 작업을 처리해야할…","fields":{"slug":"/os-interview-study-1week/"},"frontmatter":{"date":"November 09, 2023","title":"[JSCODE] - OS 면접 스터디 1주차","tags":["operating-system","interview"]},"rawMarkdownBody":"\n## 📝 질문 리스트\n\n### 운영체제는 무엇이고 어떤 역할을 수행하는가?\n\n운영체제는 컴퓨팅 시스템을 운영하는 시스템이다.\n\n약간 말장난 같지만, 실제로 운영체제는 사용자와 하드웨어 사이에서 자원을 어떻게 사용할 지를 결정한다.\n\n대표적인 역할로는 프로세스 스케줄링, 프로세스 동기화 방식 등이 있다.\n\n### 시분할 시스템\n\n하나의 프로세서가 2개 이상의 프로세스의 작업을 처리해야할 때, 각각의 프로세스의 작업을 얼마만큼의 시간동안 처리를 할 것인지에 대한 정책이다.\n\n시분할 시스템을 구현하기 위한 다양한 알고리즘이 있지만, 현재 가장 널리 사용되는 알고리즘은 Round-Robin 알고리즘이다.\n\nRound Robin 알고리즘은, 각 프로세스의 CPU 할당 시간을 동일하게 설정하고, 할당 시간이 끝이 나면 인터럽트를 발생시켜 다음 프로세스의 작업을 수행하도록 하는 과정의 알고리즘이다.\n\n보통 할당 시간은 매우 짧은 시간에 속하지만, CPU는 이 시간안에 매우 빠른 속도로 작업을 처리하기 때문에 사용자는 2개 이상의 프로세스가 동시에 실행되는 것처럼 보인다.\n\n### 다중 프로그래밍 시스템(multi-programming system)\n\n2개 이상의 프로그램이, 하나의 메모리에 동시에 적재되는 시스템을 말한다.\n\n이때, 프로그램이 메모리에 적재될 때, 프로그램 전체를 올리진 않는다. \n\n용량이 작은 메모리에 2개 이상의 프로그램 전체를 적재할 수 없기 때문이다. \n\n따라서 당장 필요한 부분만 메모리에 적재한다.\n\n### 대화형 시스템(interactive system)\n\n사용자가 컴퓨터와 1:1로 대화한다는 느낌의 시스템이다.\n\n다른 말로, 사용자가 입력한 결과가 컴퓨터에 즉시 출력된다는 것이다.\n\n실제로는 여러 사용자가 한 대의 컴퓨터를 이용함에도, 1:1처럼 인식한다는 것이다.\n\n이는 다중 프로그래밍 시스템과 시분할 시스템의 적용으로 가능하다.\n\n### 다중 처리기 시스템(multi-processor system)\n\n프로세서가 2개 이상인 시스템이다.\n\n당연하게도 모든 프로세서가 자원을 함께 사용한다.\n\n대칭적 다중 처리 시스템: 단일 운영체제 아래에서 2개 이상의 CPU가 동작하는 시스템이다.\n\n각 CPU끼리는 데이터를 효율적으로 공유하는 구조로 운영된다.\n\n비대칭적 다중 처리 시스템: 1개의 메인 CPU가 시스템을 제어하며, 나머지 CPU들은 미리 정의된 작업을 수행한다.\n\n### 시스템 콜\n\n프로세스가 OS 커널이 제공하는 서비스를 이용하고 싶을 때, 시스템 콜을 이용해 실행한다.\n\n#### 종류\n- 프로세스/스레드 관련\n- 파일 IO 관련\n- 소켓 관련\n- Device 관련 (키보드 등)\n- 프로세스 통신 관련\n\n시스템 콜이 발생하면, 커널은 CPU에게 Interrupt를 발생하고, CPU는 다음 명령을 실행할 때 체크하고, 해당 커널 코드를 실행\n\n### 커널\n\nOS의 core라고 할 수 있다.\n\n항상 메모리에 적재되어있다.\n\nHW와 프로세스 사이의 인터페이스로서, 프로세스가 요청하는 시스템 콜에 대해 직접 수행한다.\n\n왜 인터페이스?\n\n프로세스는 개발된 프로그래밍 언어가 모두 다르고, 시스템 콜을 호출하는 방식이 모두 다르기 때문에 중간에 추상 계층이 필요하기 때문\n\n운영체제와 커널의 차이?\n\n커널은 운영 체제에 포함되는 하나의 모듈의 개념이다.\n\n운영체제는 커널을 포함해, 컴퓨터 시스템을 총괄하는 개념으로 이해하고 있다.\n\n### 유저모드와 커널모드\n\n- 유저모드: 프로세서가 응용 프로그램을 실행하는 모드\n- 커널모드: 프로세서가 커널 코드를 실행하는 모드\n\n유저 모드에서 커널 모드로 전환될 때 인터럽트나 시스템 콜이 발생한다.\n\nCPU의 mode bit 레지스터로 구분한다.\n\n0이면 커널 모드, 1이면 유저 모드\n\n커널모드가 필요한 이유는 누군가 만든 프로그램이 하드웨어를 직접 통제하게 되고, 점유하는 등과 같은 문제로 다른 프로세스에 영향을 미칠 수 있기 때문이다.\n\n즉, 시스템을 보호하기 위해서\n\n### 폴링\n\nCPU가 I/O Device의 작업 내용이나 결과를 직접 확인하는 방식이다.\n\n이 방식은 CPU의 작업이 수행되어야 하기에 프로세스 관점에서는 불필요한 오버헤드이다.\n\n이 비효율적인 방식을 개선하기 위해 인터럽트가 탄생했다.\n\n### 인터럽트\n\n시스템에서 발생한 다양한 종류의 이벤트\n\n#### 종류\n\n- 전원에 문제가 생겼을 때\n- IO 작업이 완료됐을 때\n- 시간이 다 됐을 때 (timer 관련)\n- 프로그램에서 0으로 나눴을 때\n- 프로그램에서 잘못된 메모리 공간에 접근을 시도할 때\n\n- 인터럽트 벡터: 인터럽트의 처리 루틴 주소를 가짐\n- 인터럽트 처리 루틴: 인터럽트를 처리하는 커널 함수\n\n### DMA\n\nCPU는 매우 빠른 속도로 메모리에만 접근한다. 동시에 SSD 같은 비교적 빠른 I/O Device가 존재한다.\n\n이런 상황에서 Device들이 잦은 빈도로 인터럽트를 발생하면, CPU의 성능이 떨어진다.\n\n이를 방지하고자, I/O Device에서 각 buffer storage의 내용을 메모리에 block 단위로 직접 전송하고, 인터럽트를 발생하는 개념이다.\n\n이 DMA는 CPU와 I/O Device 사이에 위치하는 DMA Controller에 의해 수행된다.\n\n### 동기식 I/O과 비동기식 I/O\n\n프로세스에 의해 I/O 요청이 발생하면, 프로세서는 이 요청을 처리한다.\n이때, 동기식 I/O의 경우는 해당 I/O 작업이 완료된 후에야 다음 작업을 처리한다.\n비동기식 I/O의 경우는 I/O 작업을 요청보내고, 바로 다음 작업을 실행한다.\n\n동기식 I/O의 경우, 프로세스의 다음 작업이 디스크에서 읽은 값을 이용한 처리라면 적용할 수 있다.\n하지만, I/O 작업의 경우 시간이 오래 걸리는 작업이기에 CPU의 효율을 매우 떨어트리는 방식이다.\n즉, CPU의 성능을 희생하는 대신 작업의 결과를 보장할 수 있다.\n\n비동기식 I/O의 경우, 프로세스의 다음 작업이 I/O 작업과 관련이 없는 경우 적용할 수 있다.\n하지만 비동기식 I/O 작업의 결과를 즉시에 알 수 없다. 확인하려면 또 다른 요청이 필요할 수 있다.\n그래도 CPU가 쉬지 않고 일할 수 있으므로 CPU의 효율을 끌어올릴 수 있다.\n\n위의 경우는 하나의 프로세스에서의 경우를 가정한 것이다.\n하지만 보통은 다수의 프로세스를 동시에 처리한다.\n이때, I/O 작업을 요청한 프로세스는, 해당 작업이 완료될 때까지 대기시켜놓고, 그 시간동안 CPU는 다른 프로세스의 작업을 처리한다. 즉, 동기식 I/O 처리를 하며 CPU 스케줄링을 통해 CPU 성능과 작업의 정합성을 모두 지킨다.\n\n## 📚 추가 학습\n\n### 인터럽트와 시스템 콜, 그리고 유저 모드와 커널 모드\n\n```text\n유저 모드: 우리가 개발하는 프로그램은 일반적으로 유저 모드에서 실행\n커널 모드: 프로그램 실행 중, 인터럽트가 발생하거나 시스템 콜을 호출하게 되면 커널 모드로 전환\n전환할 때, 현재 실행 중이던 프로그램의 현재 상태를 저장\n그 후, 발생한 인터럽트 혹은 시스템 콜을 직접 처리. 즉, CPU에서 커널 코드가 실행된다.\n모든 처리가 완료되면, 중단됐던 프로그램의 상태를 복원\n다시 통제권을 프로그램에게 반환. 즉, 커널 모드에서 유저 모드로 전환\n프로그램이 이어서 실행\n\n커널: 운영체제의 핵심\n시스템 전반을 관리/감독하는 역할\n하드웨어와 관련된 작업을 직접 수행\n\n커널 모드를 만든 이유: 누군가 만든 프로그램이 하드웨어를 직접 통제하게 되고, 점유하는 등과 같은 문제로 다른 프로세스에 영향을 미칠 수 있다.\n즉, 시스템을 보호하기 위해\n\n인터럽트: 시스템에서 발생하는 이벤트 혹은 그런 이벤트를 알리는 매커니즘\n종류\n - 전원에 문제가 생겼을 때\n - IO 작업이 완료됐을 때\n - 시간이 다 됐을 때 (timer 관련)\n - 프로그램에서 0으로 나눴을 때\n - 프로그램에서 잘못된 메모리 공간에 접근을 시도할 때\n\n인터럽트가 발생하면, CPU에서는 즉시 인터럽트 처리를 위해 커널 코드가 커널 모드에서 실행.\n여기서 ‘즉시’라고 했지만, 실행중이던 프로세스의 처리까지는 마무리한 뒤에 처리를 한다.\n\n시스템 콜: 프로그램이 커널이 제공하는 서비스를 이용하고 싶을 때, 시스템 콜을 이용해 실행한다.\n종류\n- 프로세스/스레드 관련\n- 파일 IO 관련\n- 소켓 관련\n- Device 관련 (키보드 등)\n- 프로세스 통신 관련\n시스템 콜이 발생하면, 해당 시스템 콜에 대응하는 커널 코드가 커널 모드에서 실행된다.\n```\n\n### Physical Memory, Virtual Memory\n\n```text\nVirtual Memory: 각 프로세스만의 논리적인 주소이다.\nPhysical Memory: 실제 메모리에 적재된 주소이다.\n\nVirtual Memory에서, Physical Memory를 가리키기 위해선 중간에 변환해주는 역할이 필요하다. 이는 ATU(Address Translation Unit)에 의해 수행된다.\n```\n\n### I/O Bound, CPU Bound\n\n```text\n쓰레드의 관점에서, 수행할 작업의 시간이 I/O 작업과 CPU에 의해 처리되는 시간의 비중이 얼마나 많은지에 대한 내용이다.\n```\n\n### 인터럽트가 발생하면, CPU에서 어떤 일이 발생할까?\n\n## 🤔 개인적인 궁금증\n\n### ### I/O 작업은 왜 느리다고 하는가?\n\n```text\n보통 I/O 작업이 느리다는 말은 CPU와 비교해서 표현하는 것 같다.\n그럼 왜 CPU에 비해 100만 배 이상 느릴까?\n\n1. I/O 작업 자체가 느리거나 오래 걸리는 작업일 수 있다.\n2. I/O 작업은 버스를 통해 CPU와 통신한다.\n이 과정에서 대기 시간이 발생할 수 있기 때문이다.\n3. 보통 I/O 장치는 인터럽트를 발생시켜 처리가 필요함을 알린다.\nCPU의 인터럽트 마스크가 된 상태에서, 다른 중요한 작업을 하고 있을 수 있어서 바로 처리하지 않을 수 있기 때문이다.\n게다가 인터럽트 작업을 처리하기 위해 CPU는 컨텍스트 스위칭을 한다.\n4. Device의 물리적 거리도 영향이 있을까?\n```\n\n### 컨텍스트 스위칭은 프로세스 간 교환에서만 일어나는가? 쓰레드 간 교환에서는 일어나지 않는가?\n\n```text\n\n둘 다 가능하다.\n하지만 성능 상 차이가 발생한다.\n\n우선 컨텍스트 스위칭이 무엇인가?\nCPU/코어에서 실행중이던 프로세스/쓰레드가 다른 프로세스/쓰레드로 교체되는 것이다.\n\n그럼 왜 성능 차이가 발생하는가?\n쓰레드 컨텍스트 스위칭은 같은 프로세스 내에서의 컨텍스트 스위칭이다.\n이때 프로세스의 가상 메모리 주소를 공유해서 사용한다.\n\n하지만 프로세스 컨텍스트 스위칭은, 이 **가상 메모리 주소 관련 처리**에 대한 **오버헤드**가 발생한다.\n이때 수행되는 처리로는 각 프로세스 PCB에 컨텍스트 저장, CPU 레지스터에 컨텍스트 교체 등이 있다.\n이러한 오버헤드의 차이로 인해 성능 상 차이가 발생한다.\n\n그렇다고 쓰레드 컨텍스트 스위칭이 오버헤드가 없다는 건 아니다.\n상대적으로 더 적은 오버헤드이지, 아예 없는 건 아니다.\n```\n\n### Tomcat maxThreads 기본값은 왜 200이고, HikariCP Max-Connection의 기본값은 왜 10일까?\n\n```text\nTomcat의 maxThreads 값은, 들어온 요청과 1:1로 Connect를 하고, 그 Connect를 처리하기 위해 생성하는 쓰레드의 최대 개수이다.\n즉, 하나의 프로세스 내의 존재하는 쓰레드이고, 이는 쓰레드 컨텍스트 스위칭이 발생한다.\n\nHikariCP는 JDBC Connection Pool이다.\nJDBC Connection은 Java 기반 애플리케이션과 RDMBS와의 Connection이다.\n즉, 프로세스 간의 소켓 통신이 일어나고, 이는 프로세스 컨텍스트 스위칭이 발생한다.\n\n그래서 HikariCP의 Max-Connection Size를 설정할 때, Size가 클수록 처리해야 할 Thread가 많아지고, 이는 곧 컨텍스트 스위칭을 자주 유발하기 때문에 오히려 적은 수의 Thread로 처리하는 것이 오버헤드 비용을 줄여 더 나은 성능을 보이는 것 같다.\n\nHikariCP Max-Connection의 기본값이 10인 이유는, 자체 벤치 마킹을 통해 결정했다.\nhttps://github.com/brettwooldridge/HikariCP#checkered_flag-jmh-benchmarks\n\nTomcat maxThreads의 기본값이 10인 이유는? 모르겠다. 아마 Tomcat에서 자체적인 테스트를 통해 도출된 결과값이 아닐까?\n```\n\n### EC2의 Swap Memory란?**\n\n```text\n우리는 흔히 EC2에서 Linux 게열의 OS를 사용한다.\n따라서 Linux에서의 Swap Memory는, 메인 메모리의 용량보다 더 많은 공간이 필요할 때 디스크의 일부 공간을 사용하는 개념이다.\n메모리 관리 알고리즘(LRU, LFU)에 의해 잘 사용되지 않는 데이터를 디스크에 저장하는 방식이다.\n하지만, 디스크를 사용하는만큼 속도는 느리다.\n\n이때, 디스크에 할당되는 일부 공간을 Swap Memory, 혹은 Swap Space 라고 한다.\n```\n\n## 🎯 피드백\n\n- 목소리, 말투 침착 또박. 듣는 입장에서 잘 들어옴 속도도 빨라지지 않고 일정했다.\n- 눈을 마주치고 답변\n- 꼬리 질문에도 맞든 안맞든 최대한 답변하는 모습\n- 아는 개념에 대해선 핵심 단어를 강조\n- 폴링에 대해 설명할 때, 짧게 대답했다. 이는 꼬리 질문을 유도해보인걸로 보여서 좋았다.\n근데 너무 짧았던 것 같다. 근데 꼬리 질문에서 대답 못하면, 이거밖에 모르나보네. 할 수 있다.\n- 아는건 많아 보인다. 근데, 정리가 되지 않고 내용이 길어진다.\n장황하게 설명하기보다 개념에 대해 짚고, 부가 설명을 하는 게 좋아 보인다.\n- 모르는 질문에 있어서는 시간을 가지고 답변 굿.\n- 말이 길어지는 경향이 있다. 개념을 물어봤을 때, 이런 개념입니다. 하고 부연 설명.\n- 질문에 대해 답변이 한번씩은 많이 짧았다.\n한 마디만 더 했으면 면접관 입장에서 한번 더 물어볼 의향이 들었을텐데.."},{"excerpt":"이번 글에선 셀럽잇이 왜? 그리고 어떻게? 무중단 배포를 적용했는 지에 대해 작성하고자 합니다. 무중단 배포를 왜 하는가? 서버가 1개인 상황에서 새로 배포를 하게 되면, 배포가 완료되는 시간동안 사용자들은 서비스를 이용하지 못합니다. 배포 시간이 3초만 되어도 사용자는 3초 동안 에러 페이지만 보게 될 것입니다. 아래 사진은 구글이 제공하는 데이터입니다…","fields":{"slug":"/zero-downtime-deployment-of-celuveat/"},"frontmatter":{"date":"October 21, 2023","title":"[셀럽잇] 무중단 배포 적용기","tags":["셀럽잇","zero-downtime-deployment"]},"rawMarkdownBody":"\n이번 글에선 셀럽잇이 왜? 그리고 어떻게? 무중단 배포를 적용했는 지에 대해 작성하고자 합니다.\n\n## 무중단 배포를 왜 하는가?\n\n서버가 1개인 상황에서 새로 배포를 하게 되면, 배포가 완료되는 시간동안 사용자들은 서비스를 이용하지 못합니다.\n\n배포 시간이 3초만 되어도 사용자는 3초 동안 에러 페이지만 보게 될 것입니다.\n\n아래 사진은 구글이 제공하는 데이터입니다.\n\n![페이지 로딩 시간에 따른 사용자 이탈률](user-bounce-rate-due-to-page-load-time.png)\n\n무려 3초만 서비스를 이용하지 못하더라도 약 1/3 의 사용자가 이탈합니다.\n\n**따라서 실사용자가 있는 상황에서 배포에 따른 Down-Time을 제거하기 위해 무중단 배포를 적용해야 합니다.**  \n\n## 무중단 배포 방식 결정하기\n\n무중단 배포에는 *롤링 방식*, *블루/그린 방식*, *카나리 방식*이 있습니다.\n\n> ([무중단 배포 방식에 대해 정리한 글](https://kdkdhoho.github.io/zero-downtime-deployment))\n\n이 중, 현재 주어진 리소스를 고려하여 최적의 방식을 선택하려고 합니다. \n\n현재 리소스 상황으로는, 배포 서버로 EC2 인스턴스 1대를 사용중입니다.<br>\n해당 인스턴스에서 리버스 프록시 역할의 Nginx와 스프링 애플리케이션이 존재합니다.<br>\n이러한 상황에서 무중단 배포를 위한 인스턴스를 추가로 사용하기 위해선 기술 검토 요청을 보내야 하며 그 동안의 대기 시간이 필요합니다.<br>\n또한, 추가 사용이 가능한지도 미지수인 상황이었습니다.\n\n따라서 한 대의 서버에서 무중단 배포를 적용하기엔 **블루/그린 방식**이 최적의 방식이라고 판단했습니다.\n\n> 더불에 블루/그린 방식의 문제점을 생각해봤을 때, 배포 서버의 자원을 2배로 사용한다는 점은 서비스를 운영함에 있어 문제되지 않았습니다.\n\n## 배포 전략\n\n기본적으로 2개의 포트를 사용합니다.<br>\n저희 팀은 8080, 그리고 8081 포트를 사용하기로 했습니다.\n\n배포 과정의 전체적인 Flow는 다음과 같습니다.\n\n1. 비어있는 포트(Idle Port)를 찾는다.\n2. 해당 포트에 새로운 버전(그린)을 배포한다.\n3. 그린이 이상없이 동작하는지 확인하기 위해 헬스 체크를 진행한다.\n4. 문제가 없다면, Nginx의 포트 포워딩 설정을 그린으로 변경하고 기존 버전(블루)을 종료한다.<br>\n   문제가 있다면, 그린을 종료한다.\n\n---\n\n위 시나리오의 과정에 대해 더 자세히 말씀드리겠습니다.\n\n1)<br>\nIdle Port를 찾는 방법은, 8080 포트에 API 요청을 보내고 그에 따른 응답의 결과로 찾습니다.<br>\n만약 응답 코드가 200이면 현재 8080 포트에서 블루가 동작중인 셈이니 8081포트가 Idle Port가 됩니다.\n\n21)<br>\n이런 식으로 찾은 Idle Port에 그린을 배포합니다.\n\n3)<br>\n이후, 그린의 헬스 체크를 진행하기 위해 30초의 대기 시간을 가집니다.<br>\n대기 시간을 가지는 이유는 그린이 띄어진 직후에 실행한 헬스 체크는 정상으로 확인되었지만, 이후에 혹여나 예상치 못한 문제로 그린이 Down 될 가능성을 염두했기 때문입니다.<br>\n또한 당시 스프링 애플리케이션이 서버에 완전히 띄어지기까지 약 20초의 시간이 소요되는 점을 고려했기 때문입니다.\n\n4)<br>\n이후 헬스 체크를 진행합니다.<br>\n체크 결과, 그린 서버가 정상적으로 배포가 됐다면 Nginx의 포트 포워딩 설정을 변경합니다. 그리고 5초의 여유를 가지고 블루 서버를 종료합니다.\n그린 서버에 문제가 생겨 정상적으로 실행되지 않았다면 종료합니다.\n\n## 배포 스크립트\n\n위 시나리오대로 작성한 배포 스크립트는 아래와 같습니다.\n\n```bash\nRESPONSE_CODE=$(curl -o /dev/null -w \"%{http_code}\" http://localhost:8080/celebs)\nif [ ${RESPONSE_CODE} = 200 ];\n    then\n        IDLE_PORT=8081\n        IDLE_MONITORING_PORT=18082\n        USED_PORT=8080\n    else\n        IDLE_PORT=8080\n        IDLE_MONITORING_PORT=18081\n        USED_PORT=8081\nfi\n\necho \"IDLE_PORT=${IDLE_PORT}\"\necho \"IDLE_MONITORING_PORT=${IDLE_MONITORING_PORT}\"\n\nIMAGE_TAG=back-prod-${APP_VERSION_TAG}\nDOCKER_CONTAINER_NAME=backend-${IDLE_PORT}\nDOCKER_HUB_REPOSITORY=celuveat/celuveat\nSERVER_LOG_DIR_PATH=~/log\nDOCKER_LOG_DIR_PATH=/app/logs\n\ndocker pull ${DOCKER_HUB_REPOSITORY}:${IMAGE_TAG}\ndocker run \\\n-d \\\n--name ${DOCKER_CONTAINER_NAME} \\\n-p $IDLE_PORT:8080 \\\n-p $IDLE_MONITORING_PORT:18080 \\\n-e \"SPRING_PROFILES_ACTIVE=prod\" \\\n-v ${SERVER_LOG_DIR_PATH}:${DOCKER_LOG_DIR_PATH} \\\n${DOCKER_HUB_REPOSITORY}:${IMAGE_TAG}\n\n# 새로 뜬 컨테이너 확인\nsleep 30\nHEALTHY_CODE=$(curl -o /dev/null -w \"%{http_code}\" http://localhost:${IDLE_PORT}/celebs)\nif [ ${HEALTHY_CODE} != 200 ];\n    then\n            IDLE_CONTAINER_ID=$(docker ps -q --filter \"publish=${IDLE_PORT}\")\n            docker stop ${IDLE_CONTAINER_ID}\n            docker rm ${IDLE_CONTAINER_ID}\n            echo \"TERMINATED\"\n            exit 1\nfi\n\necho \"set \\$service_url http://127.0.0.1:${IDLE_PORT};set \\$service_monitoring_url http://127.0.0.1:${IDLE_MONITORING_PORT};\" | sudo tee /etc/nginx/conf.d/service-url.inc\nsudo service nginx reload\n\nsleep 5\nUSED_CONTAINER_ID=$(docker ps -q --filter \"publish=${USED_PORT}\")\ndocker stop ${USED_CONTAINER_ID}\ndocker rm ${USED_CONTAINER_ID}\ndocker image prune -f\n```\n\n## 추가로 신경써야 할 점\n\n기존에 애플리케이션 모니터링을 위해 Spring Actuator에 포트를 할당해 사용하고 있었습니다.<br>\n따라서 모니터링을 위한 포트도 함께 변경해줘야 합니다.\n\n저희 팀은 `8080, 18080` / `8081, 18081` 으로 묶어 관리하기로 결정했습니다.\n\n모니터링을 위한 포트도 마찬가지로 Nginx에서 포트 포워딩을 통해 처리해줍니다.\n\n**그런데 여기서 중요한 점이 있습니다!!**<br>\nNginx에서 Actuator로 향하는 포트를 listen하기 위해 18080 포트를 사용하고 있었습니다.<br>\n즉, Actuator의 포트로 18080 포트를 사용하지 못하게 됩니다.\n\n결과적으로 모니터링을 위한 포트는 `18081` 과 `18082` 를 사용하게 되었습니다.\n\n## Nginx 설정\n\n아래는 Nginx 포트 포워딩 설정 스크립트입니다.\n\n```bash\nserver {\n  listen 443 ssl;\n  server_name api.celuveat.com;\n\n  ssl_certificate /etc/letsencrypt/live/api.celuveat.com/fullchain.pem;\n  ssl_certificate_key /etc/letsencrypt/live/api.celuveat.com/privkey.pem;\n\n  include /etc/nginx/conf.d/service-url.inc;\n  location / {\n    proxy_pass $service_url;\n    proxy_set_header Host $http_host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header X-Forwarded-Proto $scheme;\n  }\n}\n\nserver {\n  listen 18080;\n  include /etc/nginx/conf.d/service-url.inc;\n  location / {\n    proxy_pass $service_monitoring_url;\n    proxy_set_header Host $http_host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header X-Forwarded-Proto $scheme;\n  }\n}\n```\n\n## 기타 설정 (참고용)\n\nGithub Actions Workflow는 아래와 같이 수정되었습니다.\n\n```bash\nname: ✨ Celuveat backend PROD CD ✨\n\nenv:\n  PROFILE: prod\n  IMAGE_TAG: back-prod-${{ secrets.APP_VERSION_TAG }}\n  DOCKER_CONTAINER_NAME: backend\n  DOCKER_HUB_REPOSITORY: celuveat/celuveat\n  SERVER_LOG_DIR_PATH: ~/log\n  DOCKER_LOG_DIR_PATH: /app/logs\n\non:\n  workflow_dispatch:\n  push:\n    branches:\n      - main\n    paths:\n      - \"backend/**\"\n\njobs:\n  backend-docker-build-and-push:\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: ./backend\n\n    steps:\n      - name: ✨ Checkout repository\n        uses: actions/checkout@v3\n        with:\n          submodules: true\n          token: ${{ secrets.ACTION_TOKEN }}\n\n      - name: ✨ JDK 17 설정\n        uses: actions/setup-java@v3\n        with:\n          java-version: '17'\n          distribution: 'temurin'\n\n      - name: ✨ Gradle Caching\n        uses: actions/cache@v3\n        with:\n          path: |\n            ~/.gradle/caches\n            ~/.gradle/wrapper\n          key: ${{ runner.os }}-gradle-${{ hashFiles('**/*.gradle*', '**/gradle-wrapper.properties') }}\n          restore-keys: |\n            ${{ runner.os }}-gradle-\n\n      - name: ✨ Gradlew 권한 설정\n        run: chmod +x ./gradlew\n\n      - name: ✨ Jar 파일 빌드\n        run: ./gradlew bootJar\n\n      - name: ✨ DockerHub에 로그인\n        uses: docker/login-action@v2\n        with:\n          username: ${{ secrets.DOCKER_HUB_USERNAME }}\n          password: ${{ secrets.DOCKER_HUB_PASSWORD }}\n\n      - name: ✨ Docker Image 빌드 후 DockerHub에 Push\n        uses: docker/build-push-action@v4\n        with:\n          context: ./backend\n          file: ./backend/Dockerfile\n          push: true\n          platforms: linux/arm64\n          tags: ${{ env.DOCKER_HUB_REPOSITORY }}:${{ env.IMAGE_TAG }}\n\n  backend-docker-pull-and-run:\n    runs-on: [self-hosted, prod]\n    if: ${{ needs.backend-docker-build-and-push.result == 'success' }}\n    needs: [ backend-docker-build-and-push ]\n    steps:\n      - name: ✨ 배포 스크립트 실행\n        run: |\n          export APP_VERSION_TAG=${{ secrets.APP_VERSION_TAG }}\n          sh /home/ubuntu/deploy.sh\n```\n"},{"excerpt":"무중단 배포란? 적용하는 이유는? 한 대의 서버로만 배포하는 모놀리식 구조의 서비스인 경우, 서비스를 새로운 버전으로 업데이트해야 하는 경우가 있습니다. 이때 새롭게 배포하게 되는데, 배포되는 시간동안 서비스는 잠깐 사용할 수 없게 됩니다. 이 시간을 Down Time 이라고 부릅니다. 이 Down Time을 제거하여 사용자에게 불편함을 주지 않기 위해 …","fields":{"slug":"/zero-downtime-deployment/"},"frontmatter":{"date":"October 20, 2023","title":"무중단 배포 이해하기","tags":["infra","zero-downtime-deployment"]},"rawMarkdownBody":"\n## 무중단 배포란? 적용하는 이유는?\n\n한 대의 서버로만 배포하는 모놀리식 구조의 서비스인 경우, 서비스를 새로운 버전으로 업데이트해야 하는 경우가 있습니다.\n\n이때 새롭게 배포하게 되는데, 배포되는 시간동안 서비스는 **잠깐 사용할 수 없게 됩니다.**\n\n이 시간을 *Down Time* 이라고 부릅니다.\n\n**이 Down Time을 제거하여 사용자에게 불편함을 주지 않기 위해 적용하는 것**이 무중단 배포입니다.\n\n## 적용하는 방법\n\n무중단 배포를 적용하는 방법에는 크게 3가지 방식이 있습니다.\n\n1. 롤링 배포\n2. 블루/그린 배포\n3. 카나리 배포\n\n각 방식에 따른 장단점이 분명하기에, 상황에 맞는 배포 전략을 적절히 선택하는 것이 중요합니다.\n\n### 롤링 배포\n\n신 버전을 **점진적으로 배포**한다는 특징을 가진 배포 방식입니다.\n\n![롤링 배포](Rolling-Deployment.png)\n\n#### 장점\n\n- 모두 완료되기 전, 기존 버전도 유지되기 때문에 신 버전에 문제가 생겼을 경우 롤백이 쉽습니다.\n- 추가적인 인스턴스가 필요하지 않아도 됩니다.\n\n#### 단점\n\n- 무중단 배포가 진행되는 동안, 트래픽이 일부 인스턴스에 과도하게 몰릴 수 있습니다.\n- 기존 버전과 신 버전이 공존하는 시간이 존재하기에 호환성 문제가 발생할 수 있습니다.\n\n### 블루/그린 배포\n\n블루를 기존 버전, 그린을 신 버전으로 지칭합니다.\n\n신 버전의 새로운 인스턴스를 우선적으로 구성한 후에, 배포가 완료되면 트래픽을 한꺼번에 신 버전으로 라우팅하는 방식입니다.\n\n![블루/그린 배포](Blue-Green-Deployment.png)\n\n#### 장점\n\n- 신 버전을 동일한 환경의 새로운 인스턴스를 구성하여 미리 테스트할 수 있습니다.\n- 배포 도중이나 이후에 신 버전에 문제가 생겼을 경우, 기존 버전을 유지시킨다면 쉽게 복구할 수 있습니다.\n- 기존 버전과 공존하지 않기 때문에 호환성 문제가 발생하지 않습니다.\n\n#### 단점\n\n- 배포를 위한 리소스가 두 배로 필요합니다.\n\n### 카나리 배포\n\n옛날 광부들이 유독 가스에 민감한 카나리아 새를 이용해 가스 누출 위험을 감지했던 것에서 유래한 것으로 잠재적 문제 상황을 미리 발견하기 위한 방식입니다.\n\n신 버전 배포 후 트래픽의 일부를 신 버전으로 라우팅 한 다음, 모니터링 및 피드백 과정을 거칠 수 있습니다.\n\n![카나리 배포](Canary-Deployment.png)\n\n#### 장점\n\n- 신 버전 배포 이후 신 버전을 일부 사용자만이 경험할 수 있도록 조절할 수 있습니다.\n- 신 버전에 문제가 생겼을 경우 기존 버전으로 롤백이 쉽습니다.\n\n#### 단점\n\n- 기존 버전과 신 버전이 공존하는 시간이 존재하기 때문에 호환성 문제가 발생할 수 있습니다.\n\n### Reference\n> - https://www.samsungsds.com/kr/insights/1256264_4627.html\n> - https://hudi.blog/zero-downtime-deployment/"},{"excerpt":"DBCP (DataBase Connection Pool) 탄생 배경 서버 애플리케이션에서 DB 서버로 요청을 보내면 TCP 통신을 한다. TCP 통신은 Connection이 유지되는 상태에서 통신한다. 이 Connection은 생성하고 사용하고 삭제되기까지 꽤 많은 작업이 포함된다. 즉, 통신 비용이 비싸다. 이러한 상황에서 서버는 많은 요청을 처리하는데…","fields":{"slug":"/dbcp/"},"frontmatter":{"date":"October 16, 2023","title":"DBCP와 HikariCP 이해하기","tags":["database","DBCP","hikariCP"]},"rawMarkdownBody":"\n## DBCP (DataBase Connection Pool) 탄생 배경\n\n서버 애플리케이션에서 DB 서버로 요청을 보내면 TCP 통신을 한다.\n\nTCP 통신은 **Connection**이 유지되는 상태에서 통신한다.\n\n이 Connection은 생성하고 사용하고 삭제되기까지 꽤 많은 작업이 포함된다. 즉, 통신 비용이 **비싸다**.\n\n이러한 상황에서 서버는 많은 요청을 처리하는데, 이 요청들이 DB 서버로의 요청으로 이루어진다면 비싸고 오래걸리는 작업이 많아질 것이다.\n\n결국, 서버 성능에 악영향을 줄 수 있다.\n\n이를 해결하기 위해 DBCP는 탄생했다.\n\n## DBCP 기본 개념\n\n서버가 실행되면 DB와 Connection을 **미리 만들어 놓고 Pool에 저장해놓는다.**\n\n미리 만들어 놓은 DB Connection을 서버에 미리 가지고 있다가, DB로 요청을 보내야 하는 상황이 발생한다면 기존에 가지고 있는 Connection을 이용해 쿼리를 요청한다.\n\n요청을 모두 수행하면 요청에 사용된 Connection을 다시 Pool에 **반환**을 한다. \n\n결국, Connection을 생성하고 삭제하는 과정이 모두 생략되기 때문에 더 빠른 속도로 요청을 처리할 수 있게 된다.\n\n## 종류\n\n이 DBCP를 구현한 제품으로는 [HikariCP](https://github.com/brettwooldridge/HikariCP), [Apache Commons DBCP](https://commons.apache.org/proper/commons-dbcp/), [c3p0](https://github.com/swaldman/c3p0) 등이 있다.\n\n## HikariCP\n\nSpring Boot 2.0부터 내장되어 사용되는 DBCP 이다.\n\n오픈소스로서, 타 제품에 비해 성능이 매우매우 뛰어나다.\n\n실제로 README를 읽어보면 타 제품에 비해 성능이 너무 뛰어나다고 엄청나게 자랑한다.\n\n아래 사진은 HikariCP 에서 자체적으로 진행한 벤치마크 결과이다. \n\n![자체 벤치마크 결과](result_of_benchmark.png)\n\n> One *Connection Cycle*은 한 쌍의 `DataSource.getConnection()`, `Connection.close()`<br>\n> One *Statement Cycle*은 한 쌍의 `Connection.prepareStatement()`, `Statement.execute()`, `Statement.close()` 이다.\n\n## 중요한 설정 포인트\n\n이 유용한 DBCP를 정말 유용하게 사용하려면, DB와 서버에서 각각 DB Connection 관련 설정을 잘 알아야 한다.\n\n> DB는 MySQL, 서버는 Spring, DBCP는 hikariCP를 기준으로 설명한다.\n\n### 데이터베이스 서버 설정 (MySQL 기준)\n\n- `max_connections`\n  - DB 서버가 클라이언트와 맺을 수 있는 최대 Connection 수이다.\n  - 컴퓨팅 자원을 적절히 사용하면서 동시에 가장 높은 처리량만큼을 설정하는 것이 중요하다.\n\n- `wait_timeout`\n  - 생성된 Connection이 아무 처리도 하지 않을 때, 새로운 요청이 오기까지 얼마만큼의 시간을 기다릴 지 설정한다.\n    만약 `wait_timeout` 시간이 지나게 될 경우, Connection은 Close 된다.\n  - 클라이언트에서 문제가 생겨 Connection을 정상적으로 종료하지 못하는 상황(비정상적 Connection 종료, Connection 반환 X, 네트워크 단절)에, 생성된 Connection을 알아서 종료해주기 위한 적절한 시간을 결정하는 것이 중요하다.\n  - `wait_timeout`으로 인해 Connection이 종료되기 전에 새로운 요청이 들어올 경우, 0초부터 다시 시간을 측정한다.\n\n### DBCP 설정 (HikariCP 기준)\n\n- `minimumIdle`\n  - Pool에서 유지하는 최소한의 Idle Connection의 수이다.\n  - 만약 Idle 상태의 Connection 수가 `mimimumIdle` 값보다 작고, 전체 Connection 수가 `maximumPoolSize`보다 작은 상황이라면, 신속하게 새로운 Connection을 생성하고 Pool에 추가한다.\n  - Default는 `maximumPoolSize`와 동일한 값이다. 값을 동일하게 설정해야 요청이 들어올 때 새로운 Connection을 만들지 않고 DBCP를 사용하는 장점을 누릴 수 있기 때문이다.   \n\n- `maximumPoolSize`\n  - Pool이 가질 수 있는 최대 Connection의 수이다.\n  - Idle 상태와 Active 상태의 Connection을 모두 포함한다.\n\n- `maxLifetime`\n  - Pool에서 Connection의 최대 수명 시간이다.\n  - Active인 상태는 절대 종료되지 않으며 Connection이 Pool에 반환된 이후의 시간을 토대로 종료한다.<br>\n    이 특징 떄문에, Active 상태의 Connection이 Pool에 모종의 이유로 Pool에 반환되지 않은 시간이 DB 서버의 `wait_timeout`보다 길어진 이후에 DB로 요청을 보내게 되면, DB는 해당 Connection을 이미 Close 했기 떄문에, 해당 과정에서 예외가 발생할 수 있다. 이는 Connection Leak의 문제가 될 수 있다. \n  - 풀에서 갑작스레 대량 소멸을 방지하기 위해 Connection 별로 일종의 조치를 취해놓았다.\n  - HikariCP 가 권장하길, DB의 `wait_timeout`보다 몇 초 더 짧게 설정하는 것이 좋다고 한다.<br>\n    이유로는, 만약 `wait_timeout`과 `maxLifetime` 값을 동일하게 설정했다고 가정했을 때, `maxLifetime`에 얼마 남지 않은 시점에 요청을 처리해야 한다면, 해당 Connection이 DB 서버로 요청을 보내는 시간이 포함되는데, 요청이 DB 서버에 도달하기 전에 `wait_timeout` 시간이 지나게 되어 DB 서버 쪽에서 Connection을 끊게 된다면 예외가 발생할 수 있기 때문이다.\n\n- `connectionTimeout`\n  - 서버 사용자가 Pool에서 Connection을 획득하기까지 얼마만큼의 시간을 기다릴 것인지 최대 시간을 정하는 설정이다.\n  - 설정 시간만큼 Connection을 Pool에서 획득하지 못하면 `SQLException`이 발생한다.\n  - 위 설정도 마찬가지로 `wait_timeout` 보다 더 짧은 시간으로 설정해야 한다.<br>\n    만약 `connectionTimeout`과 `wait_timeout` 시간이 같다면, 사용자가 Pool에서 `connectionTimeout` 직전에 Connection을 획득했다 하더라도, 실제 DB 서버에 도달하기까지 추가적인 시간이 반드시 존재하고, 만약 도달한 시점에 이미 `wait_timeout` 시간을 넘은 상태라면 예외를 발생하기 때문이다.\n\n## 적절한 파라미터 값 설정하는 방법\n\n우선 서버 애플리케이션을 모니터링을 하며 리소스 사용률, 서버 스레드 수, DBCP 정보들을 확인할 수 있어야 한다.\n\n그리고 [nGrinder](https://naver.github.io/ngrinder/)와 같은 부하 테스트 툴을 사용하여 실제로 서버에 부하를 줌으로써 병목 지점을 확인한다.\n\n만약 병목 지점이 생긴다면, 해당 시점 이후의 지표들을 토대로 병목 지점을 유추한다.\n\n유추되는 지점이 존재한다면 해당 값을 적절히 조절해가며 적절한 설정 값을 찾는 과정을 거친다.\n\n> ### Reference\n> - https://www.youtube.com/watch?v=zowzVqx3MQ4"},{"excerpt":"사용자 유치 전, 서버 성능 개선을 위해 Tomcat 성능 최적화를 진행하려고 합니다. 그 과정에서 설정할 값들인 , , 에 대해 이해하려고 합니다. Max-Threads JVM 기반에서 동작하는 Tomcat은 HTTP 요청을 받으면, 각 요청을 하나의 쓰레드가 처리하도록 동작합니다. 이  설정은 Tomcat에서 최대 몇 개까지의 쓰레드를 동작시킬 것인지…","fields":{"slug":"/tomcat-tuning/"},"frontmatter":{"date":"September 09, 2023","title":"[셀럽잇] Tomcat 성능 최적화를 위해 Max-Threads, Max-Connections, Accept-Count 설정하기","tags":["셀럽잇","tomcat"]},"rawMarkdownBody":"\n사용자 유치 전, 서버 성능 개선을 위해 Tomcat 성능 최적화를 진행하려고 합니다.\n\n그 과정에서 설정할 값들인 `Max-Threads`, `Max-Connections`, `Accept-Count`에 대해 이해하려고 합니다.\n\n## Max-Threads\n\nJVM 기반에서 동작하는 Tomcat은 HTTP 요청을 받으면, 각 요청을 하나의 쓰레드가 처리하도록 동작합니다.\n\n이 `Max-Threads` 설정은 Tomcat에서 최대 몇 개까지의 쓰레드를 동작시킬 것인지 설정하는 옵션입니다.\n\n즉, 동시에 최대 몇 개의 요청을 처리할 것인지에 대한 설정입니다.\n\ndefault 값은 200 입니다.\n\n## Max-Connections\n\nHTTP 요청을 수락하고 처리할 수 있는 최대 Connection의 수에 대한 설정입니다.\n\n설정 수치에 도달하면, 추가 요청에 대해 Accept는 하지만 처리하진 않습니다.\n\nAccept된 Connection은 현재 처리 중인 Connection의 수가 `Max-Connections`보다 아래로 떨어질 때까지 Block 됩니다.\n\nBlock 된 요청은 `Accept-Count` 수 만큼 존재할 수 있습니다.\n\ndefault 값은 8192 입니다.\n\n## Accept-Count\n\n`Max-Connections` 보다 더 많은 요청이 들어오게 되어 요청을 Block 하면, 운영체제는 Block 된 요청을 대기 큐에 대기시킵니다.\n\n이때, 대기 큐의 Size가 `Accept-Count` 입니다.\n\n위 크기보다도 더 많은 요청이 들어오게 된다면, 운영체제가 해당 요청을 거절하거나 Connection Time Out이 발생하게 됩니다.\n\n## 그렇다면 어떤 식으로 조절할까?\n\n현재 vUser를 300명으로 했을 때를 가정하여 부하 테스트를 진행하고 있습니다.\n\n그 결과 아래와 같은 최적의 값이 도출되는 것을 확인할 수 있었습니다.\n\n- Max-Threads: 300\n- Max-Connection: 8192 (default)\n- Accept-Count: 10 (default)\n\n> ### 참고\n> - [톰캣 공식문서](https://tomcat.apache.org/tomcat-8.5-doc/config/http.html)"},{"excerpt":"로그아웃 API 명세 우선 OAuth 관련 기능을 구현하려면 무조건 해당 OAuth 서버의 공식 문서를 통해 API 명세를 확인해야합니다. 구현하고자 하는 OAuth 서버인 카카오의 로그아웃 API 명세는 다음과 같습니다.  인증 방식 선택 인증 방식을 보면 , (이하 어드민 키)로 두 가지가 있습니다. 액세스 토큰 방식은 쉽게 생각해 JWT를 통해 인증…","fields":{"slug":"/oauth-logout-and-withdraw/"},"frontmatter":{"date":"August 14, 2023","title":"[셀럽잇] OAuth 2.0 로그아웃 구현","tags":["셀럽잇","OAuth-2.0"]},"rawMarkdownBody":"\n## 로그아웃 API 명세\n\n우선 OAuth 관련 기능을 구현하려면 무조건 해당 OAuth 서버의 공식 문서를 통해 API 명세를 확인해야합니다.\n\n구현하고자 하는 OAuth 서버인 카카오의 로그아웃 API 명세는 다음과 같습니다.\n\n![카카오 OAuth 공식문서의 로그아웃 API 명세](kakao-logout-api-docs.png)\n\n## 인증 방식 선택\n\n인증 방식을 보면 `액세스 토큰`, `서비스 앱 어드민 키`(이하 어드민 키)로 두 가지가 있습니다.\n\n액세스 토큰 방식은 쉽게 생각해 JWT를 통해 인증하는 것이고, 어드민 키는 API 요청을 보낼 때, 어드민 키를 함께 파라미터로 담아 요청하는 방식입니다.\n\n**저희 팀이 선택한 인증 방식은 어드민 키 방식**인데요. 이유로는, 로그인 시 발급되는 액세스 토큰을 따로 관리하지 않으며, 서비스의 인증 방식으로 토큰 방식이 아닌 세션 방식을 채택해 사용하고 있기 때문입니다.\n\n하지만 액세스 토큰 방식이 더 간단하니 상황이 된다면 액세스 토큰 방식을 사용하는 것을 추천합니다.\n\n어드민 키는 '내 애플리케이션 - 앱 키' 에서 확인할 수 있습니다.\n\n## 구현\n\n그럼 이제 `어드민 키` 방식으로 구현해보겠습니다.\n\n일단 그 전에 어드민 키를 환경 변수에 설정해줍니다.\n\n![환경 변수 적용](admin-key-in-properties.png)\n\n그 다음 API 명세를 자세히 확인합니다.\n\n![API 명세 상세 정보](api-specific.png)\n\n눈여겨 볼 포인트는 **헤더**와 **본문**, **응답**입니다.\n\n![OauthController](code-of-OauthController.png)\n\n![KakaoMemberClient](code-of-KakaoMemberClient.png)\n\n![KakaoApiClient](code-of-KakaoApiClient.png)\n\n![KakaoLogoutResponse](KakaoLogoutResponse.png)\n\n위 코드를 통해 간단하게 로그아웃 기능을 구현할 수 있습니다.\n\n## 테스트\n\n테스트 방식은 다음과 같습니다.\n\n1. 로컬에서 프론트 애플리케이션을 띄운다.\n2. 브라우저를 통해 로그인 요청을 보낸다.\n3. 로그인 후 획득한 세션 아이디를 통해 포스트맨으로 로그아웃 요청을 보낸다.\n4. 결과를 확인한다.\n\n### 로그인 요청 실패\n\n이때, 브라우저를 통해 로그인 요청을 보내면 이런 에러가 뜰 수 있습니다.\n\n![테스트 과정 중, 로그인 요청 실패](error-of-test-login.png)\n\n이는 **허용 IP 주소를 설정해주지 않아서 생기는 오류입니다.**\n\n이를 해결하기 위해 아래 페이지를 통해 현재 사용중인 IP를 추가합니다.\n\n![허용 서버 IP 주소 관리 페이지](accept-server-ip-addresses.png)\n\n입력할 IP는 `외부 IP 주소`입니다. 외부 IP는 (맥 기준) 터미널에 `curl ipecho.net/plain; echo` 명령어를 통해 확인할 수 있습니다.\n\n그리고 다시 로그인 요청을 보내면 정상적으로 되는 것을 확인할 수 있습니다.\n\n### 로그아웃 요청\n\n로그인을 통해 얻은 세션을 아래와 같이 설정해주고 요청을 보내봅니다.\n\n![포스트맨을 이용해 로그아웃 요청](test-logout.png)\n\n![테스트 결과 - 성공!](result-of-test.png)\n\n정상적으로 로그아웃되는 것을 확인할 수 있습니다."},{"excerpt":"Java 17 우아한테크코스에서 진행한 미션들의 Java 버전은 11이었습니다. 모든 팀원들에게 익숙한 버전은 11 버전임은 부정할 수 없는 사실이지만, 그럼에도 17 버전을 선택한 이유는 다음과 같습니다. 생산성 17 버전에 포함된 Record 타입, String 블럭 사용, Stream.toList()사용 으로 생산성 향상을 기대했기 때문입니다. 우테…","fields":{"slug":"/tech-stacks/"},"frontmatter":{"date":"July 07, 2023","title":"[셀럽잇] 기술 스택 및 선정 이유","tags":["셀럽잇"]},"rawMarkdownBody":"\n![프로젝트에서 선택한 기술 스택 아이콘](tech_stack_icons.png)\n\n## Java 17\n\n우아한테크코스에서 진행한 미션들의 Java 버전은 11이었습니다.\n\n모든 팀원들에게 익숙한 버전은 11 버전임은 부정할 수 없는 사실이지만, 그럼에도 17 버전을 선택한 이유는 다음과 같습니다.\n\n### 생산성\n\n17 버전에 포함된 **Record 타입, String 블럭 사용, Stream.toList()**사용 으로 생산성 향상을 기대했기 때문입니다.\n\n우테코에서 진행하는 프로젝트 특성상 2주 단위로 기능을 빠르게 추가해야 하는 상황이었습니다.\n\n따라서 팀에게 있어 생산성도 중요한 가치라고 판단했습니다.\n\n> Record 타입은 JDK 14에 추가됐습니다. [관련 글](https://www.baeldung.com/java-record-keyword)<br>\n> String 블럭은 JDK 15에 추가됐습니다. [관련 글](https://www.baeldung.com/java-text-blocks)\n> Streams.toList()는 JDK 17에 추가됐습니다. [관련 글](https://www.baeldung.com/java-stream-to-list-collecting)\n\n### JDK 8, 11에 이은 LTS 버전\n\nJDK 11은 26년 9월, JDK 17은 31년 9월까지 지원하는 LTS 버전입니다.\n\n셀럽잇 프로젝트를 앞으로 몇 년이고 지속할 수 있는 가능성이 존재했습니다.\n\n또한, 신규 프로젝트를 진행하는 상황에 있어 레거시를 고려할 필요는 없을 뿐더러 앞으로 있을 미래를 대비하는 것이 현명하다고 판단했습니다.\n\n### 더 나은 GC 성능\n\n이는 추가적인 이유를 찾아보다가 발견한 이유입니다.\n\n바로, JDK 11보다 더 나은 GC 성능을 가진다는 것인데요.\n\nG1 GC의 경우 JDK 11에 비해, 8.66% 더 빠르다는 벤치마킹 결과가 존재하는 것을 확인할 수 있었습니다.\n\n> [참고 문서](https://www.optaplanner.org/blog/2021/09/15/HowMuchFasterIsJava17.html)\n\nGC 개선, JIT 컴파일러 개선, 클래스 데이터 공유로 인한 개선으로 인한 것이라고 합니다.\n\n## Spring Boot 3\n\nJava 17은 Spring Boot 3 부터 지원을 강제하는 이유에서 선택했습니다.\n\n## Nginx\n\n초기 개발 단계에서 React와 Spring이 한 대의 EC2 인스턴스 내에 존재했습니다.\n\n따라서 접속 URL에 따라 포트 포워딩을 할 필요가 있었습니다.\n\n추가로, Nginx와 연동 가능하며 HTTPS 적용에 필요한 SSL 인증서를 발급해주는 무료 오픈소스인 [Let's Encrypt](https://letsencrypt.org/)를 사용하여 간편하게 HTTPS 적용을 하기 위해 사용했습니다.\n\n> [[10분 테코톡] 🤫 피케이의 Nginx](https://youtu.be/6FAwAXXj5N0?feature=shared)\n\n## JPA & Spring Data JPA\n\n사실 처음에는, JPA를 사용해본 적이 없는 팀원이 저를 포함해 2명이 있었습니다.\n\n따라서 처음엔 발생할 수 있는 문제에 모든 팀원이 대응할 수 있기 위해 JDBC를 사용했는데요.\n\n개발을 본격적으로 시작하기까지는 조금의 시간이 있었습니다.\n\n그 시간동안 JPA를 학습할 수 있었고, JPA를 통해 개발 초기에 빠르게 개발할 수 있다는 점에서 선택을 변경했습니다.\n\n또한, JPA는 객체와 RDBMS의 패러다임 불일치를 해결해준다는 점에서 큰 메리트를 느꼈습니다.\n\n## QueryDsl\n\n셀럽잇이 제공하는 기능 중, 음식점을 다양한 조건으로 필터링하여 조회하는 기능이 있었습니다.\n\n해당 기능을 구현하기 위해서는 동적 쿼리를 만들어야 했는데요.\n\nQueryDsl을 선택하기 전의 동적 쿼리는 유지보수가 매우 힘들었습니다.\n\n따라서 유지보수를 위해 도입했습니다.\n\n## Docker\n\n초기 개발 단계에서 운영 단계를 준비하는 과정에서 동일한 환경을 쉽게 세팅하기 위해 사용했습니다.\n\n## nGrinder\n\n부하 테스트 툴로써 다른 대안이었던 JMeter에 비해 더 편리한 사용성과 편리한 UI/UX, 그리고 무엇보다 한국어를 지원한다는 점에서 선택했습니다.\n\n## Grafana, Prometheus, Loki, Promtail\n\n다른 대안이었던 AWS CloudWatch는 새로 학습해야 하는 학습 비용이 존재했습니다. 동시에 금전적인 비용이 들어갑니다.\n\n사실 _Grafana, Prometheus, Loki, Promtail_도 팀원 전체가 학습을 해야하는 건 마찬가지였지만, 인프런에 김영한 님의 관련 강의가 존재하였고 이를 통해 쉽게 기술을 익혀 사용할 수 있을 것으로 판단했습니다.\n\n동시에 모두 무료 오픈소스인 점에서 선택했습니다.\n\n## S3\n\n음식점에 대한 정보 중, 음식점 사진이 존재했습니다.\n\n초기에는 사진을 모두 EC2 인스턴스 내에 저장하여, 정적으로 Serving 했습니다.\n\n하지만 음식점의 수가 증가함에 따라 사진 파일의 수도 증가하고, 그에 따라 서버의 용량을 걱정하지 않을 수 없었습니다.\n\n동시에, React로 개발한 결과물을 빌드한 것은 모두 정적인 자원이며, EC2 인스턴스 내에서 빌드하는 데에 많은 시간이 걸렸습니다.\n\n따라서 정적 자원들은 스토리지에서 제공하기 위해 사용했습니다.\n\n## CloudFront\n\nS3를 그냥 사용하면 보안과 관련된 문제가 발생할 수 있습니다.\n\n또, S3에 있는 자원에 접근하려면 매우 길고 이상한 URL로 접근을 해야 했습니다.\n\n이를 해결하기 위해 CDN 역할의 AWS CloudFront를 도입했습니다.\n\n## Github Actions\n\nCI/CD 툴로 사용했습니다.\n\n다른 대안이었던 Jenkins는 팀원 전체가 학습할 시간이 필요했습니다.\n\n따라서 바로 적용할 수 있는 CI/CD 툴을 적용했습니다.\n\n## OAuth 2.0\n\nOAuth 2.0을 적용하면 사용자의 개인 정보를 저장하지 않아도 되는 장점이 있습니다.\n\n또한, 사용자 퍼널 개선을 위한 목적으로 선택했습니다."},{"excerpt":"드디어 2학기 현장실습이 끝이 났다.\n2022년 08월 29일부터 12월 15일까지, 총 75일 간의 내 첫 사회 생활이 끝이 났다. 끝이 난 지 일주일이 지났지만 이제서야 작성하는 이유는, 그동안 SSAFY  면접과 우아한테크코스 프리코스 최종 코딩테스트를 마무리하고 쉬기 바빠 이제서야 작성한다. 😅 약 3-4개월 간의 치열했던 현장 실습을 회고해보자.…","fields":{"slug":"/kissoft-retrospection/"},"frontmatter":{"date":"December 23, 2022","title":"키스소프트 현장실습 회고","tags":["retrospection","kissoft"]},"rawMarkdownBody":"\n드디어 2학기 현장실습이 끝이 났다.\n2022년 08월 29일부터 12월 15일까지, 총 75일 간의 내 첫 사회 생활이 끝이 났다.\n\n끝이 난 지 일주일이 지났지만 이제서야 작성하는 이유는, 그동안 SSAFY  면접과 우아한테크코스 프리코스 최종 코딩테스트를 마무리하고 쉬기 바빠 이제서야 작성한다. 😅\n\n약 3-4개월 간의 치열했던 현장 실습을 회고해보자.\n\n## 웹이 아닌 앱 부서 배치\n\n내 첫 부서는 다름아닌 **앱 개발 부서**였다.<br>\n이전 포스팅에서도 작성했지만, 전혀 예상치도 못한 부서로 배치가 되었다.<br>\n게다가 java를 이용해서 안드로이드 개발하는 것이면 그래도 괜찮은데, 웬 iOS 개발을 한다고 했다.\n\n덕분에 난생 처음으로 맥을 만져보았지만, 설레는 꿈과 희망으로 첫 출근한 내 기대가 산산히 부셔지는 순간이었다.<br>\n때문에 한 달동안 swift를 공부했다. java처럼 블로그들에 정보들이 많이 활성화되지 않아서, 공식 홈페이지를 항상 모니터 한 켠에 켜놓고, 정말 많은 검색을 통해 꾸역꾸역 공부했다.\n\n실제로 공식 홈페이지를 통해 swift의 기본을 가벼운 마음으로 이틀간 공부했고, 그 다음부터는 바로 프로젝트를 진행했다. ([내 프로젝트 Repository](https://github.com/kdkdhoho/Swift))<br>\n\n사흘 째 되는 날, \"이게 맞나?\", \"이거 하려고 정말 많은 고민과 스트레스를 받으며 서울까지 왔나?\" 싶었다. 회의감도 들었다. 그래서 현장 실습을 포기하려고 했다. 포기로 인해 학교 휴학이나 자퇴를 해야 한다면 할 각오도 있었다. 그 정도로 싫었다.<br>\n이 때문에 퇴근 후 집에 와 밥을 먹을 때까지 멍을 때리며 정말 많은 고민과 생각을 했다.<br>\n그런데 막상 밥먹으면서 혼잣말로 생각을 정리하고 나니, 왠지 모를 독기와 끈기가 생겼다.\n\n돈보다 **경험**을 하기 위해 힘든 결정을 했고, 그 경험이 내가 원하는 것이 아니더라도 앞으로 있을 개발자 인생에서 피가 되고 살이 될 것이다고 판단했다. 그래서 이 악물고 해보려고 했다.\n\n그렇게 큰 고비를 넘기고, 연구소 사람들과 가까워졌다. swift도 할만해졌다.<br>\n이때만해도 정말 많은 것을 배웠다. 특히, 팀장님 덕분에 정말 많은 것을 느끼고 배울 수 있었다.\n\n특히 **요구사항에 대한 정확한 이해와 정확한 커뮤니케이션**을 몸소 겪었다.<br>\n한 번은 팀장님이 요구하신 사항을 다르게 이해하여 괜히 더 힘든 방향으로 구현을 했다. 이때 팀장님이, 구현하기 전에 정확한 요구사항 파악이 개발자에게 있어서 중요한 역량이라고 말씀해주셨다. 또한 자신이 이해한 것에 확신이 서지 않다면 되물어볼 수 있는 용기와 커뮤니케이션 능력 또한 중요하다고 말씀해주셨다.\n\n사실 다양한 매체를 통해 위 내용들이 중요하다고는 알고 있었다. 하지만 직접 몸소 겪어보니, 개발자는 개발을 잘하는 것은 기본 역량이며 다른 사람과 함께 일을 하는 능력 또한 중요한 요소라고 깨달았다.\n\n또 **동기와 비동기의 중요성**을 느꼈다.<br>\n여태까지 공부하고 프로젝트 한 것들은 동기, 비동기를 신경쓰며 개발하지 않았다. 하지만 앱 개발을 하려다보니, 동기 비동기가 정말 까다롭고 신경을 많이 써야하는 것이라고 느꼈다.<br>\n내 인사이트가 확장된 순간이었다.\n\n## 마침내 웹으로\n\n어느때와 마찬가지로 점심을 먹고 연구소 분들과 카페를 가는 길에, 소장님께서 \"아직도 웹 하고 싶냐?\"는 질문을 받았다. 솔직하게 그렇다고 말했다. 실제로 앱보다 웹이 내 적성에 맞고 더 재밌다.\n\n그런데 뭔가 느낌이 쎄했다. 갑자기 이런 걸 물어보실 분이 아닌데, 나한테 물어보신다는건 웹으로 갈 기회가 생겼다는 말처럼 들렸다.<br>\n아니나 다를까, 실제로 웹에 TO가 생겼고 내가 가고 싶다면 부서 이동도 가능하다고 하셨다.<br>\n한 달간 가까워진 연구소 분들을 두고 본사로 가야한다는 생각이 조금 아쉬웠지만, 웹이 하고 싶다고 했다.\n\n그렇게 약 이주 간의 시간이 흐르고 웹 부서로 자리를 이동했다.\n\n그곳에서는 회사가 운영하는 서비스의 기능을 파악하고, 레거시 코드를 파악 및 분석하고, 마침내 몇 가지 기능을 직접 추가하는 업무가 주어졌다.\n\n마침내 내가 정말 하고 싶었던 분야를 하게 되었기에, 정말 괜히 신이 나고 의지가 더 생겼다.<br>\n그래서 남은 기간, 정말 열심히 했다고 자부할 수 있다.\n\n그곳에서는 느낀 것들이 더더욱 많다.\n\n1. 보안에 대한 중요성\n2. DB 서버의 다중화 및 DB 크기의 놀라움\n3. 약어를 굉장히 많이 사용한다\n4. 개발자가 무엇을 하는 사람인가?\n5. 타인의 코드를 보고 분석하는 경험\n6. 기본이 정말 중요하다\n7. 클린 코드의 중요성\n8. 기존 프로젝트를 분석하여, 흐름을 코드 상으로도 파악하고, 기존에 있는 프로그램을 해치지 않으며 새로운 기능을 추가하는 경험을 해본 것\n\n모두 이야기하고 싶지만, 6, 7, 8번을 이야기 하고자 한다.<br>\n이번에 실무에서 프로젝트를 진행하면서, 기본이 정말 중요하다고 느꼈다.<br>\n실제 웹 부서 팀장님이 기본을 항상 중요시하며 유행을 쫓지말고 기본을 탄탄히 하라고 말씀해주셨다.<br>\n그래서 바로 자바의 정석 책을 구매하여, 퇴근 후 자바의 정석을 짬짬히 공부하였다.\n\n실제로 타인의 코드를 보면서 해당 코드를 해치지 않고 새로운 기능만을 추가하는 작업은 이번이 처음이다.<br>\n게다가 팀장님이 굉장히 쉽지 않은 작업이라고 해주셨다. 하지만 난 이번에 그것을 경험하면서 한 단계 성장한 기분이 들었다.<br>\n그런데, 회사의 코드는 옛날 스타일대로 작성되어 있었다. 또한 메서드 하나에 라인이 50줄이 되는 메서드도 있었고, Repository에서 비즈니스 로직을 처리하는 부분도 있었다. 그리고 변수명과 함수명이 약어로 정말 많이 되어 있어서 코드를 처음에 이해하는 것도 너무 힘들었고, 술술 읽히지도 않았다.<br>\n지금 생각하면 꽤나 단순한 기능들에도 불구하고 전체 코드를 이해하는 데에 일주일이 넘게 걸렸다.\n\n이때 느꼈다. 클린 코드와 클린 아키텍쳐가 이래서 정말 중요하구나.\n\n개발은 혼자서 절대 이루어질 수 없다. 내가 짠 코드를 타인이 읽어야 하고, 타인이 짠 코드를 내가 읽어야 한다.<br>\n이때 코드가 지저분하다면 개발이 정말 힘들 것이다.<br>\n또 서비스와 코드는 항상 발전한다. 새로운 기능이 추가되며, 새로운 구조가 적용될 수 있다.<br>\n이때, 아키텍쳐가 정교하고 깔끔하다면, 다른 사람이 봤을 때 꽤나 쉽게 기능을 구현할 수 있을 것이다.\n\n사실, 현장실습을 진행하면서 우테코 프리코스도 병행했다.<br>\n우테코에서도 위 내용들의 중요성을 간접적으로 그리고 단순하게 알려주려는 노력이 보였다.\n\n프리코스와 위 경험들을 동시에 경험하다보니, 클린 코드와 클린 아키텍쳐의 중요성과 필요성을 정말 잘 이해할 수 있는 소중한 시간이었다고 생각한다.\n\n## 마무리하며\n\n그동안 정말 많은 것을 얻을 수 있는 시간이었다.\n학교에 남아 수업을 들으며 공부를 했다면 결코 얻을 수 없던 것들이었다.\n\n이번 현장실습을 통해 앞으로 내 개발자 커리어에 대한 로드맵을 그리게 될 수 있을 것 같고, 개발 공부의 방향성을 일깨울 수 있었다.\n\n소중한 경험을 할 수 있게 해주신 대표님과 이사님들께 감사하다는 말씀을 드리며, 마무리한다.\n\n![](kakaotalk.png)\n\n## 각 부서에서 배운 것들\n\n### iOS\n\n- **요구사항에 대한 정확한 이해와 정확한 커뮤니케이션**\n  > 한 번은 viewAccssoryView에 버튼 달아서 버튼 누르면 키패드 내려가도록 요구\n  > 하지만 난 텍스트필드와 버튼이 같이 올라가도록 구현. 당시 내가 원하는 방법을 아무리 찾아도 나오지 않아 정말 힘들었던 경험이 있음. 그날 사수분이 피드백 해주시며 개발자는 커뮤니케이션이 중요하다. 요구사항이 있으면 그에 따른 정확한 니즈 파악이 중요하다. 기껏 만들어 줬더니 맘에 안들면 모두 폐기해야 한다. 따라서 요청이 들어올 때 개발자만 알고 있는 개발 관련 지식을 바탕으로 요청자와 많은 대화를 통해 상호간에 좀 더 합리적이고 이상적인 방향이 되도록 많은 의사소통이 중요하다고, 그래서 그 후로 의사소통의 중요성을 몸소 느끼고, 많이 하려고 노력했다. 실제로 그 다음 날 사수가 코로나에 걸렸는데 전에 지시하셨던 업무를 어떤 식으로 진행해야 할 지 확신이 서지 않았고 전 날 해주신 말씀이 바로 생각나서 바로 전화로 물어보며 진행 방향을 새롭게 조율했다.\n\n- 언어를 몰라도 구글링으로 원하는 기능을 개발은 할 수 있다. 하지만 개발하는 속도나 요구사항이 변경되었을 때 수정하는 능력은 떨어진다. 그래서 기본이 되어 있으면 개발 속도, 유지보수 속도가 매우 빨라짐\n- 공식문서의 중요성\n- 한 분야만 안다고 진짜 고수가 아니다\n- **동기 / 비동기의 중요성 → 서버 단에서는 크게 못느끼는데 클라이언트 단에서 통신하고 화면에 결과를 뿌려줄 때에는 동기 비동기의 중요성이 대두된다.**\n- 인턴 or 신입한테는 잘할 것이라고 기대안한다. 맡긴 일을 정확하게만 하자.\n- 기능은 구현하는 것은 쉽다. 하지만 개발하는 것은 어렵다. → 구현하고자는 기술에 대한 기본적인 원리, 이치 등을 정확하게 이해하고 있어야 트래픽이 거대해져도 큰 문제없이, 문제가 있더라도 쉽게 문제를 해결 할 힘이 생긴다.\n\n### WEB\n\n- 보안을 매우 중요하게 생각한다. (ex. DB url, 사용자 phone 번호 암호화)\n- DB 서버도 한 대가 아니고, 테이블도 무지하게 많고, 데이터도 어마어마하게 많다.\n- 약어를 굉장히 많이 흔하게 사용. 근데 별로 안좋은듯하다.\n- 개발자가 무엇을 하는 사람인지 깨달음 → 단순히 Spring 기술을 잘 사용하는 것에 그치는 것이 아닌, java, mysql, CS, OS 등 **기본 지식**들을 바탕으로 **문제를 해결하는 사람** → 단순히 새로운 기능을 구현하고 코드를 짜는 것만이 개발자가 아니라, 주어진 문제를 어떻게 좀 더 최적의 비용으로 해결할 수 있을지에 대한 고민을 하는 사람\n- 타인이 짠 소스코드를 보는 연습을 많이 했다\n    - 내가 소스코드를 짠 사람이라 생각하고 이해\n    - 코드를 그대로 작성하며 위에서부터 한 줄 한 줄 이해\n    - 길이가 짧다면 손코딩\n    - 결국, **반복**해서 읽다보면 코드의 패턴이 눈에 들어옴\n    - Entity → DB → Repo → Service → Controller 순으로 보는 것이 도움이 되었음\n- 기본이 중요하다. Java나 DB, Spring, JPA 같은. 하지만 아직 많이 부족하다.\n- **기존 프로젝트를 분석하여, 흐름을 코드 상으로도 파악하고, 기존에 있는 프로그램을 해치지 않으며 새로운 기능을 추가하는 경험을 해본 것이 가장 큰 경험이지 않을까 싶다.**"}]}},"pageContext":{}},"staticQueryHashes":[],"slicesMap":{}}